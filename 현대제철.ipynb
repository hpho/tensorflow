{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Data = pd.read_csv(r'D:\\Desktop\\현대제철\\MiningProcess_Flotation_Plant_Database.csv')\n",
    "# Data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Data[Data.columns[2]],Data[Data.columns[-1]],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data1 = Data.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Data1[Data1.columns[1]],Data1[Data1.columns[-1]],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Data1[Data1.columns[2]],Data1[Data1.columns[-1]],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra 부분 test set으로 interpo 부분 랜덤하게 섞어서 train, validation set으로 나누어서 진행\n",
    "plt.scatter([i for i in range(len(Data))],Data[Data.columns[-1]],s=1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([i for i in range(len(Data))],Data[Data.columns[-2]],s=1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_directory = 'D:\\Desktop\\현대제철\\MiningProcess_Flotation_Plant_Database.csv'\n",
    "save_ANN_model_directory = './model/ANN_model'\n",
    "standardization = True\n",
    "normalization = False\n",
    "max_norm = 1\n",
    "min_norm = 0\n",
    "\n",
    "number_of_k_folds = 5\n",
    "number_of_averaging = 1\n",
    "\n",
    "num_layer = 8\n",
    "num_neuron = 40\n",
    "activation_function = 'elu'\n",
    "learning_rate = 0.00001\n",
    "l2_reg = 0.000001\n",
    "batch_norm_momentum = 0.9\n",
    "initializer = 'he_normal'\n",
    "\n",
    "Endure_overfitting_step = 20\n",
    "epochs = 1000\n",
    "batch_size = 1024\n",
    "shuffle = True\n",
    "print_step = 100\n",
    "using_device = '/GPU:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import utils\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "if os.path.isdir('./save') == False:\n",
    "    os.mkdir('./save')\n",
    "\n",
    "if os.path.isdir('./model') == False:\n",
    "    os.mkdir('./model')\n",
    "\n",
    "def stz(Data):\n",
    "    return (Data - Data.mean())/Data.std()\n",
    "\n",
    "def norm(Data, max_norm=1, min_norm=0):\n",
    "\n",
    "    Data = (Data - Data.min())/(Data.max() - Data.min())*(max_norm - min_norm) + min_norm\n",
    "\n",
    "    return Data\n",
    "\n",
    "def preprocessing_Data(Data, standardization=True, normalization=True, max_normalization=1, min_normalization=0):\n",
    "    if standardization:\n",
    "        std_Data = stz(Data)\n",
    "    else:\n",
    "        std_Data = Data\n",
    "\n",
    "    if normalization:\n",
    "        normm_Data = norm(std_Data,max_normalization, min_normalization)\n",
    "    else:\n",
    "        normm_Data = std_Data\n",
    "\n",
    "    return normm_Data\n",
    "\n",
    "def pretreat_Data(Data, origin_Data, standardization=True, normalization=True):\n",
    "    if standardization:\n",
    "        std = origin_Data.std()\n",
    "        mean = origin_Data.mean()\n",
    "\n",
    "        stz_Data = stz(origin_Data)\n",
    "    else:\n",
    "        std = 1\n",
    "        mean = 0\n",
    "    if normalization:\n",
    "        max_stz_Data = stz_Data.max()\n",
    "        min_stz_Data = stz_Data.min()\n",
    "    else:\n",
    "        max_stz_Data = Data.max()\n",
    "        min_stz_Data = Data.min()\n",
    "\n",
    "    denorm_Data = norm(Data,max_stz_Data,min_stz_Data)\n",
    "    destz_Data  = denorm_Data*std + mean\n",
    "\n",
    "    return destz_Data\n",
    "    \n",
    "####### import input parameters #############\n",
    "\n",
    "# from input import *\n",
    "# print('input parameter loading complete')\n",
    "\n",
    "##################################################################################################################################\n",
    "save_file_name = 'l{}_n{}_{}'.format(num_layer,num_neuron,activation_function)\n",
    "\n",
    "####################################################################\n",
    "\n",
    "### Load Data & preprocessing ###\n",
    "\n",
    "Data = pd.read_csv(r'%s'%data_file_directory)\n",
    "Data = Data[Data.columns[1:]]\n",
    "Data = Data.stack().str.replace(',','.').unstack()\n",
    "Data = Data.astype(float)\n",
    "\n",
    "print(Data.head(5))\n",
    "\n",
    "preprocessed_Data = preprocessing_Data(Data,standardization,normalization,max_norm,min_norm)\n",
    "\n",
    "Train_Data = preprocessed_Data.sample(frac=0.8)\n",
    "Test_Data  = preprocessed_Data.drop(Train_Data.index)\n",
    "\n",
    "X_Train_Data = Train_Data[Train_Data.columns[:-2]]\n",
    "Y_Train_Data = Train_Data[Train_Data.columns[-2:]]\n",
    "X_Test_Data  = Test_Data[Test_Data.columns[:-2]]\n",
    "Y_Test_Data  = Test_Data[Test_Data.columns[-2:]]\n",
    "\n",
    "X_Data_for_model = preprocessed_Data[preprocessed_Data.columns[:-2]]\n",
    "Y_Data_for_model = preprocessed_Data[preprocessed_Data.columns[-2:]]\n",
    "\n",
    "print('Load Data & preprocessing complete')\n",
    "### \n",
    "\n",
    "### Define Network Sturucture ###\n",
    "with tf.device(using_device):\n",
    "\n",
    "    l2_regularizer = tf.contrib.layers.l2_regularizer(scale=l2_reg)\n",
    "    batch_prob = tf.placeholder(tf.bool)\n",
    "\n",
    "    dense_layer = partial(tf.layers.dense, activation=None, kernel_initializer=getattr(tf.keras.initializers,initializer)(), use_bias=True, kernel_regularizer=l2_regularizer)\n",
    "    batch_norm_layer = partial(tf.layers.batch_normalization, training=batch_prob, momentum=batch_norm_momentum)\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, len(X_Train_Data.columns)])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, len(Y_Train_Data.columns)])\n",
    "\n",
    "    for i in range(num_layer):\n",
    "        \n",
    "        if i == 0:\n",
    "            setattr(mod, 'hidden{}'.format(i+1),dense_layer(X,num_neuron))\n",
    "\n",
    "        else:\n",
    "            setattr(mod, 'hidden{}'.format(i+1),dense_layer(getattr(mod, 'BN{}_act'.format(i)),num_neuron))\n",
    "        \n",
    "        setattr(mod, 'BN{}'.format(i+1),batch_norm_layer(getattr(mod, 'hidden{}'.format(i+1))))\n",
    "        setattr(mod, 'BN{}_act'.format(i+1),getattr(tf.nn,activation_function)(getattr(mod, 'BN{}'.format(i+1))))\n",
    "\n",
    "    model = dense_layer(getattr(mod,'BN{}_act'.format(num_layer)), len(Y_Train_Data.columns), use_bias=False)\n",
    "\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(model - Y))\n",
    "#     reconstruction_loss = tf.reduce_mean(tf.abs((model - Y)/Y))\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "          train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    print('Define Network Structure complete')\n",
    "\n",
    "###\n",
    "\n",
    "### training ###\n",
    "\n",
    "    print('training start')\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    train_rmse = []\n",
    "    test_rmse  = []\n",
    "    loss_value_test = 9999\n",
    "    num_over = 0\n",
    "\n",
    "    for epoch_index in range(epochs):\n",
    "        X_Train_batch = X_Train_Data.sample(n=4)\n",
    "        Y_Train_batch = Y_Train_Data.loc[X_Train_batch.index]\n",
    "        sess.run(train_op, feed_dict={X: X_Train_batch, Y: Y_Train_batch, batch_prob:True})\n",
    " \n",
    "        if loss_value_test <= sess.run(loss, feed_dict={X: X_Test_Data, Y: Y_Test_Data, batch_prob:False}):\n",
    "            num_over += 1\n",
    "        else:\n",
    "            num_over = 0\n",
    "\n",
    "        if num_over == Endure_overfitting_step:\n",
    "            print('over fitting occurs!')\n",
    "            break\n",
    "\n",
    "        loss_value_train = sess.run(loss, feed_dict={X: X_Train_Data, Y: Y_Train_Data, batch_prob:True})\n",
    "        loss_value_test  = sess.run(loss, feed_dict={X: X_Test_Data , Y: Y_Test_Data , batch_prob:False})\n",
    "\n",
    "        if (epoch_index+1)%print_step == 0:\n",
    "            print('epoch: {}/{}, train loss: {:.4e}, test loss: {:.4e}'.format(epoch_index+1,epochs,loss_value_train,loss_value_test))\n",
    "            train_rmse.append(loss_value_train)\n",
    "            test_rmse.append(loss_value_test)\n",
    "\n",
    "    epoch_index1 = [(i+1)*print_step for i in range(len(train_rmse))]\n",
    "    plt.plot(epoch_index1, train_rmse)\n",
    "    plt.plot(epoch_index1, test_rmse)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.legend(['train','test'])\n",
    "    plt.savefig(r'./save/l%d_n%d_%s.png'%(num_layer,num_neuron,activation_function))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stz(Data):\n",
    "    return (Data - Data.mean())/Data.std()\n",
    "\n",
    "def norm(Data, max_norm=1, min_norm=0):\n",
    "\n",
    "    Data = (Data - Data.min())/(Data.max() - Data.min())*(max_norm - min_norm) + min_norm\n",
    "\n",
    "    return Data\n",
    "\n",
    "Data = Data[Data.columns[1:]]\n",
    "Data = Data.stack().str.replace(',','.').unstack()\n",
    "Data = Data.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stz(Data).boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        % Iron Feed  % Silica Feed  Starch Flow  Amina Flow  Ore Pulp Flow  \\\n",
      "246925     1.499737      -1.232727     0.546797    0.152014       0.326464   \n",
      "504569     0.499300      -0.790564     1.135612    0.774305      -0.307777   \n",
      "391486    -0.702776       0.474229    -1.375692   -2.497009       1.500539   \n",
      "87693     -0.555425       0.358179    -1.247931    0.287374       0.188729   \n",
      "117617     0.160004      -0.236758     0.026769   -0.550043       0.281824   \n",
      "\n",
      "        Ore Pulp pH  Ore Pulp Density  Flotation Column 01 Air Flow  \\\n",
      "246925    -0.245703          0.160729                      0.684276   \n",
      "504569     1.772993          0.356545                     -1.028951   \n",
      "391486    -1.440305         -2.316842                      0.664831   \n",
      "87693     -0.642439          0.770703                     -1.015177   \n",
      "117617    -0.355416          0.913233                     -2.023911   \n",
      "\n",
      "        Flotation Column 02 Air Flow  Flotation Column 03 Air Flow  ...  \\\n",
      "246925                      0.802473                      0.596451  ...   \n",
      "504569                     -0.984332                     -1.114717  ...   \n",
      "391486                      0.657096                      0.785818  ...   \n",
      "87693                      -1.064234                     -1.088175  ...   \n",
      "117617                     -1.868828                     -2.177982  ...   \n",
      "\n",
      "        Flotation Column 05 Air Flow  Flotation Column 06 Air Flow  \\\n",
      "246925                     -0.896396                      0.268137   \n",
      "504569                     -2.456737                     -1.215922   \n",
      "391486                      0.170816                      0.040986   \n",
      "87693                       0.858000                     -1.385524   \n",
      "117617                     -0.444691                     -1.457270   \n",
      "\n",
      "        Flotation Column 07 Air Flow  Flotation Column 01 Level  \\\n",
      "246925                      0.274891                  -0.526084   \n",
      "504569                     -1.525103                   0.246569   \n",
      "391486                      0.203318                  -1.846432   \n",
      "87693                      -1.438322                   0.603704   \n",
      "117617                     -1.433893                   1.298853   \n",
      "\n",
      "        Flotation Column 02 Level  Flotation Column 03 Level  \\\n",
      "246925                  -0.516479                  -0.533191   \n",
      "504569                   0.216357                   0.125093   \n",
      "391486                   1.867447                   2.266703   \n",
      "87693                   -2.222423                   0.431327   \n",
      "117617                   1.384024                   1.122421   \n",
      "\n",
      "        Flotation Column 04 Level  Flotation Column 05 Level  \\\n",
      "246925                  -0.957552                  -0.912710   \n",
      "504569                   0.488178                   0.062924   \n",
      "391486                   2.445155                   2.666598   \n",
      "87693                    0.444657                   0.272610   \n",
      "117617                  -0.403848                  -0.337806   \n",
      "\n",
      "        Flotation Column 06 Level  Flotation Column 07 Level  \n",
      "246925                  -0.651720                  -0.794900  \n",
      "504569                   0.262390                   0.417059  \n",
      "391486                   2.132336                  -1.442286  \n",
      "87693                    0.148327                   1.062907  \n",
      "117617                   0.976550                   1.418373  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "        % Iron Concentrate  % Silica Concentrate\n",
      "246925           -1.144303              1.459936\n",
      "504569            0.688272             -1.001075\n",
      "391486           -0.053697              0.065067\n",
      "87693            -0.017940             -0.050431\n",
      "117617           -1.720637              1.169479\n"
     ]
    }
   ],
   "source": [
    "stz_Data = stz(Data)\n",
    "X_Data = stz_Data[Data.columns[:-2]]\n",
    "Y_Data = stz_Data[Data.columns[-2:]]\n",
    "\n",
    "X_train = X_Data.sample(frac=0.7)\n",
    "Y_train = Y_Data.loc[X_train.index]\n",
    "\n",
    "print(X_train.head())\n",
    "print(Y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    % Iron Feed  % Silica Feed  Starch Flow  Amina Flow  Ore Pulp Flow  \\\n",
      "1     -0.212252       0.342021     0.127772    0.831085      -0.020142   \n",
      "7     -0.212252       0.342021     0.233532    0.774218      -0.059421   \n",
      "11    -0.212252       0.342021     0.231442    0.770875       0.028210   \n",
      "15    -0.212252       0.342021     0.570735    0.791613      -0.297983   \n",
      "17    -0.212252       0.342021     0.753634    0.765515      -0.282931   \n",
      "\n",
      "    Ore Pulp pH  Ore Pulp Density  Flotation Column 01 Air Flow  \\\n",
      "1      0.774045          0.860958                     -1.027398   \n",
      "7      0.786965          0.860958                     -1.021456   \n",
      "11     0.795492          0.860958                     -1.045932   \n",
      "15     0.804019          0.860958                     -1.017743   \n",
      "17     0.808153          0.860958                     -1.016258   \n",
      "\n",
      "    Flotation Column 02 Air Flow  Flotation Column 03 Air Flow  ...  \\\n",
      "1                      -0.883202                     -1.058201  ...   \n",
      "7                      -0.780447                     -1.114367  ...   \n",
      "11                     -0.906519                     -1.048222  ...   \n",
      "15                     -0.827811                     -1.107434  ...   \n",
      "17                     -1.015841                     -1.094373  ...   \n",
      "\n",
      "    Flotation Column 05 Air Flow  Flotation Column 06 Air Flow  \\\n",
      "1                       1.782495                     -1.387741   \n",
      "7                       1.782495                     -1.410277   \n",
      "11                      1.782495                     -1.361564   \n",
      "15                      1.782495                     -1.372485   \n",
      "17                      1.782495                     -1.379766   \n",
      "\n",
      "    Flotation Column 07 Air Flow  Flotation Column 01 Level  \\\n",
      "1                      -1.456599                  -0.521725   \n",
      "7                      -1.304838                  -0.566804   \n",
      "11                     -1.542438                  -0.448764   \n",
      "15                     -1.461971                  -0.653138   \n",
      "17                     -1.468842                  -0.520962   \n",
      "\n",
      "    Flotation Column 02 Level  Flotation Column 03 Level  \\\n",
      "1                   -0.726326                  -0.652428   \n",
      "7                   -0.478356                  -0.462156   \n",
      "11                  -0.789915                  -0.421385   \n",
      "15                  -0.828850                  -0.632427   \n",
      "17                  -0.751488                  -0.585570   \n",
      "\n",
      "    Flotation Column 04 Level  Flotation Column 05 Level  \\\n",
      "1                    0.302470                   0.841197   \n",
      "7                    0.236899                   0.194927   \n",
      "11                   0.416899                   0.338180   \n",
      "15                   0.431410                   0.699210   \n",
      "17                   0.105421                   0.323854   \n",
      "\n",
      "    Flotation Column 06 Level  Flotation Column 07 Level  \n",
      "1                    0.177839                   0.907674  \n",
      "7                    0.354198                   0.337581  \n",
      "11                   0.201519                   0.357147  \n",
      "15                   0.310575                   0.622274  \n",
      "17                   0.120507                   0.499458  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "    % Iron Concentrate  % Silica Concentrate\n",
      "1             1.662666             -0.903345\n",
      "7             1.662666             -0.903345\n",
      "11            1.662666             -0.903345\n",
      "15            1.662666             -0.903345\n",
      "17            1.662666             -0.903345\n"
     ]
    }
   ],
   "source": [
    "X_test = X_Data.drop(X_train.index)\n",
    "Y_test = Y_Data.drop(X_train.index)\n",
    "\n",
    "print(X_test.head())\n",
    "print(Y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "num_output = 2\n",
    "num_layer = 10\n",
    "num_neuron = 30\n",
    "activation_function = 'relu'\n",
    "initializer = tf.keras.initializers.he_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 steps train : 0.8774765133857727 test : 0.8818874359130859 \n",
      "2000 steps train : 0.8790948390960693 test : 0.884797215461731 \n",
      "3000 steps train : 0.842574954032898 test : 0.8479149341583252 \n",
      "4000 steps train : 0.8029224872589111 test : 0.8077775239944458 \n",
      "5000 steps train : 0.8017635345458984 test : 0.8066582679748535 \n",
      "6000 steps train : 0.7869660258293152 test : 0.7912655472755432 \n",
      "7000 steps train : 0.7893572449684143 test : 0.793049693107605 \n",
      "8000 steps train : 0.7946521043777466 test : 0.8001031279563904 \n",
      "9000 steps train : 0.7671710848808289 test : 0.7707956433296204 \n",
      "10000 steps train : 0.7542188763618469 test : 0.7600439190864563 \n"
     ]
    }
   ],
   "source": [
    "dense_layer = partial(tf.layers.dense, activation=None, kernel_initializer=initializer, use_bias=True, bias_initializer=initializer)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, len(X_Data.columns)])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_output])\n",
    "\n",
    "for i in range(num_layer):\n",
    "    if i == 0:\n",
    "        setattr(mod, 'hidden{}'.format(i+1),dense_layer(X,num_neuron))\n",
    "\n",
    "    else:\n",
    "        setattr(mod, 'hidden{}'.format(i+1),dense_layer(getattr(mod, 'hidden{}_act'.format(i)),num_neuron))\n",
    "        \n",
    "    setattr(mod,'hidden{}_act'.format(i+1),getattr(tf.nn,activation_function)(getattr(mod,'hidden{}'.format(i+1))))\n",
    "    \n",
    "model = dense_layer(getattr(mod,'hidden{}'.format(num_layer)), num_output)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(model - Y))\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    X_batch = X_train.sample(n=4)\n",
    "    Y_batch = Y_train.loc[X_batch.index]\n",
    "    sess.run(train_op, feed_dict={X: X_batch, Y: Y_batch})\n",
    "    \n",
    "    if (i+1)%1000==0:\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, Y: Y_train})\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, Y: Y_test})\n",
    "        print('{} steps train : {} test : {} '.format(i+1,train_loss,test_loss))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 steps train : 0.5581269860267639 test : 0.5638108849525452 \n",
      "2000 steps train : 0.5428184270858765 test : 0.5479255318641663 \n",
      "3000 steps train : 0.5414838790893555 test : 0.5476937890052795 \n",
      "4000 steps train : 0.5466351509094238 test : 0.5533009171485901 \n",
      "5000 steps train : 0.5498315095901489 test : 0.5563878417015076 \n",
      "6000 steps train : 0.5509857535362244 test : 0.5565089583396912 \n",
      "7000 steps train : 0.5571990013122559 test : 0.5613856911659241 \n",
      "8000 steps train : 0.5388202667236328 test : 0.5456218719482422 \n",
      "9000 steps train : 0.557174563407898 test : 0.5633724331855774 \n",
      "10000 steps train : 0.542865514755249 test : 0.5494778156280518 \n",
      "11000 steps train : 0.5487719178199768 test : 0.5555046796798706 \n",
      "12000 steps train : 0.5442540645599365 test : 0.5493817329406738 \n",
      "13000 steps train : 0.5362587571144104 test : 0.5420275330543518 \n",
      "14000 steps train : 0.5482608675956726 test : 0.5513421297073364 \n",
      "15000 steps train : 0.5487582683563232 test : 0.5541578531265259 \n",
      "16000 steps train : 0.5316963791847229 test : 0.5375483632087708 \n",
      "17000 steps train : 0.5325778722763062 test : 0.5392467975616455 \n",
      "18000 steps train : 0.5463979840278625 test : 0.5513638854026794 \n",
      "19000 steps train : 0.550495982170105 test : 0.5551486611366272 \n",
      "20000 steps train : 0.5340705513954163 test : 0.5393477082252502 \n",
      "21000 steps train : 0.545501172542572 test : 0.5511345267295837 \n",
      "22000 steps train : 0.5320038199424744 test : 0.538877546787262 \n",
      "23000 steps train : 0.530550479888916 test : 0.5361084342002869 \n",
      "24000 steps train : 0.5287050604820251 test : 0.5345680117607117 \n",
      "25000 steps train : 0.5299992561340332 test : 0.5365229845046997 \n",
      "26000 steps train : 0.5379629135131836 test : 0.5437065958976746 \n",
      "27000 steps train : 0.5268359780311584 test : 0.5337698459625244 \n",
      "28000 steps train : 0.5469955801963806 test : 0.554726779460907 \n",
      "29000 steps train : 0.547297477722168 test : 0.5535263419151306 \n",
      "30000 steps train : 0.5326807498931885 test : 0.5384788513183594 \n",
      "31000 steps train : 0.5292503237724304 test : 0.5351279973983765 \n",
      "32000 steps train : 0.5348522663116455 test : 0.5404562950134277 \n",
      "33000 steps train : 0.5318183898925781 test : 0.5380849242210388 \n",
      "34000 steps train : 0.5436325073242188 test : 0.5487704873085022 \n",
      "35000 steps train : 0.5318490266799927 test : 0.5383687615394592 \n",
      "36000 steps train : 0.5270597338676453 test : 0.5334065556526184 \n",
      "37000 steps train : 0.532992422580719 test : 0.5393801331520081 \n",
      "38000 steps train : 0.5185387134552002 test : 0.5247584581375122 \n",
      "39000 steps train : 0.5383309721946716 test : 0.5436120629310608 \n",
      "40000 steps train : 0.5228375196456909 test : 0.5272884368896484 \n",
      "41000 steps train : 0.5458644032478333 test : 0.5511879324913025 \n",
      "42000 steps train : 0.5402067303657532 test : 0.5459221005439758 \n",
      "43000 steps train : 0.5404029488563538 test : 0.5463657975196838 \n",
      "44000 steps train : 0.5307193398475647 test : 0.5366371273994446 \n",
      "45000 steps train : 0.5271412134170532 test : 0.5329740047454834 \n",
      "46000 steps train : 0.5262816548347473 test : 0.531961977481842 \n",
      "47000 steps train : 0.5261306166648865 test : 0.5311911702156067 \n",
      "48000 steps train : 0.5372443199157715 test : 0.5413986444473267 \n",
      "49000 steps train : 0.5323469638824463 test : 0.5387240052223206 \n",
      "50000 steps train : 0.524988055229187 test : 0.5300654768943787 \n",
      "51000 steps train : 0.5309199690818787 test : 0.5381388068199158 \n",
      "52000 steps train : 0.5256749391555786 test : 0.532315731048584 \n",
      "53000 steps train : 0.5291147828102112 test : 0.5361502170562744 \n",
      "54000 steps train : 0.5316870808601379 test : 0.5377631187438965 \n",
      "55000 steps train : 0.525590181350708 test : 0.5314169526100159 \n",
      "56000 steps train : 0.5250688791275024 test : 0.5311200022697449 \n",
      "57000 steps train : 0.5237368941307068 test : 0.52891606092453 \n",
      "58000 steps train : 0.5460430979728699 test : 0.552753210067749 \n",
      "59000 steps train : 0.5235579013824463 test : 0.5281124114990234 \n",
      "60000 steps train : 0.5382221341133118 test : 0.5455988049507141 \n",
      "61000 steps train : 0.5175944566726685 test : 0.5236490368843079 \n",
      "62000 steps train : 0.521006166934967 test : 0.528247058391571 \n",
      "63000 steps train : 0.5256544947624207 test : 0.5334358215332031 \n",
      "64000 steps train : 0.5333414673805237 test : 0.5392778515815735 \n",
      "65000 steps train : 0.5249745845794678 test : 0.5316416025161743 \n",
      "66000 steps train : 0.5317031741142273 test : 0.5390668511390686 \n",
      "67000 steps train : 0.5277202129364014 test : 0.5352082252502441 \n",
      "68000 steps train : 0.5308689475059509 test : 0.5361455678939819 \n",
      "69000 steps train : 0.5206806063652039 test : 0.5272267460823059 \n",
      "70000 steps train : 0.5232318639755249 test : 0.530302107334137 \n",
      "71000 steps train : 0.5173593759536743 test : 0.5236008763313293 \n",
      "72000 steps train : 0.5371931195259094 test : 0.5438735485076904 \n",
      "73000 steps train : 0.5189491510391235 test : 0.526015043258667 \n",
      "74000 steps train : 0.5312051177024841 test : 0.5383425951004028 \n",
      "75000 steps train : 0.513821005821228 test : 0.5207274556159973 \n",
      "76000 steps train : 0.5292059779167175 test : 0.5354704260826111 \n",
      "77000 steps train : 0.5200779438018799 test : 0.5266314744949341 \n",
      "78000 steps train : 0.5254567861557007 test : 0.5320335030555725 \n",
      "79000 steps train : 0.5238595008850098 test : 0.5290327072143555 \n",
      "80000 steps train : 0.5392358303070068 test : 0.5455538034439087 \n",
      "81000 steps train : 0.5316852331161499 test : 0.5367917418479919 \n",
      "82000 steps train : 0.5064283609390259 test : 0.5130006074905396 \n",
      "83000 steps train : 0.5184382200241089 test : 0.524158775806427 \n",
      "84000 steps train : 0.521720826625824 test : 0.5287507176399231 \n",
      "85000 steps train : 0.5240805745124817 test : 0.5304400324821472 \n",
      "86000 steps train : 0.5185731649398804 test : 0.5246925354003906 \n",
      "87000 steps train : 0.5191006660461426 test : 0.523542046546936 \n",
      "88000 steps train : 0.5254403948783875 test : 0.5300533771514893 \n",
      "89000 steps train : 0.5206771492958069 test : 0.5261934399604797 \n",
      "90000 steps train : 0.5245331525802612 test : 0.5304785966873169 \n",
      "91000 steps train : 0.5108928680419922 test : 0.5173961520195007 \n",
      "92000 steps train : 0.522465705871582 test : 0.5292027592658997 \n",
      "93000 steps train : 0.5194396376609802 test : 0.5258777737617493 \n",
      "94000 steps train : 0.5177686810493469 test : 0.5236597657203674 \n",
      "95000 steps train : 0.5206132531166077 test : 0.5261568427085876 \n",
      "96000 steps train : 0.5171112418174744 test : 0.5222371220588684 \n",
      "97000 steps train : 0.5210863947868347 test : 0.5255897641181946 \n",
      "98000 steps train : 0.5119533538818359 test : 0.5182638168334961 \n",
      "99000 steps train : 0.5200307369232178 test : 0.5252538919448853 \n",
      "100000 steps train : 0.5418044328689575 test : 0.5483501553535461 \n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    X_batch = X_train.sample(n=4)\n",
    "    Y_batch = Y_train.loc[X_batch.index]\n",
    "    sess.run(train_op, feed_dict={X: X_batch, Y: Y_batch})\n",
    "    \n",
    "    if (i+1)%1000==0:\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, Y: Y_train})\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, Y: Y_test})\n",
    "        print('{} steps train : {} test : {} '.format(i+1,train_loss,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 steps train : 0.5094892978668213 test : 0.5150378942489624 \n",
      "2000 steps train : 0.5332385897636414 test : 0.5386466383934021 \n",
      "3000 steps train : 0.5207266807556152 test : 0.5276154279708862 \n",
      "4000 steps train : 0.5292285084724426 test : 0.5364152193069458 \n",
      "5000 steps train : 0.5014687180519104 test : 0.5090327262878418 \n",
      "6000 steps train : 0.5140431523323059 test : 0.5207023620605469 \n",
      "7000 steps train : 0.5080601572990417 test : 0.5141549706459045 \n",
      "8000 steps train : 0.5055476427078247 test : 0.5117136240005493 \n",
      "9000 steps train : 0.5257136225700378 test : 0.5336307883262634 \n",
      "10000 steps train : 0.5145565271377563 test : 0.5218815207481384 \n",
      "11000 steps train : 0.5139123201370239 test : 0.5209696292877197 \n",
      "12000 steps train : 0.5106471180915833 test : 0.5168747305870056 \n",
      "13000 steps train : 0.5128198266029358 test : 0.5193194150924683 \n",
      "14000 steps train : 0.5146442651748657 test : 0.5205898880958557 \n",
      "15000 steps train : 0.5331644415855408 test : 0.539254903793335 \n",
      "16000 steps train : 0.5060613751411438 test : 0.5111852884292603 \n",
      "17000 steps train : 0.529380738735199 test : 0.5355522632598877 \n",
      "18000 steps train : 0.5193593502044678 test : 0.5244553089141846 \n",
      "19000 steps train : 0.5084954500198364 test : 0.5144935846328735 \n",
      "20000 steps train : 0.5178192853927612 test : 0.5246961712837219 \n",
      "21000 steps train : 0.5162675976753235 test : 0.5220180153846741 \n",
      "22000 steps train : 0.506763756275177 test : 0.512662410736084 \n",
      "23000 steps train : 0.5014262199401855 test : 0.5084152221679688 \n",
      "24000 steps train : 0.513176441192627 test : 0.5215123295783997 \n",
      "25000 steps train : 0.5256742835044861 test : 0.5324968099594116 \n",
      "26000 steps train : 0.4983692765235901 test : 0.5058777928352356 \n",
      "27000 steps train : 0.5092337131500244 test : 0.5163683295249939 \n",
      "28000 steps train : 0.5079535245895386 test : 0.5134462714195251 \n",
      "29000 steps train : 0.5367205739021301 test : 0.5425841808319092 \n",
      "30000 steps train : 0.5099779963493347 test : 0.5152506828308105 \n",
      "31000 steps train : 0.5137450695037842 test : 0.520643413066864 \n",
      "32000 steps train : 0.5010082721710205 test : 0.5068426132202148 \n",
      "33000 steps train : 0.5172350406646729 test : 0.5222772359848022 \n",
      "34000 steps train : 0.5199292302131653 test : 0.5258356332778931 \n",
      "35000 steps train : 0.5208155512809753 test : 0.5262177586555481 \n",
      "36000 steps train : 0.5140215754508972 test : 0.5219159722328186 \n",
      "37000 steps train : 0.5163222551345825 test : 0.5226958990097046 \n",
      "38000 steps train : 0.5068617463111877 test : 0.5142582654953003 \n",
      "39000 steps train : 0.49567070603370667 test : 0.5027608871459961 \n",
      "40000 steps train : 0.5067010521888733 test : 0.5127713680267334 \n",
      "41000 steps train : 0.49636322259902954 test : 0.5027530789375305 \n",
      "42000 steps train : 0.5063856840133667 test : 0.5136941075325012 \n",
      "43000 steps train : 0.4977933168411255 test : 0.5035253763198853 \n",
      "44000 steps train : 0.5023112297058105 test : 0.5065598487854004 \n",
      "45000 steps train : 0.4909849464893341 test : 0.4989182949066162 \n",
      "46000 steps train : 0.5148445963859558 test : 0.5219491720199585 \n",
      "47000 steps train : 0.5164073705673218 test : 0.5227993130683899 \n",
      "48000 steps train : 0.5072444081306458 test : 0.5169174075126648 \n",
      "49000 steps train : 0.5019211769104004 test : 0.508203387260437 \n",
      "50000 steps train : 0.5173170566558838 test : 0.5239148139953613 \n",
      "51000 steps train : 0.48517677187919617 test : 0.49269282817840576 \n",
      "52000 steps train : 0.5065122246742249 test : 0.5127424597740173 \n",
      "53000 steps train : 0.5054734349250793 test : 0.5127685070037842 \n",
      "54000 steps train : 0.5053842663764954 test : 0.5116050839424133 \n",
      "55000 steps train : 0.49094465374946594 test : 0.4995437562465668 \n",
      "56000 steps train : 0.5175871253013611 test : 0.5249077081680298 \n",
      "57000 steps train : 0.5210138559341431 test : 0.5292777419090271 \n",
      "58000 steps train : 0.5216801762580872 test : 0.5292895436286926 \n",
      "59000 steps train : 0.4888463318347931 test : 0.49543359875679016 \n",
      "60000 steps train : 0.484181672334671 test : 0.490748792886734 \n",
      "61000 steps train : 0.498516708612442 test : 0.5053942799568176 \n",
      "62000 steps train : 0.48944151401519775 test : 0.4961663484573364 \n",
      "63000 steps train : 0.5057922601699829 test : 0.5143728256225586 \n",
      "64000 steps train : 0.4919120669364929 test : 0.4993011951446533 \n",
      "65000 steps train : 0.49368685483932495 test : 0.5006938576698303 \n",
      "66000 steps train : 0.5148918032646179 test : 0.5204900503158569 \n",
      "67000 steps train : 0.4996121823787689 test : 0.5064391493797302 \n",
      "68000 steps train : 0.5083203315734863 test : 0.5148144364356995 \n",
      "69000 steps train : 0.5008698105812073 test : 0.5078041553497314 \n",
      "70000 steps train : 0.5063884258270264 test : 0.514610767364502 \n",
      "71000 steps train : 0.4947134554386139 test : 0.5013871192932129 \n",
      "72000 steps train : 0.49792903661727905 test : 0.5044260025024414 \n",
      "73000 steps train : 0.4980058968067169 test : 0.5039676427841187 \n",
      "74000 steps train : 0.49476999044418335 test : 0.500288188457489 \n",
      "75000 steps train : 0.486964613199234 test : 0.4930288791656494 \n",
      "76000 steps train : 0.4910552501678467 test : 0.4974108040332794 \n",
      "77000 steps train : 0.4866935908794403 test : 0.4943729043006897 \n",
      "78000 steps train : 0.4909977614879608 test : 0.49863603711128235 \n",
      "79000 steps train : 0.5028388500213623 test : 0.5092240571975708 \n",
      "80000 steps train : 0.48950129747390747 test : 0.49749019742012024 \n",
      "81000 steps train : 0.48829981684684753 test : 0.49304065108299255 \n",
      "82000 steps train : 0.49186721444129944 test : 0.4982230067253113 \n",
      "83000 steps train : 0.5135595202445984 test : 0.5212283134460449 \n",
      "84000 steps train : 0.49261027574539185 test : 0.5002838373184204 \n",
      "85000 steps train : 0.49044182896614075 test : 0.4964679181575775 \n",
      "86000 steps train : 0.4993254542350769 test : 0.5046268701553345 \n",
      "87000 steps train : 0.48715144395828247 test : 0.4937160611152649 \n",
      "88000 steps train : 0.4863981306552887 test : 0.4935656487941742 \n",
      "89000 steps train : 0.4970073401927948 test : 0.5038428902626038 \n",
      "90000 steps train : 0.4969480037689209 test : 0.5038701891899109 \n",
      "91000 steps train : 0.4849070906639099 test : 0.49117720127105713 \n",
      "92000 steps train : 0.480736643075943 test : 0.48778483271598816 \n",
      "93000 steps train : 0.48657557368278503 test : 0.49276506900787354 \n",
      "94000 steps train : 0.49000808596611023 test : 0.495434045791626 \n",
      "95000 steps train : 0.49983665347099304 test : 0.5061811208724976 \n",
      "96000 steps train : 0.48981764912605286 test : 0.49714648723602295 \n",
      "97000 steps train : 0.5019444823265076 test : 0.5077957510948181 \n",
      "98000 steps train : 0.499735563993454 test : 0.5063007473945618 \n",
      "99000 steps train : 0.48571285605430603 test : 0.4929620921611786 \n",
      "100000 steps train : 0.48701369762420654 test : 0.49358341097831726 \n",
      "101000 steps train : 0.4953051507472992 test : 0.5018600225448608 \n",
      "102000 steps train : 0.4797787368297577 test : 0.4856507480144501 \n",
      "103000 steps train : 0.4871442914009094 test : 0.49351003766059875 \n",
      "104000 steps train : 0.49709635972976685 test : 0.503516674041748 \n",
      "105000 steps train : 0.4905456304550171 test : 0.49725139141082764 \n",
      "106000 steps train : 0.4843691289424896 test : 0.4902310073375702 \n",
      "107000 steps train : 0.4871554970741272 test : 0.4932069182395935 \n",
      "108000 steps train : 0.5085694789886475 test : 0.5138888955116272 \n",
      "109000 steps train : 0.5058472156524658 test : 0.5120077729225159 \n",
      "110000 steps train : 0.4921112358570099 test : 0.4976412355899811 \n",
      "111000 steps train : 0.4907681345939636 test : 0.4969678521156311 \n",
      "112000 steps train : 0.504503607749939 test : 0.5097850561141968 \n",
      "113000 steps train : 0.49762386083602905 test : 0.5039065480232239 \n",
      "114000 steps train : 0.4792518615722656 test : 0.4848220646381378 \n",
      "115000 steps train : 0.4818245470523834 test : 0.48683395981788635 \n",
      "116000 steps train : 0.4880731999874115 test : 0.4936870038509369 \n",
      "117000 steps train : 0.4973119795322418 test : 0.5025840401649475 \n",
      "118000 steps train : 0.4963414669036865 test : 0.5041314959526062 \n",
      "119000 steps train : 0.4868040680885315 test : 0.4931340515613556 \n",
      "120000 steps train : 0.4886478781700134 test : 0.4937404990196228 \n",
      "121000 steps train : 0.4817327857017517 test : 0.48927026987075806 \n",
      "122000 steps train : 0.4829632341861725 test : 0.48970410227775574 \n",
      "123000 steps train : 0.5293724536895752 test : 0.5355350971221924 \n",
      "124000 steps train : 0.4769956171512604 test : 0.48385632038116455 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125000 steps train : 0.48873835802078247 test : 0.4952086806297302 \n",
      "126000 steps train : 0.48559409379959106 test : 0.4926728308200836 \n",
      "127000 steps train : 0.5054194331169128 test : 0.5110374093055725 \n",
      "128000 steps train : 0.48228317499160767 test : 0.48825183510780334 \n",
      "129000 steps train : 0.4843192398548126 test : 0.49053895473480225 \n",
      "130000 steps train : 0.4881370961666107 test : 0.4953557848930359 \n",
      "131000 steps train : 0.4859865605831146 test : 0.4921390116214752 \n",
      "132000 steps train : 0.46745848655700684 test : 0.47439101338386536 \n",
      "133000 steps train : 0.4876694977283478 test : 0.49311304092407227 \n",
      "134000 steps train : 0.48292276263237 test : 0.4905496835708618 \n",
      "135000 steps train : 0.47354137897491455 test : 0.48093974590301514 \n",
      "136000 steps train : 0.4743792712688446 test : 0.4801986813545227 \n",
      "137000 steps train : 0.5088922381401062 test : 0.5139175653457642 \n",
      "138000 steps train : 0.49616676568984985 test : 0.5016181468963623 \n",
      "139000 steps train : 0.4876681864261627 test : 0.4919942319393158 \n",
      "140000 steps train : 0.4832170009613037 test : 0.4892518222332001 \n",
      "141000 steps train : 0.4868835508823395 test : 0.49349668622016907 \n",
      "142000 steps train : 0.5010945796966553 test : 0.508029580116272 \n",
      "143000 steps train : 0.482227623462677 test : 0.48950430750846863 \n",
      "144000 steps train : 0.49820128083229065 test : 0.5055654048919678 \n",
      "145000 steps train : 0.4803348183631897 test : 0.4878069758415222 \n",
      "146000 steps train : 0.48749715089797974 test : 0.49453404545783997 \n",
      "147000 steps train : 0.47671595215797424 test : 0.4841141402721405 \n",
      "148000 steps train : 0.49191993474960327 test : 0.4963621199131012 \n",
      "149000 steps train : 0.47492516040802 test : 0.4819781184196472 \n",
      "150000 steps train : 0.4789811670780182 test : 0.4880502223968506 \n",
      "151000 steps train : 0.4800359010696411 test : 0.4867400825023651 \n",
      "152000 steps train : 0.47803089022636414 test : 0.4851563572883606 \n",
      "153000 steps train : 0.487863153219223 test : 0.49523141980171204 \n",
      "154000 steps train : 0.47461438179016113 test : 0.4816376566886902 \n",
      "155000 steps train : 0.4833962321281433 test : 0.48964497447013855 \n",
      "156000 steps train : 0.48178502917289734 test : 0.4883324205875397 \n",
      "157000 steps train : 0.4773770868778229 test : 0.48423412442207336 \n",
      "158000 steps train : 0.5017976760864258 test : 0.5079613327980042 \n",
      "159000 steps train : 0.4975370764732361 test : 0.5026353001594543 \n",
      "160000 steps train : 0.49482476711273193 test : 0.50304114818573 \n",
      "161000 steps train : 0.4979599118232727 test : 0.5041185617446899 \n",
      "162000 steps train : 0.48114997148513794 test : 0.4902259409427643 \n",
      "163000 steps train : 0.5006325244903564 test : 0.5095786452293396 \n",
      "164000 steps train : 0.47784778475761414 test : 0.4834919571876526 \n",
      "165000 steps train : 0.4878317415714264 test : 0.4945642352104187 \n",
      "166000 steps train : 0.47167956829071045 test : 0.47632163763046265 \n",
      "167000 steps train : 0.4617850184440613 test : 0.46799781918525696 \n",
      "168000 steps train : 0.47884514927864075 test : 0.4849977195262909 \n",
      "169000 steps train : 0.47743409872055054 test : 0.48338761925697327 \n",
      "170000 steps train : 0.47348693013191223 test : 0.4809951186180115 \n",
      "171000 steps train : 0.479147732257843 test : 0.48644939064979553 \n",
      "172000 steps train : 0.48119056224823 test : 0.48746687173843384 \n",
      "173000 steps train : 0.4780142903327942 test : 0.48471900820732117 \n",
      "174000 steps train : 0.4713735580444336 test : 0.4784722924232483 \n",
      "175000 steps train : 0.48266908526420593 test : 0.4898480772972107 \n",
      "176000 steps train : 0.49282336235046387 test : 0.49934571981430054 \n",
      "177000 steps train : 0.47295862436294556 test : 0.47943827509880066 \n",
      "178000 steps train : 0.4758625328540802 test : 0.48256996273994446 \n",
      "179000 steps train : 0.4820639193058014 test : 0.49007904529571533 \n",
      "180000 steps train : 0.48018282651901245 test : 0.48764607310295105 \n",
      "181000 steps train : 0.47841203212738037 test : 0.48516035079956055 \n",
      "182000 steps train : 0.4845599830150604 test : 0.4917377233505249 \n",
      "183000 steps train : 0.47405341267585754 test : 0.481109619140625 \n",
      "184000 steps train : 0.4816543757915497 test : 0.48852774500846863 \n",
      "185000 steps train : 0.47305789589881897 test : 0.4810992181301117 \n",
      "186000 steps train : 0.483921080827713 test : 0.49086475372314453 \n",
      "187000 steps train : 0.47154417634010315 test : 0.4776351749897003 \n",
      "188000 steps train : 0.46478065848350525 test : 0.4732573926448822 \n",
      "189000 steps train : 0.4755769670009613 test : 0.4820241332054138 \n",
      "190000 steps train : 0.47147420048713684 test : 0.4781549274921417 \n",
      "191000 steps train : 0.48661839962005615 test : 0.4937218129634857 \n",
      "192000 steps train : 0.47853025794029236 test : 0.48347967863082886 \n",
      "193000 steps train : 0.4800850450992584 test : 0.48664259910583496 \n",
      "194000 steps train : 0.4725166857242584 test : 0.47857964038848877 \n",
      "195000 steps train : 0.4730997383594513 test : 0.47939780354499817 \n",
      "196000 steps train : 0.4711664617061615 test : 0.4794119596481323 \n",
      "197000 steps train : 0.4665103852748871 test : 0.47366639971733093 \n",
      "198000 steps train : 0.4949071407318115 test : 0.5062218308448792 \n",
      "199000 steps train : 0.5171472430229187 test : 0.5238355398178101 \n",
      "200000 steps train : 0.4768942892551422 test : 0.48203346133232117 \n",
      "201000 steps train : 0.47759583592414856 test : 0.4861116111278534 \n",
      "202000 steps train : 0.48598915338516235 test : 0.49168041348457336 \n",
      "203000 steps train : 0.46176645159721375 test : 0.4693674147129059 \n",
      "204000 steps train : 0.4758545756340027 test : 0.48312467336654663 \n",
      "205000 steps train : 0.49091941118240356 test : 0.4972817599773407 \n",
      "206000 steps train : 0.4696321487426758 test : 0.47578442096710205 \n",
      "207000 steps train : 0.47201770544052124 test : 0.4779854118824005 \n",
      "208000 steps train : 0.4719788432121277 test : 0.4799558222293854 \n",
      "209000 steps train : 0.4762714207172394 test : 0.48369112610816956 \n",
      "210000 steps train : 0.47789689898490906 test : 0.48409178853034973 \n",
      "211000 steps train : 0.4651181697845459 test : 0.47089701890945435 \n",
      "212000 steps train : 0.4729478359222412 test : 0.4791710674762726 \n",
      "213000 steps train : 0.4713846445083618 test : 0.4765806794166565 \n",
      "214000 steps train : 0.4740844666957855 test : 0.4805394411087036 \n",
      "215000 steps train : 0.4923577904701233 test : 0.49809879064559937 \n",
      "216000 steps train : 0.47248581051826477 test : 0.47918611764907837 \n",
      "217000 steps train : 0.4725893437862396 test : 0.4782731533050537 \n",
      "218000 steps train : 0.48122698068618774 test : 0.4882473051548004 \n",
      "219000 steps train : 0.4722515940666199 test : 0.47854965925216675 \n",
      "220000 steps train : 0.47612881660461426 test : 0.48322585225105286 \n",
      "221000 steps train : 0.48247814178466797 test : 0.48810118436813354 \n",
      "222000 steps train : 0.48720550537109375 test : 0.49328842759132385 \n",
      "223000 steps train : 0.47169172763824463 test : 0.47689247131347656 \n",
      "224000 steps train : 0.4801692068576813 test : 0.48660463094711304 \n",
      "225000 steps train : 0.45865902304649353 test : 0.4653257429599762 \n",
      "226000 steps train : 0.4639364778995514 test : 0.46924787759780884 \n",
      "227000 steps train : 0.46789154410362244 test : 0.47369781136512756 \n",
      "228000 steps train : 0.4729823172092438 test : 0.4804653525352478 \n",
      "229000 steps train : 0.47480565309524536 test : 0.4820752441883087 \n",
      "230000 steps train : 0.46356305480003357 test : 0.47105318307876587 \n",
      "231000 steps train : 0.4749206304550171 test : 0.48183536529541016 \n",
      "232000 steps train : 0.48443886637687683 test : 0.48994630575180054 \n",
      "233000 steps train : 0.46610546112060547 test : 0.47218403220176697 \n",
      "234000 steps train : 0.4778149127960205 test : 0.4844945967197418 \n",
      "235000 steps train : 0.4760938584804535 test : 0.482481449842453 \n",
      "236000 steps train : 0.49014392495155334 test : 0.4987105429172516 \n",
      "237000 steps train : 0.4671495854854584 test : 0.47436830401420593 \n",
      "238000 steps train : 0.4543767273426056 test : 0.46251896023750305 \n",
      "239000 steps train : 0.4788036346435547 test : 0.48555290699005127 \n",
      "240000 steps train : 0.46760162711143494 test : 0.4736463129520416 \n",
      "241000 steps train : 0.47322511672973633 test : 0.47982966899871826 \n",
      "242000 steps train : 0.45851653814315796 test : 0.464634507894516 \n",
      "243000 steps train : 0.4592781364917755 test : 0.4666801393032074 \n",
      "244000 steps train : 0.45394524931907654 test : 0.4620174765586853 \n",
      "245000 steps train : 0.46369773149490356 test : 0.4696052074432373 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246000 steps train : 0.4628976583480835 test : 0.47081729769706726 \n",
      "247000 steps train : 0.46402835845947266 test : 0.4707314074039459 \n",
      "248000 steps train : 0.46280014514923096 test : 0.46994343400001526 \n",
      "249000 steps train : 0.45555198192596436 test : 0.4622083306312561 \n",
      "250000 steps train : 0.46067655086517334 test : 0.466823011636734 \n",
      "251000 steps train : 0.4665546119213104 test : 0.4738237261772156 \n",
      "252000 steps train : 0.4829890727996826 test : 0.490888774394989 \n",
      "253000 steps train : 0.4762473702430725 test : 0.4836897850036621 \n",
      "254000 steps train : 0.4734289348125458 test : 0.4811467230319977 \n",
      "255000 steps train : 0.4612962603569031 test : 0.4687562584877014 \n",
      "256000 steps train : 0.4761972725391388 test : 0.4858493506908417 \n",
      "257000 steps train : 0.4638148248195648 test : 0.47100359201431274 \n",
      "258000 steps train : 0.45583510398864746 test : 0.46440646052360535 \n",
      "259000 steps train : 0.48876118659973145 test : 0.4973771572113037 \n",
      "260000 steps train : 0.4807850420475006 test : 0.48794519901275635 \n",
      "261000 steps train : 0.46361154317855835 test : 0.4718070924282074 \n",
      "262000 steps train : 0.4722138047218323 test : 0.4791140854358673 \n",
      "263000 steps train : 0.4701419770717621 test : 0.47817474603652954 \n",
      "264000 steps train : 0.47944727540016174 test : 0.4878885746002197 \n",
      "265000 steps train : 0.4696001410484314 test : 0.4777686893939972 \n",
      "266000 steps train : 0.4611808955669403 test : 0.4688173830509186 \n",
      "267000 steps train : 0.46295052766799927 test : 0.4698841869831085 \n",
      "268000 steps train : 0.47276490926742554 test : 0.480646550655365 \n",
      "269000 steps train : 0.46253857016563416 test : 0.47059327363967896 \n",
      "270000 steps train : 0.4536905586719513 test : 0.4618912935256958 \n",
      "271000 steps train : 0.4681594967842102 test : 0.4752214550971985 \n",
      "272000 steps train : 0.4534907937049866 test : 0.4617456793785095 \n",
      "273000 steps train : 0.4651602506637573 test : 0.4717716574668884 \n",
      "274000 steps train : 0.4722974896430969 test : 0.47993987798690796 \n",
      "275000 steps train : 0.472900390625 test : 0.480711430311203 \n",
      "276000 steps train : 0.45190903544425964 test : 0.45942434668540955 \n",
      "277000 steps train : 0.45617005228996277 test : 0.4645332098007202 \n",
      "278000 steps train : 0.47320041060447693 test : 0.48066338896751404 \n",
      "279000 steps train : 0.46740642189979553 test : 0.4764787256717682 \n",
      "280000 steps train : 0.46622803807258606 test : 0.47315600514411926 \n",
      "281000 steps train : 0.48505839705467224 test : 0.49231117963790894 \n",
      "282000 steps train : 0.45724403858184814 test : 0.46415698528289795 \n",
      "283000 steps train : 0.4752787947654724 test : 0.48232585191726685 \n",
      "284000 steps train : 0.4600660502910614 test : 0.4657951593399048 \n",
      "285000 steps train : 0.4580678641796112 test : 0.4641105830669403 \n",
      "286000 steps train : 0.46305420994758606 test : 0.4704008102416992 \n",
      "287000 steps train : 0.46189218759536743 test : 0.4686380922794342 \n",
      "288000 steps train : 0.47354093194007874 test : 0.48199498653411865 \n",
      "289000 steps train : 0.4748972952365875 test : 0.48191481828689575 \n",
      "290000 steps train : 0.46456825733184814 test : 0.4720821976661682 \n",
      "291000 steps train : 0.4618232250213623 test : 0.4719873070716858 \n",
      "292000 steps train : 0.4691954255104065 test : 0.47797608375549316 \n",
      "293000 steps train : 0.45701760053634644 test : 0.46567875146865845 \n",
      "294000 steps train : 0.459684282541275 test : 0.4687407612800598 \n",
      "295000 steps train : 0.4752808213233948 test : 0.4832068085670471 \n",
      "296000 steps train : 0.4634973108768463 test : 0.4711923897266388 \n",
      "297000 steps train : 0.4614914655685425 test : 0.46914181113243103 \n",
      "298000 steps train : 0.4650830328464508 test : 0.47310560941696167 \n",
      "299000 steps train : 0.4705154299736023 test : 0.47758185863494873 \n",
      "300000 steps train : 0.45817244052886963 test : 0.46544530987739563 \n",
      "301000 steps train : 0.4563387334346771 test : 0.4631597399711609 \n",
      "302000 steps train : 0.4606190025806427 test : 0.46830400824546814 \n",
      "303000 steps train : 0.4609706997871399 test : 0.4680587947368622 \n",
      "304000 steps train : 0.4526307284832001 test : 0.45968616008758545 \n",
      "305000 steps train : 0.45649826526641846 test : 0.46284040808677673 \n",
      "306000 steps train : 0.4756562113761902 test : 0.4853475093841553 \n",
      "307000 steps train : 0.4662300646305084 test : 0.47367897629737854 \n",
      "308000 steps train : 0.4566288888454437 test : 0.46415776014328003 \n",
      "309000 steps train : 0.44803714752197266 test : 0.45590388774871826 \n",
      "310000 steps train : 0.45084550976753235 test : 0.45677492022514343 \n",
      "311000 steps train : 0.4578861594200134 test : 0.4661787152290344 \n",
      "312000 steps train : 0.4558883011341095 test : 0.461698979139328 \n",
      "313000 steps train : 0.4591978192329407 test : 0.46717193722724915 \n",
      "314000 steps train : 0.46788036823272705 test : 0.47469332814216614 \n",
      "315000 steps train : 0.4615747630596161 test : 0.470927894115448 \n",
      "316000 steps train : 0.4707537293434143 test : 0.47916945815086365 \n",
      "317000 steps train : 0.44687408208847046 test : 0.4540274441242218 \n",
      "318000 steps train : 0.45938345789909363 test : 0.4676429331302643 \n",
      "319000 steps train : 0.4538908004760742 test : 0.46119096875190735 \n",
      "320000 steps train : 0.46364831924438477 test : 0.47182518243789673 \n",
      "321000 steps train : 0.4550936222076416 test : 0.4619167745113373 \n",
      "322000 steps train : 0.45210957527160645 test : 0.4600273072719574 \n",
      "323000 steps train : 0.4581611156463623 test : 0.4665961265563965 \n",
      "324000 steps train : 0.45725497603416443 test : 0.4631471037864685 \n",
      "325000 steps train : 0.45079123973846436 test : 0.4581238925457001 \n",
      "326000 steps train : 0.4545397162437439 test : 0.461340993642807 \n",
      "327000 steps train : 0.4667292833328247 test : 0.475155234336853 \n",
      "328000 steps train : 0.45936378836631775 test : 0.46711429953575134 \n",
      "329000 steps train : 0.4626888036727905 test : 0.46986624598503113 \n",
      "330000 steps train : 0.4480880796909332 test : 0.45534154772758484 \n",
      "331000 steps train : 0.4509165287017822 test : 0.45914581418037415 \n",
      "332000 steps train : 0.45698803663253784 test : 0.46540969610214233 \n",
      "333000 steps train : 0.45418882369995117 test : 0.4627111554145813 \n",
      "334000 steps train : 0.46000176668167114 test : 0.466869592666626 \n",
      "335000 steps train : 0.44392433762550354 test : 0.4513126313686371 \n",
      "336000 steps train : 0.45038408041000366 test : 0.45800071954727173 \n",
      "337000 steps train : 0.46119970083236694 test : 0.4697491526603699 \n",
      "338000 steps train : 0.462555468082428 test : 0.46965089440345764 \n",
      "339000 steps train : 0.4442434310913086 test : 0.45164477825164795 \n",
      "340000 steps train : 0.460390567779541 test : 0.46896886825561523 \n",
      "341000 steps train : 0.475600928068161 test : 0.48357903957366943 \n",
      "342000 steps train : 0.45291876792907715 test : 0.46152517199516296 \n",
      "343000 steps train : 0.4516223073005676 test : 0.4595724642276764 \n",
      "344000 steps train : 0.4540556073188782 test : 0.46120330691337585 \n",
      "345000 steps train : 0.48619192838668823 test : 0.49361392855644226 \n",
      "346000 steps train : 0.44970524311065674 test : 0.45859548449516296 \n",
      "347000 steps train : 0.46393516659736633 test : 0.4725295901298523 \n",
      "348000 steps train : 0.515557050704956 test : 0.522402822971344 \n",
      "349000 steps train : 0.4678924083709717 test : 0.47610121965408325 \n",
      "350000 steps train : 0.4896211326122284 test : 0.4973967373371124 \n",
      "351000 steps train : 0.4567137658596039 test : 0.46366241574287415 \n",
      "352000 steps train : 0.4535616338253021 test : 0.46101394295692444 \n",
      "353000 steps train : 0.45971280336380005 test : 0.4667656123638153 \n",
      "354000 steps train : 0.4622343182563782 test : 0.46871769428253174 \n",
      "355000 steps train : 0.461597740650177 test : 0.46908020973205566 \n",
      "356000 steps train : 0.46231356263160706 test : 0.47001102566719055 \n",
      "357000 steps train : 0.4534451365470886 test : 0.462863564491272 \n",
      "358000 steps train : 0.44512733817100525 test : 0.4531600773334503 \n",
      "359000 steps train : 0.45909857749938965 test : 0.46799054741859436 \n",
      "360000 steps train : 0.4574756622314453 test : 0.4643818438053131 \n",
      "361000 steps train : 0.4522913098335266 test : 0.45949798822402954 \n",
      "362000 steps train : 0.45723703503608704 test : 0.46409720182418823 \n",
      "363000 steps train : 0.4631955027580261 test : 0.4732446074485779 \n",
      "364000 steps train : 0.45371973514556885 test : 0.46323031187057495 \n",
      "365000 steps train : 0.45623406767845154 test : 0.4650607109069824 \n",
      "366000 steps train : 0.47796085476875305 test : 0.48514068126678467 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367000 steps train : 0.4521274268627167 test : 0.4598979651927948 \n",
      "368000 steps train : 0.4593597948551178 test : 0.467534601688385 \n",
      "369000 steps train : 0.4623554050922394 test : 0.47109371423721313 \n",
      "370000 steps train : 0.4393250346183777 test : 0.4463977813720703 \n",
      "371000 steps train : 0.4519519507884979 test : 0.460208535194397 \n",
      "372000 steps train : 0.4374418556690216 test : 0.4448627531528473 \n",
      "373000 steps train : 0.45389196276664734 test : 0.46187055110931396 \n",
      "374000 steps train : 0.4553121030330658 test : 0.46177437901496887 \n",
      "375000 steps train : 0.4495924413204193 test : 0.456930935382843 \n",
      "376000 steps train : 0.4486709535121918 test : 0.45656004548072815 \n",
      "377000 steps train : 0.47739699482917786 test : 0.4871324598789215 \n",
      "378000 steps train : 0.4482876658439636 test : 0.45703986287117004 \n",
      "379000 steps train : 0.4481777250766754 test : 0.4570070505142212 \n",
      "380000 steps train : 0.44959524273872375 test : 0.4566214382648468 \n",
      "381000 steps train : 0.45205339789390564 test : 0.46076205372810364 \n",
      "382000 steps train : 0.4481426775455475 test : 0.4552134871482849 \n",
      "383000 steps train : 0.4409368634223938 test : 0.4470668435096741 \n",
      "384000 steps train : 0.46476900577545166 test : 0.4717359244823456 \n",
      "385000 steps train : 0.4377546012401581 test : 0.44399043917655945 \n",
      "386000 steps train : 0.4474375247955322 test : 0.4560406506061554 \n",
      "387000 steps train : 0.4417537450790405 test : 0.4513546824455261 \n",
      "388000 steps train : 0.4373251795768738 test : 0.4438040852546692 \n",
      "389000 steps train : 0.44557875394821167 test : 0.4530528485774994 \n",
      "390000 steps train : 0.4421890377998352 test : 0.45006823539733887 \n",
      "391000 steps train : 0.4463576674461365 test : 0.45381611585617065 \n",
      "392000 steps train : 0.4568284749984741 test : 0.46526166796684265 \n",
      "393000 steps train : 0.45688214898109436 test : 0.4630511701107025 \n",
      "394000 steps train : 0.44505634903907776 test : 0.4531552195549011 \n",
      "395000 steps train : 0.45171022415161133 test : 0.46209976077079773 \n",
      "396000 steps train : 0.45052820444107056 test : 0.45819029211997986 \n",
      "397000 steps train : 0.4543146789073944 test : 0.46174487471580505 \n",
      "398000 steps train : 0.47156691551208496 test : 0.4791947901248932 \n",
      "399000 steps train : 0.46205535531044006 test : 0.4698614478111267 \n",
      "400000 steps train : 0.4560522139072418 test : 0.465425968170166 \n",
      "401000 steps train : 0.4560925364494324 test : 0.46360254287719727 \n",
      "402000 steps train : 0.4559021294116974 test : 0.4636519253253937 \n",
      "403000 steps train : 0.450355589389801 test : 0.4584210216999054 \n",
      "404000 steps train : 0.4501648247241974 test : 0.4593120217323303 \n",
      "405000 steps train : 0.4807570278644562 test : 0.4873662292957306 \n",
      "406000 steps train : 0.4393315613269806 test : 0.4461381137371063 \n",
      "407000 steps train : 0.45085033774375916 test : 0.45805856585502625 \n",
      "408000 steps train : 0.4530160129070282 test : 0.4613517224788666 \n",
      "409000 steps train : 0.44618964195251465 test : 0.4535979926586151 \n",
      "410000 steps train : 0.4416840374469757 test : 0.4474451243877411 \n",
      "411000 steps train : 0.49267569184303284 test : 0.49809467792510986 \n",
      "412000 steps train : 0.4628879725933075 test : 0.47131314873695374 \n",
      "413000 steps train : 0.45206260681152344 test : 0.46097928285598755 \n",
      "414000 steps train : 0.4474918246269226 test : 0.45680859684944153 \n",
      "415000 steps train : 0.4416835606098175 test : 0.44946861267089844 \n",
      "416000 steps train : 0.44354814291000366 test : 0.4513843059539795 \n",
      "417000 steps train : 0.4560031294822693 test : 0.4634847640991211 \n",
      "418000 steps train : 0.44744864106178284 test : 0.4537433087825775 \n",
      "419000 steps train : 0.44852858781814575 test : 0.45701175928115845 \n",
      "420000 steps train : 0.44835057854652405 test : 0.45639583468437195 \n",
      "421000 steps train : 0.45840543508529663 test : 0.46744102239608765 \n",
      "422000 steps train : 0.4441668689250946 test : 0.4513270854949951 \n",
      "423000 steps train : 0.4662655293941498 test : 0.4733794033527374 \n",
      "424000 steps train : 0.4606158435344696 test : 0.4689936935901642 \n",
      "425000 steps train : 0.4445042014122009 test : 0.45244354009628296 \n",
      "426000 steps train : 0.46038347482681274 test : 0.46772125363349915 \n",
      "427000 steps train : 0.4555887281894684 test : 0.46202853322029114 \n",
      "428000 steps train : 0.4493269920349121 test : 0.45636266469955444 \n",
      "429000 steps train : 0.4895537197589874 test : 0.49590829014778137 \n",
      "430000 steps train : 0.45764386653900146 test : 0.4637366533279419 \n",
      "431000 steps train : 0.4505044221878052 test : 0.4570338726043701 \n",
      "432000 steps train : 0.44347918033599854 test : 0.44942551851272583 \n",
      "433000 steps train : 0.4866476356983185 test : 0.4957214891910553 \n",
      "434000 steps train : 0.46163424849510193 test : 0.47045662999153137 \n",
      "435000 steps train : 0.4436233341693878 test : 0.451467365026474 \n",
      "436000 steps train : 0.48545244336128235 test : 0.4928983449935913 \n",
      "437000 steps train : 0.46405085921287537 test : 0.47182798385620117 \n",
      "438000 steps train : 0.47086864709854126 test : 0.47792762517929077 \n",
      "439000 steps train : 0.4586264193058014 test : 0.4681665599346161 \n",
      "440000 steps train : 0.4401928186416626 test : 0.4474250376224518 \n",
      "441000 steps train : 0.4377802610397339 test : 0.4461554288864136 \n",
      "442000 steps train : 0.45183929800987244 test : 0.4588824212551117 \n",
      "443000 steps train : 0.46529388427734375 test : 0.47521838545799255 \n",
      "444000 steps train : 0.45686909556388855 test : 0.46315643191337585 \n",
      "445000 steps train : 0.4484046399593353 test : 0.45655879378318787 \n",
      "446000 steps train : 0.4450215995311737 test : 0.4519108831882477 \n",
      "447000 steps train : 0.45021572709083557 test : 0.45826128125190735 \n",
      "448000 steps train : 0.45626339316368103 test : 0.464140921831131 \n",
      "449000 steps train : 0.4477718770503998 test : 0.4550774395465851 \n",
      "450000 steps train : 0.45453622937202454 test : 0.4623192548751831 \n",
      "451000 steps train : 0.4423265755176544 test : 0.4505997598171234 \n",
      "452000 steps train : 0.451897531747818 test : 0.45972728729248047 \n",
      "453000 steps train : 0.441722571849823 test : 0.4484199285507202 \n",
      "454000 steps train : 0.4440303146839142 test : 0.45077693462371826 \n",
      "455000 steps train : 0.458609938621521 test : 0.46490105986595154 \n",
      "456000 steps train : 0.45283615589141846 test : 0.4590238332748413 \n",
      "457000 steps train : 0.5233576893806458 test : 0.5329461693763733 \n",
      "458000 steps train : 0.45374035835266113 test : 0.4628794193267822 \n",
      "459000 steps train : 0.43029099702835083 test : 0.4388354420661926 \n",
      "460000 steps train : 0.4378381669521332 test : 0.44598251581192017 \n",
      "461000 steps train : 0.4330005645751953 test : 0.4413236677646637 \n",
      "462000 steps train : 0.4465901553630829 test : 0.45328429341316223 \n",
      "463000 steps train : 0.46429765224456787 test : 0.4725552797317505 \n",
      "464000 steps train : 0.4552743136882782 test : 0.4627716541290283 \n",
      "465000 steps train : 0.4645598828792572 test : 0.47065970301628113 \n",
      "466000 steps train : 0.449590265750885 test : 0.45743799209594727 \n",
      "467000 steps train : 0.451584130525589 test : 0.4616445302963257 \n",
      "468000 steps train : 0.44050148129463196 test : 0.44796112179756165 \n",
      "469000 steps train : 0.4438105821609497 test : 0.4498283267021179 \n",
      "470000 steps train : 0.4577452838420868 test : 0.4658515155315399 \n",
      "471000 steps train : 0.4445314407348633 test : 0.45266881585121155 \n",
      "472000 steps train : 0.4420356750488281 test : 0.4495672583580017 \n",
      "473000 steps train : 0.43744057416915894 test : 0.4448835849761963 \n",
      "474000 steps train : 0.4419972598552704 test : 0.4492778778076172 \n",
      "475000 steps train : 0.44632333517074585 test : 0.454582542181015 \n",
      "476000 steps train : 0.43816372752189636 test : 0.44582223892211914 \n",
      "477000 steps train : 0.4390799403190613 test : 0.4468078017234802 \n",
      "478000 steps train : 0.4303414821624756 test : 0.43765053153038025 \n",
      "479000 steps train : 0.43104127049446106 test : 0.4383695423603058 \n",
      "480000 steps train : 0.4480896592140198 test : 0.45732665061950684 \n",
      "481000 steps train : 0.44795405864715576 test : 0.4554216265678406 \n",
      "482000 steps train : 0.4513079822063446 test : 0.45732977986335754 \n",
      "483000 steps train : 0.4388621151447296 test : 0.4461694061756134 \n",
      "484000 steps train : 0.45603296160697937 test : 0.4629334509372711 \n",
      "485000 steps train : 0.43473881483078003 test : 0.4420968294143677 \n",
      "486000 steps train : 0.4556311070919037 test : 0.4618808627128601 \n",
      "487000 steps train : 0.45431002974510193 test : 0.46219825744628906 \n",
      "488000 steps train : 0.4831790328025818 test : 0.49101221561431885 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489000 steps train : 0.43644797801971436 test : 0.4424765110015869 \n",
      "490000 steps train : 0.4493494927883148 test : 0.4606934189796448 \n",
      "491000 steps train : 0.4434003233909607 test : 0.4502854347229004 \n",
      "492000 steps train : 0.44620242714881897 test : 0.4539327025413513 \n",
      "493000 steps train : 0.44938910007476807 test : 0.45708975195884705 \n",
      "494000 steps train : 0.44615161418914795 test : 0.45317313075065613 \n",
      "495000 steps train : 0.44041740894317627 test : 0.44729411602020264 \n",
      "496000 steps train : 0.43789052963256836 test : 0.4444260001182556 \n",
      "497000 steps train : 0.428270548582077 test : 0.43492186069488525 \n",
      "498000 steps train : 0.4627602696418762 test : 0.46905654668807983 \n",
      "499000 steps train : 0.43651604652404785 test : 0.44486844539642334 \n",
      "500000 steps train : 0.447733610868454 test : 0.45428621768951416 \n",
      "501000 steps train : 0.4276636838912964 test : 0.43657487630844116 \n",
      "502000 steps train : 0.4440375864505768 test : 0.45234817266464233 \n",
      "503000 steps train : 0.4325515031814575 test : 0.44006142020225525 \n",
      "504000 steps train : 0.45432421565055847 test : 0.46103033423423767 \n",
      "505000 steps train : 0.46081098914146423 test : 0.4652555584907532 \n",
      "506000 steps train : 0.44580307602882385 test : 0.45466387271881104 \n",
      "507000 steps train : 0.43004506826400757 test : 0.43795478343963623 \n",
      "508000 steps train : 0.4297392666339874 test : 0.4365893006324768 \n",
      "509000 steps train : 0.92738276720047 test : 0.9728169441223145 \n",
      "510000 steps train : 0.461577445268631 test : 0.46862778067588806 \n",
      "511000 steps train : 0.44316720962524414 test : 0.45069751143455505 \n",
      "512000 steps train : 0.4351106286048889 test : 0.44135037064552307 \n",
      "513000 steps train : 0.45597007870674133 test : 0.4612102806568146 \n",
      "514000 steps train : 0.4555278718471527 test : 0.4624471664428711 \n",
      "515000 steps train : 0.44217464327812195 test : 0.4523375630378723 \n",
      "516000 steps train : 0.4437903165817261 test : 0.4517991244792938 \n",
      "517000 steps train : 0.4308526813983917 test : 0.4379377067089081 \n",
      "518000 steps train : 0.43718644976615906 test : 0.44522374868392944 \n",
      "519000 steps train : 0.45151570439338684 test : 0.4594242572784424 \n",
      "520000 steps train : 0.4354729950428009 test : 0.442747563123703 \n",
      "521000 steps train : 0.4475138783454895 test : 0.45461273193359375 \n",
      "522000 steps train : 0.45139679312705994 test : 0.4596319794654846 \n",
      "523000 steps train : 0.43684452772140503 test : 0.4455866813659668 \n",
      "524000 steps train : 0.44799578189849854 test : 0.4574429988861084 \n",
      "525000 steps train : 0.44123178720474243 test : 0.4491100311279297 \n",
      "526000 steps train : 0.4831073582172394 test : 0.4895174503326416 \n",
      "527000 steps train : 0.45700010657310486 test : 0.4650842547416687 \n",
      "528000 steps train : 0.4420604407787323 test : 0.44993144273757935 \n",
      "529000 steps train : 0.44322940707206726 test : 0.4506514072418213 \n",
      "530000 steps train : 0.4435388147830963 test : 0.4520808160305023 \n",
      "531000 steps train : 0.45378634333610535 test : 0.46126794815063477 \n",
      "532000 steps train : 0.45084232091903687 test : 0.45683860778808594 \n",
      "533000 steps train : 0.43488362431526184 test : 0.44231075048446655 \n",
      "534000 steps train : 0.4386790096759796 test : 0.44673600792884827 \n",
      "535000 steps train : 0.4279228746891022 test : 0.4361444413661957 \n",
      "536000 steps train : 0.4330882728099823 test : 0.44106554985046387 \n",
      "537000 steps train : 0.4715960621833801 test : 0.4807344079017639 \n",
      "538000 steps train : 0.46006229519844055 test : 0.46597760915756226 \n",
      "539000 steps train : 0.44718247652053833 test : 0.4507213830947876 \n",
      "540000 steps train : 1.0621140003204346 test : 1.2509368658065796 \n",
      "541000 steps train : 0.43264082074165344 test : 0.4404580891132355 \n",
      "542000 steps train : 0.4449272155761719 test : 0.451928973197937 \n",
      "543000 steps train : 0.4414072036743164 test : 0.4484545886516571 \n",
      "544000 steps train : 0.4469723403453827 test : 0.4559113681316376 \n",
      "545000 steps train : 0.44046828150749207 test : 0.4482896327972412 \n",
      "546000 steps train : 0.46617645025253296 test : 0.47540077567100525 \n",
      "547000 steps train : 0.44254767894744873 test : 0.4496992826461792 \n",
      "548000 steps train : 0.4504232704639435 test : 0.4606934189796448 \n",
      "549000 steps train : 0.46696197986602783 test : 0.47378677129745483 \n",
      "550000 steps train : 0.43834125995635986 test : 0.44584760069847107 \n",
      "551000 steps train : 0.4513934552669525 test : 0.46035465598106384 \n",
      "552000 steps train : 0.4564732313156128 test : 0.4613802433013916 \n",
      "553000 steps train : 0.4406041204929352 test : 0.4482848644256592 \n",
      "554000 steps train : 0.4462431073188782 test : 0.454190194606781 \n",
      "555000 steps train : 0.45785385370254517 test : 0.4645940065383911 \n",
      "556000 steps train : 0.4402117431163788 test : 0.44690924882888794 \n",
      "557000 steps train : 0.44839349389076233 test : 0.4562050998210907 \n",
      "558000 steps train : 0.4450904428958893 test : 0.4521406292915344 \n",
      "559000 steps train : 0.4485096335411072 test : 0.4554376006126404 \n",
      "560000 steps train : 0.45476606488227844 test : 0.4618304967880249 \n",
      "561000 steps train : 0.4390069842338562 test : 0.44597524404525757 \n",
      "562000 steps train : 0.4946129322052002 test : 0.4890574514865875 \n",
      "563000 steps train : 0.5003348588943481 test : 0.5461894869804382 \n",
      "564000 steps train : 0.7549532651901245 test : 0.6592451333999634 \n",
      "565000 steps train : 0.47655072808265686 test : 0.4731636941432953 \n",
      "566000 steps train : 0.4756483733654022 test : 0.47444844245910645 \n",
      "567000 steps train : 0.44683367013931274 test : 0.44831904768943787 \n",
      "568000 steps train : 0.45348480343818665 test : 0.4557996690273285 \n",
      "569000 steps train : 0.44102856516838074 test : 0.4626066982746124 \n",
      "570000 steps train : 0.43242502212524414 test : 0.43961432576179504 \n",
      "571000 steps train : 0.4493851363658905 test : 0.45838043093681335 \n",
      "572000 steps train : 0.4672558307647705 test : 0.4649932384490967 \n",
      "573000 steps train : 0.5368554592132568 test : 0.5468877553939819 \n",
      "574000 steps train : 0.4630226492881775 test : 0.47282087802886963 \n",
      "575000 steps train : 0.4489594101905823 test : 0.45176541805267334 \n",
      "576000 steps train : 0.46899113059043884 test : 0.4760715067386627 \n",
      "577000 steps train : 0.5091204643249512 test : 0.523863673210144 \n",
      "578000 steps train : 0.464659720659256 test : 0.4702959954738617 \n",
      "579000 steps train : 0.4732941687107086 test : 0.481573224067688 \n",
      "580000 steps train : 0.4574281573295593 test : 0.4642997086048126 \n",
      "581000 steps train : 0.4396131932735443 test : 0.44740259647369385 \n",
      "582000 steps train : 0.43432262539863586 test : 0.44322577118873596 \n",
      "583000 steps train : 0.4471985697746277 test : 0.4551703929901123 \n",
      "584000 steps train : 0.4475065767765045 test : 0.4569300413131714 \n",
      "585000 steps train : 0.43410348892211914 test : 0.44290775060653687 \n",
      "586000 steps train : 0.44062721729278564 test : 0.450008749961853 \n",
      "587000 steps train : 0.4266382157802582 test : 0.4361463189125061 \n",
      "588000 steps train : 0.4420722723007202 test : 0.4504472315311432 \n",
      "589000 steps train : 0.44982439279556274 test : 0.45805439352989197 \n",
      "590000 steps train : 0.44958487153053284 test : 0.4577496647834778 \n",
      "591000 steps train : 0.4341645836830139 test : 0.4415123760700226 \n",
      "592000 steps train : 0.433071494102478 test : 0.4389931261539459 \n",
      "593000 steps train : 0.4404281675815582 test : 0.44780686497688293 \n",
      "594000 steps train : 0.43096253275871277 test : 0.4374639093875885 \n",
      "595000 steps train : 0.43971559405326843 test : 0.4489309787750244 \n",
      "596000 steps train : 0.42890462279319763 test : 0.43701136112213135 \n",
      "597000 steps train : 0.44297873973846436 test : 0.4494726359844208 \n",
      "598000 steps train : 0.447062611579895 test : 0.4542504847049713 \n",
      "599000 steps train : 0.44176945090293884 test : 0.44934263825416565 \n",
      "600000 steps train : 0.4383610486984253 test : 0.44621479511260986 \n",
      "601000 steps train : 0.4264814555644989 test : 0.43453386425971985 \n",
      "602000 steps train : 0.4367488622665405 test : 0.44450753927230835 \n",
      "603000 steps train : 0.4361848533153534 test : 0.44424131512641907 \n",
      "604000 steps train : 0.4467763900756836 test : 0.45462074875831604 \n",
      "605000 steps train : 0.444898784160614 test : 0.4531993567943573 \n",
      "606000 steps train : 0.43498876690864563 test : 0.4421372413635254 \n",
      "607000 steps train : 0.4322517216205597 test : 0.4390469491481781 \n",
      "608000 steps train : 0.4506387412548065 test : 0.4593486189842224 \n",
      "609000 steps train : 0.46366941928863525 test : 0.47136688232421875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610000 steps train : 0.4333786368370056 test : 0.44530239701271057 \n",
      "611000 steps train : 0.4441394507884979 test : 0.4507763087749481 \n",
      "612000 steps train : 0.5051832795143127 test : 0.4914029836654663 \n",
      "613000 steps train : 2.863320827484131 test : 4.091821193695068 \n",
      "614000 steps train : 11.112788200378418 test : 9.990782737731934 \n",
      "615000 steps train : 0.5626762509346008 test : 0.5178737044334412 \n",
      "616000 steps train : 0.5360408425331116 test : 0.5097832083702087 \n",
      "617000 steps train : 0.5473315715789795 test : 0.509136974811554 \n",
      "618000 steps train : 0.5431267619132996 test : 0.4982922375202179 \n",
      "619000 steps train : 0.48897990584373474 test : 0.5022926926612854 \n",
      "620000 steps train : 0.47540897130966187 test : 0.4749203324317932 \n",
      "621000 steps train : 0.46014997363090515 test : 0.4650205373764038 \n",
      "622000 steps train : 0.44327089190483093 test : 0.45122310519218445 \n",
      "623000 steps train : 0.4632148742675781 test : 0.47135400772094727 \n",
      "624000 steps train : 0.45279455184936523 test : 0.4463898539543152 \n",
      "625000 steps train : 0.43622803688049316 test : 0.44833049178123474 \n",
      "626000 steps train : 0.45897138118743896 test : 0.4672498106956482 \n",
      "627000 steps train : 0.4535910189151764 test : 0.4601288437843323 \n",
      "628000 steps train : 0.44337132573127747 test : 0.45126786828041077 \n",
      "629000 steps train : 0.46255671977996826 test : 0.49557942152023315 \n",
      "630000 steps train : 0.435380756855011 test : 0.44397228956222534 \n",
      "631000 steps train : 0.43341493606567383 test : 0.4371205270290375 \n",
      "632000 steps train : 0.4617893397808075 test : 0.4721159338951111 \n",
      "633000 steps train : 0.4470330476760864 test : 0.4494243562221527 \n",
      "634000 steps train : 0.44166263937950134 test : 0.4416665732860565 \n",
      "635000 steps train : 0.6128864884376526 test : 0.737012505531311 \n",
      "636000 steps train : 0.434048593044281 test : 0.442269504070282 \n",
      "637000 steps train : 0.43950435519218445 test : 0.44940537214279175 \n",
      "638000 steps train : 0.43656498193740845 test : 0.4446364939212799 \n",
      "639000 steps train : 0.4432791471481323 test : 0.4503638744354248 \n",
      "640000 steps train : 0.44923025369644165 test : 0.4567343592643738 \n",
      "641000 steps train : 0.44891035556793213 test : 0.45594558119773865 \n",
      "642000 steps train : 0.5526837706565857 test : 0.5945131182670593 \n",
      "643000 steps train : 0.51032555103302 test : 0.6631326675415039 \n",
      "644000 steps train : 0.4692270755767822 test : 0.47719717025756836 \n",
      "645000 steps train : 0.46481621265411377 test : 0.4992978870868683 \n",
      "646000 steps train : 0.5829721689224243 test : 0.44664302468299866 \n",
      "647000 steps train : 0.7887577414512634 test : 3.750098705291748 \n",
      "648000 steps train : 2.318115711212158 test : 7.202051162719727 \n",
      "649000 steps train : 3.266655683517456 test : 1.6010925769805908 \n",
      "650000 steps train : 4.965222358703613 test : 1.5027570724487305 \n",
      "651000 steps train : 41.496612548828125 test : 26.577096939086914 \n",
      "652000 steps train : 13.675873756408691 test : 10.47885513305664 \n",
      "653000 steps train : 5.064188003540039 test : 3.1938693523406982 \n",
      "654000 steps train : 4.5820794105529785 test : 3.36724591255188 \n",
      "655000 steps train : 2.0677835941314697 test : 0.7919629216194153 \n",
      "656000 steps train : 0.6782809495925903 test : 0.5364042520523071 \n",
      "657000 steps train : 0.7202388048171997 test : 0.5282185673713684 \n",
      "658000 steps train : 0.6283388137817383 test : 0.6654878258705139 \n",
      "659000 steps train : 0.5888100266456604 test : 0.6209051012992859 \n",
      "660000 steps train : 0.6322641968727112 test : 0.816057026386261 \n",
      "661000 steps train : 0.5334540605545044 test : 0.6558806896209717 \n",
      "662000 steps train : 0.5080546736717224 test : 0.5578765869140625 \n",
      "663000 steps train : 0.5111021399497986 test : 0.5772377848625183 \n",
      "664000 steps train : 0.4914160370826721 test : 0.5428056120872498 \n",
      "665000 steps train : 0.4813823103904724 test : 0.5020143389701843 \n",
      "666000 steps train : 0.4681433141231537 test : 0.47346487641334534 \n",
      "667000 steps train : 0.4559110999107361 test : 0.4616791903972626 \n",
      "668000 steps train : 0.4612809419631958 test : 0.455242395401001 \n",
      "669000 steps train : 0.45824313163757324 test : 0.4852343797683716 \n",
      "670000 steps train : 0.43806642293930054 test : 0.44974634051322937 \n",
      "671000 steps train : 0.4386001527309418 test : 0.46320483088493347 \n",
      "672000 steps train : 0.4452514052391052 test : 0.4587922990322113 \n",
      "673000 steps train : 0.45223432779312134 test : 0.45699357986450195 \n",
      "674000 steps train : 0.4490910768508911 test : 0.4555419683456421 \n",
      "675000 steps train : 0.47424015402793884 test : 0.49539071321487427 \n",
      "676000 steps train : 0.44265276193618774 test : 0.449134886264801 \n",
      "677000 steps train : 0.4447103440761566 test : 0.4504522979259491 \n",
      "678000 steps train : 0.47461599111557007 test : 0.4868175983428955 \n",
      "679000 steps train : 0.47236770391464233 test : 0.46809622645378113 \n",
      "680000 steps train : 0.4427233934402466 test : 0.4507501721382141 \n",
      "681000 steps train : 0.43979108333587646 test : 0.4495476186275482 \n",
      "682000 steps train : 0.4583720862865448 test : 0.46278372406959534 \n",
      "683000 steps train : 0.44584277272224426 test : 0.4578467309474945 \n",
      "684000 steps train : 0.44819024205207825 test : 0.45698249340057373 \n",
      "685000 steps train : 0.44252678751945496 test : 0.4556907117366791 \n",
      "686000 steps train : 0.4374684691429138 test : 0.4453050196170807 \n",
      "687000 steps train : 0.45411401987075806 test : 0.4640252888202667 \n",
      "688000 steps train : 0.44917842745780945 test : 0.4570545554161072 \n",
      "689000 steps train : 0.4364511966705322 test : 0.4472733438014984 \n",
      "690000 steps train : 0.4359113574028015 test : 0.4376746714115143 \n",
      "691000 steps train : 0.5100690722465515 test : 0.6774084568023682 \n",
      "692000 steps train : 0.5919414758682251 test : 0.5687598586082458 \n",
      "693000 steps train : 0.7198835611343384 test : 0.9056178331375122 \n",
      "694000 steps train : 1.0240226984024048 test : 0.8278788924217224 \n",
      "695000 steps train : 1.251489281654358 test : 2.4329278469085693 \n",
      "696000 steps train : 1.5719562768936157 test : 5.611246585845947 \n",
      "697000 steps train : 0.6934050917625427 test : 0.4549816846847534 \n",
      "698000 steps train : 0.9233837127685547 test : 0.6649999618530273 \n",
      "699000 steps train : 0.9585955142974854 test : 0.6072828769683838 \n",
      "700000 steps train : 0.9355087876319885 test : 0.9533604383468628 \n",
      "701000 steps train : 4.314913272857666 test : 4.052738666534424 \n",
      "702000 steps train : 0.5664275884628296 test : 0.5483853220939636 \n",
      "703000 steps train : 0.5242211818695068 test : 0.6151427626609802 \n",
      "704000 steps train : 0.47333911061286926 test : 0.4787091016769409 \n",
      "705000 steps train : 0.4651305079460144 test : 0.47251611948013306 \n",
      "706000 steps train : 0.46052876114845276 test : 0.46317094564437866 \n",
      "707000 steps train : 0.45595186948776245 test : 0.46063435077667236 \n",
      "708000 steps train : 0.4418918490409851 test : 0.4502521753311157 \n",
      "709000 steps train : 0.44681504368782043 test : 0.4531857669353485 \n",
      "710000 steps train : 0.45911523699760437 test : 0.4546809494495392 \n",
      "711000 steps train : 0.46554744243621826 test : 0.5057149529457092 \n",
      "712000 steps train : 0.477223664522171 test : 0.4714714288711548 \n",
      "713000 steps train : 0.44346052408218384 test : 0.45193031430244446 \n",
      "714000 steps train : 0.43557208776474 test : 0.4608996510505676 \n",
      "715000 steps train : 0.48027172684669495 test : 0.48314276337623596 \n",
      "716000 steps train : 0.46502208709716797 test : 0.4624904692173004 \n",
      "717000 steps train : 0.5411643981933594 test : 0.5429245233535767 \n",
      "718000 steps train : 0.49025580286979675 test : 0.5006324052810669 \n",
      "719000 steps train : 0.8611212968826294 test : 0.8940538763999939 \n",
      "720000 steps train : 1.1793363094329834 test : 0.7534834146499634 \n",
      "721000 steps train : 0.6962600946426392 test : 1.900849461555481 \n",
      "722000 steps train : 1.4037007093429565 test : 1.3169426918029785 \n",
      "723000 steps train : 0.9962971210479736 test : 0.5418761372566223 \n",
      "724000 steps train : 0.7560065984725952 test : 0.6258615851402283 \n",
      "725000 steps train : 0.5817524194717407 test : 0.47080206871032715 \n",
      "726000 steps train : 1.3257811069488525 test : 2.252865791320801 \n",
      "727000 steps train : 0.8954926133155823 test : 0.5606707334518433 \n",
      "728000 steps train : 0.7147804498672485 test : 1.529865026473999 \n",
      "729000 steps train : 0.5069469809532166 test : 0.48205870389938354 \n",
      "730000 steps train : 0.47877663373947144 test : 0.4776555001735687 \n",
      "731000 steps train : 0.7139219641685486 test : 1.0158941745758057 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732000 steps train : 0.5298393368721008 test : 0.4813988208770752 \n",
      "733000 steps train : 0.9750248789787292 test : 0.5368849039077759 \n",
      "734000 steps train : 0.7712063193321228 test : 0.47783249616622925 \n",
      "735000 steps train : 0.634489893913269 test : 0.5024768710136414 \n",
      "736000 steps train : 0.7017548680305481 test : 0.5094636082649231 \n",
      "737000 steps train : 0.608181893825531 test : 0.47169506549835205 \n",
      "738000 steps train : 0.4719405770301819 test : 0.48647332191467285 \n",
      "739000 steps train : 2.6221022605895996 test : 3.4778318405151367 \n",
      "740000 steps train : 0.5597352981567383 test : 0.5343151092529297 \n",
      "741000 steps train : 0.518921434879303 test : 0.4652046859264374 \n",
      "742000 steps train : 0.46944108605384827 test : 0.48544445633888245 \n",
      "743000 steps train : 0.45536360144615173 test : 0.46340563893318176 \n",
      "744000 steps train : 0.4547099173069 test : 0.46336230635643005 \n",
      "745000 steps train : 0.44390952587127686 test : 0.4532374441623688 \n",
      "746000 steps train : 0.45963650941848755 test : 0.4655602276325226 \n",
      "747000 steps train : 0.4544910788536072 test : 0.4624738395214081 \n",
      "748000 steps train : 0.44706112146377563 test : 0.4545034170150757 \n",
      "749000 steps train : 0.43850356340408325 test : 0.44889625906944275 \n",
      "750000 steps train : 0.4421825706958771 test : 0.46417009830474854 \n",
      "751000 steps train : 0.4414116144180298 test : 0.4497712254524231 \n",
      "752000 steps train : 0.4328707456588745 test : 0.4411129355430603 \n",
      "753000 steps train : 0.4410432279109955 test : 0.4492727220058441 \n",
      "754000 steps train : 0.432058185338974 test : 0.4403553307056427 \n",
      "755000 steps train : 0.4442366063594818 test : 0.45200487971305847 \n",
      "756000 steps train : 0.44218870997428894 test : 0.44857901334762573 \n",
      "757000 steps train : 0.45032379031181335 test : 0.4590812027454376 \n",
      "758000 steps train : 0.44305339455604553 test : 0.45335015654563904 \n",
      "759000 steps train : 0.4291723966598511 test : 0.43740296363830566 \n",
      "760000 steps train : 0.46461471915245056 test : 0.47513267397880554 \n",
      "761000 steps train : 0.43376627564430237 test : 0.4420139789581299 \n",
      "762000 steps train : 0.4718894958496094 test : 0.4888453185558319 \n",
      "763000 steps train : 0.42972883582115173 test : 0.43999186158180237 \n",
      "764000 steps train : 0.4303721487522125 test : 0.4392704963684082 \n",
      "765000 steps train : 0.44033506512641907 test : 0.4981301426887512 \n",
      "766000 steps train : 0.43915313482284546 test : 0.452350378036499 \n",
      "767000 steps train : 0.4843803942203522 test : 0.4600152373313904 \n",
      "768000 steps train : 0.42713022232055664 test : 0.49038809537887573 \n",
      "769000 steps train : 0.500012218952179 test : 0.5130874514579773 \n",
      "770000 steps train : 0.4652697741985321 test : 0.47992926836013794 \n",
      "771000 steps train : 0.4535028636455536 test : 0.6289173364639282 \n",
      "772000 steps train : 0.44924190640449524 test : 0.5142093300819397 \n",
      "773000 steps train : 0.45689699053764343 test : 0.47136127948760986 \n",
      "774000 steps train : 0.47745269536972046 test : 0.48701003193855286 \n",
      "775000 steps train : 0.4343561828136444 test : 0.4413001835346222 \n",
      "776000 steps train : 0.8192688822746277 test : 0.48497697710990906 \n",
      "777000 steps train : 0.42594656348228455 test : 0.4442514181137085 \n",
      "778000 steps train : 0.4453730583190918 test : 0.5317955613136292 \n",
      "779000 steps train : 0.43623676896095276 test : 0.44717881083488464 \n",
      "780000 steps train : 0.6390573382377625 test : 0.6321650743484497 \n",
      "781000 steps train : 0.7553002238273621 test : 0.9522883892059326 \n",
      "782000 steps train : 0.9061688184738159 test : 1.142796516418457 \n",
      "783000 steps train : 0.4516376256942749 test : 0.4530267119407654 \n",
      "784000 steps train : 5.4620361328125 test : 7.342846393585205 \n",
      "785000 steps train : 3.7197554111480713 test : 5.627214431762695 \n",
      "786000 steps train : 12.764298439025879 test : 2.7603108882904053 \n",
      "787000 steps train : 31.79125213623047 test : 6.561213970184326 \n",
      "788000 steps train : 4.5209784507751465 test : 11.05375862121582 \n",
      "789000 steps train : 7.555642127990723 test : 5.8644232749938965 \n",
      "790000 steps train : 1.6621794700622559 test : 0.5016965866088867 \n",
      "791000 steps train : 2.178914785385132 test : 0.806236743927002 \n",
      "792000 steps train : 3.361492872238159 test : 2.600172519683838 \n",
      "793000 steps train : 0.4915756285190582 test : 0.48718658089637756 \n",
      "794000 steps train : 0.46374082565307617 test : 0.4886299967765808 \n",
      "795000 steps train : 0.4649234712123871 test : 0.4593842625617981 \n",
      "796000 steps train : 0.44632717967033386 test : 0.4523983895778656 \n",
      "797000 steps train : 0.45620033144950867 test : 0.45835021138191223 \n",
      "798000 steps train : 0.4488067030906677 test : 0.4474380910396576 \n",
      "799000 steps train : 0.44488295912742615 test : 0.4484582841396332 \n",
      "800000 steps train : 0.5632660388946533 test : 0.4967878758907318 \n",
      "801000 steps train : 0.6222848892211914 test : 0.4853486120700836 \n",
      "802000 steps train : 0.4621776044368744 test : 0.47176259756088257 \n",
      "803000 steps train : 0.4713994562625885 test : 0.48524531722068787 \n",
      "804000 steps train : 0.5030427575111389 test : 0.4620622396469116 \n",
      "805000 steps train : 0.5712635517120361 test : 0.5743193626403809 \n",
      "806000 steps train : 0.4629479646682739 test : 0.5540806651115417 \n",
      "807000 steps train : 0.4892779588699341 test : 0.5579760670661926 \n",
      "808000 steps train : 0.49759364128112793 test : 0.514796257019043 \n",
      "809000 steps train : 0.4596235156059265 test : 0.45073795318603516 \n",
      "810000 steps train : 0.46164557337760925 test : 0.45753395557403564 \n",
      "811000 steps train : 0.4338211417198181 test : 0.4410606026649475 \n",
      "812000 steps train : 0.4475114047527313 test : 0.4558732211589813 \n",
      "813000 steps train : 0.43816015124320984 test : 0.4487817883491516 \n",
      "814000 steps train : 0.43870607018470764 test : 0.44503068923950195 \n",
      "815000 steps train : 0.45074376463890076 test : 0.4594152569770813 \n",
      "816000 steps train : 0.42760008573532104 test : 0.4356907308101654 \n",
      "817000 steps train : 0.4509234130382538 test : 0.45173609256744385 \n",
      "818000 steps train : 0.5277547836303711 test : 0.4586786925792694 \n",
      "819000 steps train : 0.643376350402832 test : 0.4596777856349945 \n",
      "820000 steps train : 6.1132049560546875 test : 7.22317361831665 \n",
      "821000 steps train : 2.342108964920044 test : 0.5093148946762085 \n",
      "822000 steps train : 0.5476351380348206 test : 0.582038402557373 \n",
      "823000 steps train : 0.6008385419845581 test : 3.5720906257629395 \n",
      "824000 steps train : 0.5088387727737427 test : 0.5046151280403137 \n",
      "825000 steps train : 0.6715735197067261 test : 0.5207267999649048 \n",
      "826000 steps train : 0.7185246348381042 test : 0.5389624238014221 \n",
      "827000 steps train : 1.1764105558395386 test : 0.9228206872940063 \n",
      "828000 steps train : 0.7891677618026733 test : 0.7467641830444336 \n",
      "829000 steps train : 0.9694424867630005 test : 0.7562968730926514 \n",
      "830000 steps train : 3.620041608810425 test : 7.762601375579834 \n",
      "831000 steps train : 15.860394477844238 test : 11.402433395385742 \n",
      "832000 steps train : 0.985852062702179 test : 1.0645124912261963 \n",
      "833000 steps train : 0.9347435235977173 test : 1.044497013092041 \n",
      "834000 steps train : 0.9191176891326904 test : 0.9802170991897583 \n",
      "835000 steps train : 0.9401824474334717 test : 1.0759590864181519 \n",
      "836000 steps train : 1.0363199710845947 test : 0.9263888597488403 \n",
      "837000 steps train : 0.8303555250167847 test : 0.7518433928489685 \n",
      "838000 steps train : 0.7485389113426208 test : 0.6863850951194763 \n",
      "839000 steps train : 0.6811617612838745 test : 0.6497117280960083 \n",
      "840000 steps train : 26.838001251220703 test : 21.11577606201172 \n",
      "841000 steps train : 14.939742088317871 test : 12.846611976623535 \n",
      "842000 steps train : 60.969581604003906 test : 35.310665130615234 \n",
      "843000 steps train : 6.037633419036865 test : 8.710561752319336 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-e79774877e6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mY_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda\\envs\\tensorflow4\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[0;32m   4864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4865\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4866\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4868\u001b[0m     _shared_docs['pipe'] = (r\"\"\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000000):\n",
    "    X_batch = X_train.sample(n=4)\n",
    "    Y_batch = Y_train.loc[X_batch.index]\n",
    "    sess.run(train_op, feed_dict={X: X_batch, Y: Y_batch})\n",
    "    \n",
    "    if (i+1)%1000==0:\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, Y: Y_train})\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, Y: Y_test})\n",
    "        print('{} steps train : {} test : {} '.format(i+1,train_loss,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.810113</td>\n",
       "      <td>-0.264085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.728174</td>\n",
       "      <td>-0.239653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.779395</td>\n",
       "      <td>-0.259624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.756681</td>\n",
       "      <td>-0.251621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774580</td>\n",
       "      <td>-0.259286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.810113 -0.264085\n",
       "1  0.728174 -0.239653\n",
       "2  0.779395 -0.259624\n",
       "3  0.756681 -0.251621\n",
       "4  0.774580 -0.259286"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.scatter(sess.run(model,feed_dict={X:X_test}).flatten()[:-1],Y_test)\n",
    "aa = pd.DataFrame(sess.run(model,feed_dict={X:X_test}))\n",
    "aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% Iron Concentrate</th>\n",
       "      <th>% Silica Concentrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.662666</td>\n",
       "      <td>-0.903345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737362</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737364</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737368</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737369</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737374</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737377</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737380</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737381</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737382</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737383</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737384</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737385</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737391</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737393</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737394</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737401</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737402</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737407</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737414</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737423</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737425</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737426</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737428</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737429</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737431</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737435</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737441</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737446</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737447</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737452</th>\n",
       "      <td>-0.697333</td>\n",
       "      <td>-0.547964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221236 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        % Iron Concentrate  % Silica Concentrate\n",
       "1                 1.662666             -0.903345\n",
       "7                 1.662666             -0.903345\n",
       "11                1.662666             -0.903345\n",
       "15                1.662666             -0.903345\n",
       "17                1.662666             -0.903345\n",
       "24                1.662666             -0.903345\n",
       "32                1.662666             -0.903345\n",
       "34                1.662666             -0.903345\n",
       "38                1.662666             -0.903345\n",
       "39                1.662666             -0.903345\n",
       "43                1.662666             -0.903345\n",
       "45                1.662666             -0.903345\n",
       "49                1.662666             -0.903345\n",
       "51                1.662666             -0.903345\n",
       "67                1.662666             -0.903345\n",
       "69                1.662666             -0.903345\n",
       "74                1.662666             -0.903345\n",
       "84                1.662666             -0.903345\n",
       "88                1.662666             -0.903345\n",
       "90                1.662666             -0.903345\n",
       "91                1.662666             -0.903345\n",
       "92                1.662666             -0.903345\n",
       "93                1.662666             -0.903345\n",
       "100               1.662666             -0.903345\n",
       "101               1.662666             -0.903345\n",
       "102               1.662666             -0.903345\n",
       "109               1.662666             -0.903345\n",
       "113               1.662666             -0.903345\n",
       "114               1.662666             -0.903345\n",
       "116               1.662666             -0.903345\n",
       "...                    ...                   ...\n",
       "737362           -0.697333             -0.547964\n",
       "737364           -0.697333             -0.547964\n",
       "737368           -0.697333             -0.547964\n",
       "737369           -0.697333             -0.547964\n",
       "737374           -0.697333             -0.547964\n",
       "737377           -0.697333             -0.547964\n",
       "737380           -0.697333             -0.547964\n",
       "737381           -0.697333             -0.547964\n",
       "737382           -0.697333             -0.547964\n",
       "737383           -0.697333             -0.547964\n",
       "737384           -0.697333             -0.547964\n",
       "737385           -0.697333             -0.547964\n",
       "737391           -0.697333             -0.547964\n",
       "737393           -0.697333             -0.547964\n",
       "737394           -0.697333             -0.547964\n",
       "737401           -0.697333             -0.547964\n",
       "737402           -0.697333             -0.547964\n",
       "737407           -0.697333             -0.547964\n",
       "737414           -0.697333             -0.547964\n",
       "737423           -0.697333             -0.547964\n",
       "737425           -0.697333             -0.547964\n",
       "737426           -0.697333             -0.547964\n",
       "737428           -0.697333             -0.547964\n",
       "737429           -0.697333             -0.547964\n",
       "737431           -0.697333             -0.547964\n",
       "737435           -0.697333             -0.547964\n",
       "737441           -0.697333             -0.547964\n",
       "737446           -0.697333             -0.547964\n",
       "737447           -0.697333             -0.547964\n",
       "737452           -0.697333             -0.547964\n",
       "\n",
       "[221236 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Iron Concentration (%)')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAEWCAYAAAB49hJtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XuYXHWd5/H3Jx0ChBAiSVAw0NHVwKBrIokXNjtIAGcIO+NlFUdseEJwjIbZGJkBo2Yc8QKDwK5mnDFuJgho2jgNgpcZRAXjDQ1CEFETGFCS0FwlAdJN24Qk3/njd8qurlRVn66uc6v6vp6nnvQ5VXXq25WqT5/zu5wjM8M550ZrXNYFOOeKycPDOdcQDw/nXEM8PJxzDfHwcM41xMPDOdcQDw9XaJJ+I+nkhLb9XkmfbfC5N0g6vdk15YmHR5NI2irptBRf77WSbpL0tKSdkn4uaXFarz9azXh/JF0j6VPl68zsFWb2gzEVV/21JgB/D1wRLR8m6TvR+90tqaPssf8q6a0Vm7gMuKTZdeWJh0cKJI1v8vZOBL4P/BB4GTAVWAosbObrpKnZ71ETvBm418wejpbfC/wCeCEwE3gr/PH/4kgzu7H8yWb2c2CypHmpVZwyD48ESDpX0m2SPiNpJ3CxpHGS/l7SNklPSPqSpMOix8+UZJIWSdou6UlJK+u8xBXAtWb2aTN70oJNZvaOshreI+mBaK/km5KOKrvPJL1P0v2SnpL0L5JU8dwtkvokbZZ0QrT+KElfk/R7SQ9Ken/Zcy6W1BP9Xn3R4cS86L4vA8cA35LUL+mDZb/zuyVtJ4Qhkq6T9JikZyT9SNIrovVLgC7gg9E2vhWt/+MejaQDJX1W0iPR7bOSDozuO1lSr6S/i97/R0fYU1tICOeSlwAbzOw54MfAS6O9j88Ay2ts4wfA/6rzGsVmZn5rwg3YCpwW/XwusAdYBowHDgbOAx4AXgpMAm4Avhw9fiZgwL9Gj50NPAf8SZXXmQjsBRbUqeUU4EngBOBA4HPAj8ruN+DfgSmEL/XvgdOj+84EHgZeA4iwZ9NJ+EOzCfgHYEL0e/wO+PPoeRcDg8AZQAfwj8DGau9Pxe/8JeAQ4OBo/XnAoVHdnwXuLnvONcCn6rzvnwA2AkcA04GfAp+M7js5+j/5BHBAVOcA8IIa7+EdwJlly39DCO2DgdsIoXAB8LE6/w9/C9yQ9Wczsc981gW0yq1KeGyvuP9W4Pyy5WOB56NwKX2RZpTd/3PgnVVe58XRY4+rU8tVwOVly5Oi15oZLRvwP8vu7wE+FP38HWB5lW2+rsrv9GHg6ujni4Fbyu47HvhDtfcnWi79zi+t83tMiR5zWLQ8Unj8Fjij7L4/B7ZGP58M/AEYX3b/E8Dra7z2/USBGi0fBKwB7iG0Z8wA7gIOA1YDP6pS23uA72f92UzqlrfjzFbyUMXyUcC2suVthOB4Ydm6x8p+HiB86Ss9BewDjgTurfHaRxE+2ACYWb+kHYTg2TrCax1N+BJW6gSOkvR02boOwi58rfoPkjTezPbUqBPK3qfoMOASwt7PdMLvCTANeKbONkqqvcdHlS3vqKil1nsM4X0+tLRgZoPAkrJarwM+QjiU6gDeAHxX0ulmdnP0sEOB8verpXibR3Iqpys/QvgClhxD2I1+fFQbNRsAfga8rc7Dhr2WpEMIjaoP13zGkIeA/1Zj/YNmNqXsdqiZnRG39Bjr30VoqDyN8Bd9ZrReVR5bTbX3+JGY9VW6B5hV7Y6oC1ZRSPx34E4Luxp3Aq8qe+ifAL9s8PVzz8MjPeuBCyS9RNIk4FLg30b4q1zLB4FzJV0kaSqApNmSvhrd/xVgsaQ5UYPhpcDtZrY1xrbXAhdKmqvgZZI6CYdRuyStkHSwpA5Jr5T0mpg1P05oJ6nnUEJbzw5C286lo9zGeuDvJU2XNI3QPrMuZn2VbiLsTQwj6SDCYcsF0aoHgZOjrt35hHagkjcA327w9XPPwyM9XwS+TDg2fpDQuLiskQ2Z2U8JjaKnAL+LenTWED7wmNmtwEeBrwGPEvYk3hlz29cRDh2+AvQBXwcON7O9wF8Cc6L6nyQEzWExy/5Hwhf7aUkX1njMlwiHGg8DmwmNn+WuAo6PtvH1Ks//FOGv/z3ArwiHbp+q8rg4vgUcV95LFfkI0G1mpcOt/084rPo90AvcCBCF6rMWumxbkqKGHedchah7+Hgz+0ADz/0acJWZ3dT8yvLBw8M515DEDlskHSvp7rLbLkkfkHS4pO9FA5S+J+kFSdXgnEtOKnseURfcw4SxAn8D7DSzyyR9iDBIZ0XiRTjnmiqtBtNTgd+a2TZCV9y10fprgbekVINzronSGiT2TkI3GsALzexRADN7VNIR1Z4QNVYtATjooIPmHnPMMakUOlb79u1j3LjidGIVqd4i1QrFqHffvn309vYyODj4pJlNH9WTkx7CSpgH8SQhNACerrj/qZG2MWvWLCuKDRs2ZF3CqBSp3iLVapb/enft2mXz58+3jo4OIwx0G9V3O41YXAjcZWalkZSPSzoSIPr3iRRqcM6V6evrY+HChWzcuJH169eP/IQq0giPsxg6ZAH4JrAo+nkR8I0UanDORSqD48wzz2xoO4mGh6SJwBsJ089LLgPeKOn+6L7LkqzBOTekWcEBCTeYWpjENbVi3Q5C74tzLkXNDA7wuS3OtYVmBwd4eDjX8pIIDvDwcK6lJRUc4OHhXMtKMjjAw8O5lpR0cICHh3MtJ43gAA8P51pKWsEBHh7OtYw0gwM8PJxrCWkHB3h4OFd4WQQHeHg4V2hZBQd4eDhXWFkGB3h4OFdIWQcHeHg4Vzh5CA7w8HCuUPISHODh4Vxh5Ck4wMPDuULIW3CAh4dzuZfH4AAPD+dyLa/BAR4ezuVWnoMDPDycy6W8Bwd4eDiXO0UIDvDwcC5XihIc4OHhXG4UKTjAw8O5XChacICHh3OZK2JwgIeHc5kqanCAh4dzmSlycICHh3OZKHpwgIeHc6lrheAADw/nUtUqwQEeHs6lppWCAzw8nEtFqwUHeHg4l7hWDA7w8HAuUa0aHJBweEiaIul6SfdK2iLpRElzJG2UdLekOyW9NskaXHq6u2HSJJCSuW3aVH39+edn/ZtXNzAw0LLBATA+4e2vAm42s7dLmgBMBHqAj5vZtyWdAVwOnJxwHS5h3d2waBHs3Zv+a69eHf79/OfTf+1a+vr6WLFiBVu2bGnJ4IAE9zwkTQZOAq4CMLPdZvY0YMDk6GGHAY8kVYNLz8qV2QRHyZo12b12pdKhyubNm1s2OABkZslsWJoDrAE2A7OBTcBy4BjgO4AI4fU/zGxblecvAZYATJ8+fW5PT08idTZbf38/kyZNyrqM2JpV76ZNTShmBDNm9NPbW7vWuXOTr2EkAwMDrFixgs2bN3PRRRdx+umnZ11SLAsWLNhkZvNG9SQzS+QGzAP2AK+LllcBnwT+CXhbtO4dwC0jbWvWrFlWFBs2bMi6hFFpVr2dnWaQ7O3KKzfUvK+joym/xpjs2rXL5s+fbx0dHdbT01OozwJwp43yO55kg2kv0Gtmt0fL1wMnAIuAG6J11wHeYNoCLrkEOjqye/0lS7J7bWjtXpVaEgsPM3sMeEjSsdGqUwmHMI8Ab4jWnQLcn1QNLj1dXXDttXDIIem/9tKl2TaWtmNwQPLjPJYB3ZLuAeYAlwLvAf6vpF9Gyxn/zXDN0tUF/f3xDkLWrYPOzvA8aWyvu3p17e7dpLtx2zU4IOGuWjO7m9D2Ue4nQA6attxYdHeHHpZt+zV1j15CbfZAst247Rwc4CNMXQO6u0MbQzOCIw1JdOO2e3CAh4drwMqVMDCQdRXxNXv8iQdH4OHhRm379qwryI4HxxAPDweEhsXx4+PNMUmyjSLPPDiGS3puiyuA888falhsRaVenbHw4Nif73m4XM0Labbx48MAtrHw4KjOw8NlOqEtSR0dcM01YfxJozw4avPwcJkOK0/S3r1w222NP9+Doz4PD5f5vJAkNXpI5sExMm8wdX8cfblmTesdwjTy+3hwxON7Hg4IAbJnz/C5JxMnZl1Vc5x2WvzHenDE5+HRwrq7YeZMGDcu/NvdHb5Icc4LevbZxRpFWs+tt4bffSQeHKPj4dGiyuefmIV/zz47fJHa0cqV9e/34Bg9D48WVbT5J0mrN6Teg6MxHh4tqp3nn1RzzDHV13twNM7Do0XV+rK0q2qnD/DgGBsPjxZ1ySWt01uSBA+OsfPwaFFdXeEiTK06enQsPDiaw8OjRXV3hxMSt9qgr7Hy4GgeD48W5b0t+/PgaC4PjxblvS3DHX20B0ezeXi0KO9tKdfHwQd7cDSbh0eL8t6Wkj5gIb/9rQdHs3l4tKDSNVUGBtq9tyUEB2xk8mQPjmbz8MixahPbRnrctGmwePHQoKi9e8MeyNKlMGFCSoXnwlBwwHqeeurMVK4g1078fB45VZrYVuox2bZt6KQ95afVq3zcjh37b2tgAK66CnbvTrbm/BgeHDC0x5HkFeTaje955FS1rtaBgf1nh8btkvXgGLJ6ddhDizNN39Xm4ZFTtbpat28ffphSlEs+pmPk4CjZsSMc3nmANM7DI6dqdbUefvjw83S4kvjBUfL88yOf58PV5uGRU9W6WkvLPnK00uiDo8QH0zXOwyOnurrCCYk7O8NpATs7w/LOnVlXljeNBwf4YLqx8PDIme7u0JhXOo9ofz98+cuwdWsIFP+wlxtbcBxwwNivJtfOPDxypLs7NOKVd7fu2AHnnTfUsOcf9pKxBcchh8DVV4/tanLtzsMjR1auDI14lXbvHmrYG8sV0FrH2IID4Nlnm1xSG0o0PCRNkXS9pHslbZF0YrR+maT7JP1G0uVJ1pBnlSND63W7btsWhpq38tXs4xl7cJR4T8vYJD3CdBVws5m9XdIEYKKkBcCbgVeZ2XOSjki4hlyKMzK00r59ydaUd4ODAzQrOMDHyIxVYuEhaTJwEnAugJntBnZLWgpcZmbPReufSKqGPPOT9YxWH2vXrgC20IzggHafNDh2sjojjSSdUO/JZnZXnefOAdYAm4HZwCZgOXAb8A3gdGAQuNDM7qjy/CXAEoDp06fP7enpGel3yYX+/n4mTZo04uM2bUqhmBhmzOint3fkerM0ODjA2rUr2L59M11dH2X27JObtu25c5u2qf3E/SzkwYIFCzaZ2bxRPcnMat6ADdHtZ8DzwJ2EEHge+MkIz50H7AFeFy2vAj4J/Br4J0DAa4EHiUKs1m3WrFlWFBs2bIj1uM7O0lVhs71deeWGzGuof9tlMN+gw84552NN3XZnZ5KfhPifhTwA7rQ638Fqt7oNpma2wMwWANuAE8xsnpnNBV4NPDBCLvUCvWZ2e7R8PXBCtP6GqOafA/uAabGSroX4yXriGN442sw9jokTvdt7rOL2thxnZr8qLZjZr4E59Z5gZo8BD0k6Nlp1KuEQ5uvAKQCSZgETgCdHWXfhVY4gnToVyvdwp04NV6pfujS7GrPVvF4VCO9t5WhdH+MxNnEbTLdIWgusAww4m9ByNZJlQHfU0/I7YDHwLPBFSb8GdgOLot2mttPVNfIHuKurHbtnmxscEEbq9vWNeTOuTNzwWAwsJTR4AvwIGPEjbWZ3E9o+Kp0d83Vd22l+cID3rCQhVniY2aCkLwA3mdl9CdfkKkydGm8cSPElExzgF79KQqw2D0lvAu4Gbo6W50j6ZpKFuSGrVmVdQRqSCw4I7RyuueI2mH6M0K36NPzxcGRmQjW1tVonPW7t3e5kg2PCBO9ZSULcNo89ZvaMpESLaXe1Tnp88MGtvNudbHCAn3EtKXH3PH4t6V1Ah6SXS/oc8NME62pLtU563LrtHckHB/jpBpMSNzyWAa8AngO+AjwDfCCpolpZtcOS0rr2mqiVTnCU+OkGm2/EwxZJHcDHzewiwPN7DKodlixeHAYutc+lESDt4IBw4mjXXCPueZjZXiDB6UOtrXxPY9Gi/Q9Lnn/egyOVV+3zyyw0W9wG019EXbPXEUaIAmBmNyRSVYuo3NNo3UbPuLIJDhg6G5sPSW+euOFxOLCDaE5KxAAPjzr8nB3lsguOEm/3aK64I0wXJ11IKyldpT5OA+gBB7RDm0f2wQF+5vlmixUekq4m7GkMY2bnNb2igtu5c/ihSj2dnWHw0m23tfLkt3wEh0/Bb764hy3/XvbzQcBbgUeaX07xPfzw6IJj+XIfx5G00nvt7R3NFfew5Wvly5LWA7ckUlHBxT38KI0ebd02kXwEhxQumOWar9FLL7wc8CPIKiZMiPe4jo76wSGFYdXFnBGQj+AAH9+RpLhtHn0Mb/N4DFiRSEUF9+IXh+PresEw0v0w1Lh3zDFFG3man+BwyYq152Fmh5rZ5LLbrMpDGRccfvj+F6heunT/U+DVmyJe3rhXrEa+/AVH67YnZS/unsd84G4ze1bS2YQTGa8ys0L9TUxLvdMLlnfjlg5NKh188NBjzz03sTKbLH/BAeE97u72xtIkxG3zWA0MSJoNfJBwNvUvJVZViyqNOC0dhtRq09ixA84+O9z27Em3xsbkMzggvMc+ozYZccNjT3SS4jcT9jhWAYcmV1Zrqjbi1CzMeymu/AZHiY8sTUbccR59kj5MOHHxSdFM2wOSK6s11foQF/catPkPDvCRpUmJ+zfvrwjn8nh3dD2WFwNXJFZVi2qtD3ExgsNHliYnbm/LY2b2/8zsx9HydjPzNo9RqnaVuIkT4ZBDsqmnccUIjqlT/eJOSYp79vTXS7pDUr+k3ZL2Snom6eJaTeVV4jo7wzk+itEoWlKM4AD4wx+yrqC1xT1s+WfgLOB+4GDgr4F/SaqodmAWel1Wr4bnnsu6mriKExwQGqe9pyU5cRtMMbMHJHVEZxa7WpKfAHmUKk8OVCzFCo4S72lJTtzwGIiuN3u3pMuBR4HCHalnrbgnBypmcECrNVLnS9zDlnOix/4fwmkIjwbellRRraqYfwWLGxze05KsuL0t2wABR5rZx83sb83sgWRLax2lkyAX7+JDxQ0OGBrm75IRt7flL/Fr1Takckh6cRQ7OCAM81+yxM+anpS4hy0X49eqbUgx2zmKHxwl3uOSnNHMbfFxHQ0oXjtH6wRHSfH+D4rBr1WbsGKdyar1ggO8xyUpjVyrdj2wC79WbYtpzeDwHpfkxO1tGTCzlWb2GjObF/08mHRxRVLqUdm0aegC1hAuxZB/rRkcpbO2+dyWZNQdJDZSj4qZvWmE508B1gKvJJwD9Twz+1l034WEmbnTzezJ0RSdN9UuYH3OOeF6LHk/B+ng4ACtGBx+1vTkjTTC9ETgIcKn6nbCWI/RWAXcbGZvj0aoTgSQdDTwRqAlmrJqneTnC1+A970Prr02rz0ufaxduwLYQisFB3g7RxpGOmx5EfARwp7DKsIX/kkz+6GZ/bDeEyVNBk4CrgIws91m9nR092cIpzMs3LCpamq15pvBTTcNn0mbn0sphEOV7ds302rB4e0c6ZDFHPYo6UDCzNorgE+Y2edGePwcYA2wGZgNbAKWA6cCp5rZcklbgXnVDlskLQGWAEyfPn1uT09P3N8pdb/61dDFnmbM6Ke3d9Kw++fOHfp506YUC6thcHCAtWtXsH37Zt7//ouYMeP0rEuKpdp7W81LXpKPXq7+/n4mTRq53jxYsGDBJjObN6onmVndG3Ag8L+B64A7gI8CL47xvHnAHuB10fIqQvDcDhwWrdsKTBtpW7NmzbI8W7fOTDIDsyuv3GBhnyPcOjuHP7az04bdn/5tl8F8gw6Dnv3qzfMtTq2V73eWNmzYkHUJsQF32gjfw8pb3cMWSdcSxnOcAHzcQm/LJ83s4Ri51Av0mtnt0fL10XZeAvwy2uuYAdwl6UUxtpdrtc4Gtm0bTJoE06aFEx3396db13Ct2atSIvnhSppGajA9hzCLdhbwfg0dsAswM5tc64lm9pikhyQda2b3EQ5X7jKzU/+4kTqHLUUR5xwdzz4bbpDlRYhaOzgATjnFu2XTVHfPw8zGWbhaXOUV4w6tFxxllgHdku4B5gCXNqPoPCnG3JXWDw6AW28dPsbGJSv2mcQaYWECXc1GGDObmeTrpyH/8ybaIzhKtm0Le4LgeyFJK/TlhvIg3+MJ2is4SnwmbTo8PMao2uUU8qE9g6Mk/3uExefhMUaVl1MYPz4PA8HaOzgg73uErcHDowm6usI8in37YPbsrKvx4PARpunw8EhAdn/12jc4pk4dupCWz6RNR6K9Le3qkktg8WJ4/vk0X7V9gwPCQLwnCztaqJh8zyMBXV1w9dXhr2E62js4IHTRTpvmYzzS5OGRkK6u8Jcw+cZTD46SHTvCHp8HSDo8PBJQOqvYuHHhlhwPjkrPP+9jPNLibR5NtnPn8Lkue/cm9UoeHLX4GI90+J5HE5TvaTz4YBpzXdo7OEq9KrX4GI90+J7HGHR3w/Llac+Ube/ggHDmjlp7Fwcc4GM80uJ7Hg0qTcX34MhGtRPgTZ0aerl8jEc6fM+jQelPxffgqKez08+Wnjbf82hQuo1yHhwj8UbS9Hl4NCi9RjkPjji8kTR9Hh4NqjUVf9y4MLO2OTw44vCJcNnw8GhQ5VT8zk5Ytw5e/Wq45ppmDE334IjDJ8JlxxtMx6Cra/8P7Q03wHvfO3TC48Z4cMRx6qlwyy1ZV9G+PDyaqLsbHn/cgyMNxx/vwZE1P2xpopUrq48/iM+DI66tW30CXNY8PBpUPiR95kw4//wwLbxxHhyjUTrJceX/gwdKevywpQGVF3ratg1Wrx7LFj04GlG6zEL5/4NfdiE9vufRgOaOLvXgGIvK/we/7EJ6PDwa0LzRjB4cSfDRpunw8Iipuzuc5k4aa6NoiQdHUny0aTq8zSOG7u5mn9DYgyNJZ5yRdQXtwfc8Yli50oOjSG66KesK2oOHRxXlhyjSWLtgy3lwpMHbPNLhhy0Vurth0aIkzj3qwZEWb/NIR1vueZQPLJo2bfhextlne3AUmc+wTU/bhUdpgNe2baHXZMeOpE8l6MGRpKVLh89s9hm26Wm7w5Z0Tx/owZGkzk74/OezrqJ9td2eR3qNaR4cSfLDk+wlGh6Spki6XtK9krZIOlHSFdHyPZJulDQlyRoqpdOY5sGRND88yV7Sex6rgJvN7DhgNrAF+B7wSjN7FfCfwIcTrmGYWqcPbJbBwQE8OJLV2enBkQeJhYekycBJwFUAZrbbzJ42s++a2Z7oYRuBGUnVUE3l6QOnTm3m1ez7WLt2BR4cyfHDlfyQNWeixv4bluYAa4DNhL2OTcByM3u27DHfAv7NzNZVef4SYAnA9OnT5/b09CRSZ6WdO+Ghh2DPnpEfW25wcIC1a1ewfftmuro+yuzZJydSX7PNmNFPb++krMuoa9w42LcPjj66n/HjJ3H44VlXFE9/fz+TJuX7vS1ZsGDBJjObN6onmVkiN2AesAd4XbS8Cvhk2f0rgRuJAqzebdasWZaFdevMJk40C526w2+S2aRJpeVdBvMNOuyccz5W9fF5vV155YbMa6h1GzfObOnSof+PDRs2ZPI5aFSR6gXutFF+x5Ns8+gFes3s9mj5euAEAEmLgL8AuqLCc6nWGdLNwl/Cvj7YtauP+fMXMm7cRqZNW8/s2ScPOxTq6Bj+by3HHz/2ek89NdQ3msOw8ePDWIlDDhlaN25cWLd06fD6ly4d/vVet2744Z9U+3UmTBj+GpWk/be/d693xebaaNNmNDfgx8Cx0c8XA1cApxMOZabH3U5Wex4j2bVrl82fP986Ojqsp6fHzIr118asWPUWqVazYtVLA3seSQ8SWwZ0S5oA/A5YDNwBHAh8T+FP1UYze1/CdTRdX18fCxcuZOPGjaxfv54zz/TGUddeEg0PM7ub0PZR7mVJvmYaPDica8MRpmPlweFc4OExCh4czg3x8IjJg8O54Tw8YvDgcG5/Hh4j8OBwrjoPjzo8OJyrzcOjBg8O5+rz8KjCg8O5kXl4VPDgcC4eD48yHhzOxefhEfHgcG50PDzw4HCuEW0fHh4czjWmrcPDg8O5xrVteHhwODc2bRkeHhzOjV3bhYcHh3PN0Vbh4cHhXPO0TXh4cDjXXG0RHh4czjVfy4eHB4dzyWjp8PDgcC45LRseHhzOJaslw8ODw7nktVx4eHA4l46WCg8PDufS0zLh4cHhXLpaIjw8OJxLX+HDw4PDuWwUOjw8OJzLTmHDw4PDuWwVMjw8OJzLXuHCw4PDuXwoVHh4cDiXH4UJDw8O5/Il0fCQNEXS9ZLulbRF0omSDpf0PUn3R/++YKTt7Nu3z4PDuZxJes9jFXCzmR0HzAa2AB8CbjWzlwO3Rst19fb2enA4lzOJhYekycBJwFUAZrbbzJ4G3gxcGz3sWuAtI21rcHDQg8O5nJGZJbNhaQ6wBthM2OvYBCwHHjazKWWPe8rM9jt0kbQEWBItvhL4dSKFNt804MmsixiFItVbpFqhWPUea2aHjuYJSYbHPGAjMN/Mbpe0CtgFLIsTHhXbutPM5iVSaJMVqVYoVr1FqhWKVW8jtSbZ5tEL9JrZ7dHy9cAJwOOSjgSI/n0iwRqccwlJLDzM7DHgIUnHRqtOJRzCfBNYFK1bBHwjqRqcc8kZn/D2lwHdkiYAvwMWEwKrR9K7ge1AnFbQNcmV2HRFqhWKVW+RaoVi1TvqWhNr83DOtbbCjDB1zuWLh4dzriG5C49mDWlPS416r4iW75F0o6QpI28pedVqLbvvQkkmaVqWNZarVa+kZZLuk/QbSZdnXSfU/BzMkbRR0t2S7pT02qzrBJB0bFRT6bZL0gdG/T0zs1zdCKNO/zr6eQIwBbgc+FC07kPAp7Ouc4R6/wwYH637dF7qrVZr9PPRwHeAbcC0rOsc4b1dANwCHBitPyLrOuvU+l1gYbTuDOAHWddZpe4O4DGgc7Tfs8yLr/hFJgMPEjXklq2/Dzgy+vlI4L6sa61Xb8Vj3gp057lWwhic2cDWvIRHnc9CD3Ba1vXFrPU7wF9FP58FfCXrWqvU/mfAbdHPo/qe5e2w5aXA74GrJf1C0lpJhwAvNLNHAaJ/j8iyyDK16i13HvApnDrHAAAC/UlEQVTt9EvbT9VaJb2JMGXglxnXV6nWezsL+FNJt0v6oaTXZFsmULvWDwBXSHoIuBL4cJZF1vBOYH3086i+Z3kLj/GEUairzezVwLPEmHWbobr1SloJ7AG6sylvmGq1XgysBP4hw7pqqfXejgdeALweuIgwZkiZVRnUqnUpcIGZHQ1cQDRJNC+i8VdvAq5raANZ7zZV7EK9CNhatvynwH+Q38OWqvVGPy8CfgZMzLrOOrXeSpgesDW67SEM3HtRTuv9D+Bm4OSy9b8Fpue01mcYGkslYFfW72tF3W8Gvlu2XNzDFivYkPZa9Uo6HVgBvMnMBjIrsEyNWu8ysyPMbKaZzSTMRzohemym6nwWvg6cAiBpFqFxMtOZq3VqfQR4Q7TuFOD+DMqr5yyGDllglN+z3I0wjabyryV8KIYNaQeOIRrSbmY7MyuyTI167wAOBHZED9toZu/LpsIh1Wo1s6fK7t8KzDOzXEwjr/HePgt8EZgD7AYuNLPvZ1ZkpEatryCcEGs8MAicb2abMiuyjKSJwEPAS83smWjdVEbxPctdeDjniiFXhy3OueLw8HDONcTDwznXEA8P51xDPDyccw3x8HCjJumt0Qzc46LlmdHysrLH/LOkc6Ofr5H0sKQDo+VpUbewKzAPD9eIs4CfEOZFlDwBLI+GPFezlzDPx7UIDw83KpImAfOBdzM8PH5PGO6+qNrzgM8CF0hK+ry5LiUeHm603kK4hOh/AjslnVB232XA30nqqPK87YS9lXNSqNGlwMPDjdZZwFejn78aLQNgZg8CPwfeVeO5lxJmwvrnrgX4LqSLLZr7cArwSklGOAuVAZ8ve9ilhJML/ajy+Wb2gKS7gXekUK5LmP8FcKPxduBLZtYZzcQ9mnAGrRmlB5jZvYQZpX9RYxuXABcmXqlLnIeHG42zgBsr1n0N+EjFuksoC5RyZvYb4K7ml+bS5rNqnXMN8T0P51xDPDyccw3x8HDONcTDwznXEA8P51xDPDyccw3x8HDONeS/AEMf0Rj4+CbBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gca().set_aspect('equal')\n",
    "plt.scatter(aa[aa.columns[0]]*Data['% Iron Concentrate'].std()+Data['% Iron Concentrate'].mean(),Y_test['% Iron Concentrate']*Data['% Iron Concentrate'].std()+Data['% Iron Concentrate'].mean(),color='b')\n",
    "plt.plot([60,70],[60,70],color='k')\n",
    "plt.axis([60,70,60,70])\n",
    "plt.grid(True)\n",
    "plt.xlabel('ANN')\n",
    "plt.ylabel('Measured')\n",
    "plt.title('Iron Concentration (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Silica Concentration (%)')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gca().set_aspect('equal')\n",
    "plt.scatter(aa[aa.columns[1]]*Data['% Silica Concentrate'].std()+Data['% Silica Concentrate'].mean(),Y_test['% Silica Concentrate']*Data['% Silica Concentrate'].std()+Data['% Silica Concentrate'].mean(),color='b')\n",
    "plt.plot([0,10],[0,10],color='k')\n",
    "plt.axis([0,10,0,10])\n",
    "plt.grid(True)\n",
    "plt.xlabel('ANN')\n",
    "plt.ylabel('Measured')\n",
    "plt.title('Silica Concentration (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.477700105407461"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt((abs(aa[aa.columns[1]]*Data['% Silica Concentrate'].std()+Data['% Silica Concentrate'].mean() - Y_test['% Silica Concentrate'].reset_index(drop=True)*Data['% Silica Concentrate'].std()+Data['% Silica Concentrate'].mean())/(Y_test['% Silica Concentrate'].reset_index(drop=True)*Data['% Silica Concentrate'].std()+Data['% Silica Concentrate'].mean()))*100).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.09171960270356"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(((aa[aa.columns[0]]*Data['% Iron Concentrate'].std()+Data['% Iron Concentrate'].mean() - Y_test['% Iron Concentrate'].reset_index(drop=True)*Data['% Iron Concentrate'].std()+Data['% Iron Concentrate'].mean())**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23074662c18>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
