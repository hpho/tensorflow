{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating correlation using Pearson method\n",
      "1000 steps -> loss: 0.004834\n",
      "2000 steps -> loss: 0.004801\n",
      "3000 steps -> loss: 0.004785\n",
      "4000 steps -> loss: 0.004769\n",
      "5000 steps -> loss: 0.004750\n",
      "6000 steps -> loss: 0.004728\n",
      "7000 steps -> loss: 0.004706\n",
      "8000 steps -> loss: 0.004687\n",
      "9000 steps -> loss: 0.004675\n",
      "10000 steps -> loss: 0.004668\n",
      "11000 steps -> loss: 0.004661\n",
      "12000 steps -> loss: 0.004651\n",
      "13000 steps -> loss: 0.004639\n",
      "14000 steps -> loss: 0.004623\n",
      "15000 steps -> loss: 0.004602\n",
      "16000 steps -> loss: 0.004573\n",
      "17000 steps -> loss: 0.004536\n",
      "18000 steps -> loss: 0.004489\n",
      "19000 steps -> loss: 0.004435\n",
      "20000 steps -> loss: 0.004379\n",
      "21000 steps -> loss: 0.004329\n",
      "22000 steps -> loss: 0.004268\n",
      "23000 steps -> loss: 0.004216\n",
      "24000 steps -> loss: 0.004164\n",
      "25000 steps -> loss: 0.004131\n",
      "26000 steps -> loss: 0.004063\n",
      "27000 steps -> loss: 0.004014\n",
      "28000 steps -> loss: 0.003967\n",
      "29000 steps -> loss: 0.003924\n",
      "30000 steps -> loss: 0.003885\n",
      "31000 steps -> loss: 0.003849\n",
      "32000 steps -> loss: 0.003819\n",
      "33000 steps -> loss: 0.003793\n",
      "34000 steps -> loss: 0.003771\n",
      "35000 steps -> loss: 0.003753\n",
      "36000 steps -> loss: 0.003739\n",
      "37000 steps -> loss: 0.003727\n",
      "38000 steps -> loss: 0.003717\n",
      "39000 steps -> loss: 0.003708\n",
      "40000 steps -> loss: 0.003703\n",
      "41000 steps -> loss: 0.003693\n",
      "42000 steps -> loss: 0.003685\n",
      "43000 steps -> loss: 0.003678\n",
      "44000 steps -> loss: 0.003671\n",
      "45000 steps -> loss: 0.003665\n",
      "46000 steps -> loss: 0.003658\n",
      "47000 steps -> loss: 0.003651\n",
      "48000 steps -> loss: 0.003645\n",
      "49000 steps -> loss: 0.003638\n",
      "50000 steps -> loss: 0.003632\n",
      "51000 steps -> loss: 0.003625\n",
      "52000 steps -> loss: 0.003622\n",
      "53000 steps -> loss: 0.003613\n",
      "54000 steps -> loss: 0.003607\n",
      "55000 steps -> loss: 0.003601\n",
      "56000 steps -> loss: 0.003595\n",
      "57000 steps -> loss: 0.003589\n",
      "58000 steps -> loss: 0.003583\n",
      "59000 steps -> loss: 0.003588\n",
      "60000 steps -> loss: 0.003572\n",
      "61000 steps -> loss: 0.003567\n",
      "62000 steps -> loss: 0.003561\n",
      "63000 steps -> loss: 0.003556\n",
      "64000 steps -> loss: 0.003559\n",
      "65000 steps -> loss: 0.003545\n",
      "66000 steps -> loss: 0.003546\n",
      "67000 steps -> loss: 0.003540\n",
      "68000 steps -> loss: 0.003530\n",
      "69000 steps -> loss: 0.003525\n",
      "70000 steps -> loss: 0.003520\n",
      "71000 steps -> loss: 0.003540\n",
      "72000 steps -> loss: 0.003510\n",
      "73000 steps -> loss: 0.003506\n",
      "74000 steps -> loss: 0.003501\n",
      "75000 steps -> loss: 0.003515\n",
      "76000 steps -> loss: 0.003492\n",
      "77000 steps -> loss: 0.003488\n",
      "78000 steps -> loss: 0.003496\n",
      "79000 steps -> loss: 0.003479\n",
      "80000 steps -> loss: 0.003475\n",
      "81000 steps -> loss: 0.003471\n",
      "82000 steps -> loss: 0.003467\n",
      "83000 steps -> loss: 0.003463\n",
      "84000 steps -> loss: 0.003459\n",
      "85000 steps -> loss: 0.003455\n",
      "86000 steps -> loss: 0.003451\n",
      "87000 steps -> loss: 0.003448\n",
      "88000 steps -> loss: 0.003444\n",
      "89000 steps -> loss: 0.003449\n",
      "90000 steps -> loss: 0.003437\n",
      "91000 steps -> loss: 0.003446\n",
      "92000 steps -> loss: 0.003430\n",
      "93000 steps -> loss: 0.003427\n",
      "94000 steps -> loss: 0.003423\n",
      "95000 steps -> loss: 0.003420\n",
      "96000 steps -> loss: 0.003417\n",
      "97000 steps -> loss: 0.003413\n",
      "98000 steps -> loss: 0.003410\n",
      "99000 steps -> loss: 0.003408\n",
      "100000 steps -> loss: 0.003404\n",
      "Selected 1 parameters\n",
      "1000 steps -> loss: 0.003348\n",
      "2000 steps -> loss: 0.002524\n",
      "3000 steps -> loss: 0.002259\n",
      "4000 steps -> loss: 0.002187\n",
      "5000 steps -> loss: 0.002119\n",
      "6000 steps -> loss: 0.002038\n",
      "7000 steps -> loss: 0.001949\n",
      "8000 steps -> loss: 0.001845\n",
      "9000 steps -> loss: 0.001734\n",
      "10000 steps -> loss: 0.001693\n",
      "11000 steps -> loss: 0.001680\n",
      "12000 steps -> loss: 0.001666\n",
      "13000 steps -> loss: 0.001655\n",
      "14000 steps -> loss: 0.001645\n",
      "15000 steps -> loss: 0.001635\n",
      "16000 steps -> loss: 0.001626\n",
      "17000 steps -> loss: 0.001616\n",
      "18000 steps -> loss: 0.001607\n",
      "19000 steps -> loss: 0.001605\n",
      "20000 steps -> loss: 0.001597\n",
      "21000 steps -> loss: 0.001584\n",
      "22000 steps -> loss: 0.001578\n",
      "23000 steps -> loss: 0.001572\n",
      "24000 steps -> loss: 0.001568\n",
      "25000 steps -> loss: 0.001563\n",
      "26000 steps -> loss: 0.001559\n",
      "27000 steps -> loss: 0.001556\n",
      "28000 steps -> loss: 0.001553\n",
      "29000 steps -> loss: 0.001551\n",
      "30000 steps -> loss: 0.001549\n",
      "31000 steps -> loss: 0.001547\n",
      "32000 steps -> loss: 0.001545\n",
      "33000 steps -> loss: 0.001544\n",
      "34000 steps -> loss: 0.001543\n",
      "35000 steps -> loss: 0.001542\n",
      "36000 steps -> loss: 0.001541\n",
      "37000 steps -> loss: 0.001541\n",
      "38000 steps -> loss: 0.001541\n",
      "39000 steps -> loss: 0.001540\n",
      "40000 steps -> loss: 0.001541\n",
      "41000 steps -> loss: 0.001539\n",
      "42000 steps -> loss: 0.001538\n",
      "43000 steps -> loss: 0.001538\n",
      "44000 steps -> loss: 0.001538\n",
      "45000 steps -> loss: 0.001547\n",
      "46000 steps -> loss: 0.001537\n",
      "47000 steps -> loss: 0.001537\n",
      "48000 steps -> loss: 0.001537\n",
      "49000 steps -> loss: 0.001537\n",
      "50000 steps -> loss: 0.001537\n",
      "51000 steps -> loss: 0.001536\n",
      "52000 steps -> loss: 0.001536\n",
      "53000 steps -> loss: 0.001536\n",
      "54000 steps -> loss: 0.001536\n",
      "55000 steps -> loss: 0.001536\n",
      "56000 steps -> loss: 0.001535\n",
      "57000 steps -> loss: 0.001535\n",
      "58000 steps -> loss: 0.001535\n",
      "59000 steps -> loss: 0.001535\n",
      "60000 steps -> loss: 0.001535\n",
      "61000 steps -> loss: 0.001534\n",
      "62000 steps -> loss: 0.001534\n",
      "63000 steps -> loss: 0.001534\n",
      "64000 steps -> loss: 0.001534\n",
      "65000 steps -> loss: 0.001534\n",
      "66000 steps -> loss: 0.001534\n",
      "67000 steps -> loss: 0.001534\n",
      "68000 steps -> loss: 0.001534\n",
      "69000 steps -> loss: 0.001533\n",
      "70000 steps -> loss: 0.001533\n",
      "71000 steps -> loss: 0.001533\n",
      "72000 steps -> loss: 0.001533\n",
      "73000 steps -> loss: 0.001533\n",
      "74000 steps -> loss: 0.001533\n",
      "75000 steps -> loss: 0.001533\n",
      "76000 steps -> loss: 0.001533\n",
      "77000 steps -> loss: 0.001532\n",
      "78000 steps -> loss: 0.001533\n",
      "79000 steps -> loss: 0.001534\n",
      "80000 steps -> loss: 0.001532\n",
      "81000 steps -> loss: 0.001532\n",
      "82000 steps -> loss: 0.001532\n",
      "83000 steps -> loss: 0.001532\n",
      "84000 steps -> loss: 0.001532\n",
      "85000 steps -> loss: 0.001531\n",
      "86000 steps -> loss: 0.001531\n",
      "87000 steps -> loss: 0.001531\n",
      "88000 steps -> loss: 0.001531\n",
      "89000 steps -> loss: 0.001533\n",
      "90000 steps -> loss: 0.001531\n",
      "91000 steps -> loss: 0.001534\n",
      "92000 steps -> loss: 0.001531\n",
      "93000 steps -> loss: 0.001531\n",
      "94000 steps -> loss: 0.001530\n",
      "95000 steps -> loss: 0.001530\n",
      "96000 steps -> loss: 0.001530\n",
      "97000 steps -> loss: 0.001530\n",
      "98000 steps -> loss: 0.001530\n",
      "99000 steps -> loss: 0.001530\n",
      "100000 steps -> loss: 0.001530\n",
      "Selected 2 parameters\n",
      "1000 steps -> loss: 0.002128\n",
      "2000 steps -> loss: 0.002013\n",
      "3000 steps -> loss: 0.001951\n",
      "4000 steps -> loss: 0.001896\n",
      "5000 steps -> loss: 0.001843\n",
      "6000 steps -> loss: 0.001786\n",
      "7000 steps -> loss: 0.001726\n",
      "8000 steps -> loss: 0.001681\n",
      "9000 steps -> loss: 0.001653\n",
      "10000 steps -> loss: 0.001621\n",
      "11000 steps -> loss: 0.001601\n",
      "12000 steps -> loss: 0.001587\n",
      "13000 steps -> loss: 0.001577\n",
      "14000 steps -> loss: 0.001570\n",
      "15000 steps -> loss: 0.001565\n",
      "16000 steps -> loss: 0.001560\n",
      "17000 steps -> loss: 0.001556\n",
      "18000 steps -> loss: 0.001552\n",
      "19000 steps -> loss: 0.001549\n",
      "20000 steps -> loss: 0.001549\n",
      "21000 steps -> loss: 0.001544\n",
      "22000 steps -> loss: 0.001542\n",
      "23000 steps -> loss: 0.001540\n",
      "24000 steps -> loss: 0.001538\n",
      "25000 steps -> loss: 0.001536\n",
      "26000 steps -> loss: 0.001535\n",
      "27000 steps -> loss: 0.001534\n",
      "28000 steps -> loss: 0.001532\n",
      "29000 steps -> loss: 0.001531\n",
      "30000 steps -> loss: 0.001530\n",
      "31000 steps -> loss: 0.001529\n",
      "32000 steps -> loss: 0.001528\n",
      "33000 steps -> loss: 0.001527\n",
      "34000 steps -> loss: 0.001526\n",
      "35000 steps -> loss: 0.001525\n",
      "36000 steps -> loss: 0.001524\n",
      "37000 steps -> loss: 0.001524\n",
      "38000 steps -> loss: 0.001523\n",
      "39000 steps -> loss: 0.001522\n",
      "40000 steps -> loss: 0.001522\n",
      "41000 steps -> loss: 0.001528\n",
      "42000 steps -> loss: 0.001520\n",
      "43000 steps -> loss: 0.001520\n",
      "44000 steps -> loss: 0.001519\n",
      "45000 steps -> loss: 0.001519\n",
      "46000 steps -> loss: 0.001518\n",
      "47000 steps -> loss: 0.001517\n",
      "48000 steps -> loss: 0.001517\n",
      "49000 steps -> loss: 0.001516\n",
      "50000 steps -> loss: 0.001516\n",
      "51000 steps -> loss: 0.001515\n",
      "52000 steps -> loss: 0.001515\n",
      "53000 steps -> loss: 0.001514\n",
      "54000 steps -> loss: 0.001514\n",
      "55000 steps -> loss: 0.001513\n",
      "56000 steps -> loss: 0.001513\n",
      "57000 steps -> loss: 0.001513\n",
      "58000 steps -> loss: 0.001512\n",
      "59000 steps -> loss: 0.001512\n",
      "60000 steps -> loss: 0.001511\n",
      "61000 steps -> loss: 0.001511\n",
      "62000 steps -> loss: 0.001510\n",
      "63000 steps -> loss: 0.001510\n",
      "64000 steps -> loss: 0.001510\n",
      "65000 steps -> loss: 0.001509\n",
      "66000 steps -> loss: 0.001509\n",
      "67000 steps -> loss: 0.001508\n",
      "68000 steps -> loss: 0.001508\n",
      "69000 steps -> loss: 0.001507\n",
      "70000 steps -> loss: 0.001507\n",
      "71000 steps -> loss: 0.001507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000 steps -> loss: 0.001506\n",
      "73000 steps -> loss: 0.001507\n",
      "74000 steps -> loss: 0.001505\n",
      "75000 steps -> loss: 0.001505\n",
      "76000 steps -> loss: 0.001505\n",
      "77000 steps -> loss: 0.001504\n",
      "78000 steps -> loss: 0.001511\n",
      "79000 steps -> loss: 0.001503\n",
      "80000 steps -> loss: 0.001503\n",
      "81000 steps -> loss: 0.001504\n",
      "82000 steps -> loss: 0.001502\n",
      "83000 steps -> loss: 0.001502\n",
      "84000 steps -> loss: 0.001502\n",
      "85000 steps -> loss: 0.001503\n",
      "86000 steps -> loss: 0.001501\n",
      "87000 steps -> loss: 0.001500\n",
      "88000 steps -> loss: 0.001500\n",
      "89000 steps -> loss: 0.001504\n",
      "90000 steps -> loss: 0.001499\n",
      "91000 steps -> loss: 0.001499\n",
      "92000 steps -> loss: 0.001499\n",
      "93000 steps -> loss: 0.001498\n",
      "94000 steps -> loss: 0.001498\n",
      "95000 steps -> loss: 0.001498\n",
      "96000 steps -> loss: 0.001497\n",
      "97000 steps -> loss: 0.001503\n",
      "98000 steps -> loss: 0.001497\n",
      "99000 steps -> loss: 0.001496\n",
      "100000 steps -> loss: 0.001496\n",
      "Selected 3 parameters\n",
      "1000 steps -> loss: 0.004706\n",
      "2000 steps -> loss: 0.003029\n",
      "3000 steps -> loss: 0.002592\n",
      "4000 steps -> loss: 0.002307\n",
      "5000 steps -> loss: 0.002030\n",
      "6000 steps -> loss: 0.001743\n",
      "7000 steps -> loss: 0.001554\n",
      "8000 steps -> loss: 0.001471\n",
      "9000 steps -> loss: 0.001426\n",
      "10000 steps -> loss: 0.001386\n",
      "11000 steps -> loss: 0.001340\n",
      "12000 steps -> loss: 0.001303\n",
      "13000 steps -> loss: 0.001273\n",
      "14000 steps -> loss: 0.001249\n",
      "15000 steps -> loss: 0.001227\n",
      "16000 steps -> loss: 0.001210\n",
      "17000 steps -> loss: 0.001193\n",
      "18000 steps -> loss: 0.001180\n",
      "19000 steps -> loss: 0.001168\n",
      "20000 steps -> loss: 0.001157\n",
      "21000 steps -> loss: 0.001147\n",
      "22000 steps -> loss: 0.001139\n",
      "23000 steps -> loss: 0.001132\n",
      "24000 steps -> loss: 0.001127\n",
      "25000 steps -> loss: 0.001122\n",
      "26000 steps -> loss: 0.001120\n",
      "27000 steps -> loss: 0.001115\n",
      "28000 steps -> loss: 0.001113\n",
      "29000 steps -> loss: 0.001110\n",
      "30000 steps -> loss: 0.001107\n",
      "31000 steps -> loss: 0.001105\n",
      "32000 steps -> loss: 0.001103\n",
      "33000 steps -> loss: 0.001102\n",
      "34000 steps -> loss: 0.001100\n",
      "35000 steps -> loss: 0.001099\n",
      "36000 steps -> loss: 0.001097\n",
      "37000 steps -> loss: 0.001096\n",
      "38000 steps -> loss: 0.001095\n",
      "39000 steps -> loss: 0.001094\n",
      "40000 steps -> loss: 0.001092\n",
      "41000 steps -> loss: 0.001091\n",
      "42000 steps -> loss: 0.001090\n",
      "43000 steps -> loss: 0.001090\n",
      "44000 steps -> loss: 0.001089\n",
      "45000 steps -> loss: 0.001088\n",
      "46000 steps -> loss: 0.001087\n",
      "47000 steps -> loss: 0.001086\n",
      "48000 steps -> loss: 0.001085\n",
      "49000 steps -> loss: 0.001093\n",
      "50000 steps -> loss: 0.001084\n",
      "51000 steps -> loss: 0.001083\n",
      "52000 steps -> loss: 0.001082\n",
      "53000 steps -> loss: 0.001083\n",
      "54000 steps -> loss: 0.001081\n",
      "55000 steps -> loss: 0.001080\n",
      "56000 steps -> loss: 0.001080\n",
      "57000 steps -> loss: 0.001083\n",
      "58000 steps -> loss: 0.001078\n",
      "59000 steps -> loss: 0.001078\n",
      "60000 steps -> loss: 0.001077\n",
      "61000 steps -> loss: 0.001077\n",
      "62000 steps -> loss: 0.001076\n",
      "63000 steps -> loss: 0.001084\n",
      "64000 steps -> loss: 0.001075\n",
      "65000 steps -> loss: 0.001074\n",
      "66000 steps -> loss: 0.001074\n",
      "67000 steps -> loss: 0.001073\n",
      "68000 steps -> loss: 0.001072\n",
      "69000 steps -> loss: 0.001112\n",
      "70000 steps -> loss: 0.001071\n",
      "71000 steps -> loss: 0.001071\n",
      "72000 steps -> loss: 0.001070\n",
      "73000 steps -> loss: 0.001069\n",
      "74000 steps -> loss: 0.001069\n",
      "75000 steps -> loss: 0.001103\n",
      "76000 steps -> loss: 0.001068\n",
      "77000 steps -> loss: 0.001067\n",
      "78000 steps -> loss: 0.001066\n",
      "79000 steps -> loss: 0.001069\n",
      "80000 steps -> loss: 0.001065\n",
      "81000 steps -> loss: 0.001065\n",
      "82000 steps -> loss: 0.001063\n",
      "83000 steps -> loss: 0.001063\n",
      "84000 steps -> loss: 0.001062\n",
      "85000 steps -> loss: 0.001061\n",
      "86000 steps -> loss: 0.001061\n",
      "87000 steps -> loss: 0.001060\n",
      "88000 steps -> loss: 0.001059\n",
      "89000 steps -> loss: 0.001062\n",
      "90000 steps -> loss: 0.001058\n",
      "91000 steps -> loss: 0.001058\n",
      "92000 steps -> loss: 0.001056\n",
      "93000 steps -> loss: 0.001055\n",
      "94000 steps -> loss: 0.001055\n",
      "95000 steps -> loss: 0.001054\n",
      "96000 steps -> loss: 0.001053\n",
      "97000 steps -> loss: 0.001052\n",
      "98000 steps -> loss: 0.001053\n",
      "99000 steps -> loss: 0.001062\n",
      "100000 steps -> loss: 0.001050\n",
      "Selected 4 parameters\n",
      "1000 steps -> loss: 0.003109\n",
      "2000 steps -> loss: 0.002194\n",
      "3000 steps -> loss: 0.001743\n",
      "4000 steps -> loss: 0.001461\n",
      "5000 steps -> loss: 0.001333\n",
      "6000 steps -> loss: 0.001265\n",
      "7000 steps -> loss: 0.001226\n",
      "8000 steps -> loss: 0.001205\n",
      "9000 steps -> loss: 0.001192\n",
      "10000 steps -> loss: 0.001180\n",
      "11000 steps -> loss: 0.001169\n",
      "12000 steps -> loss: 0.001158\n",
      "13000 steps -> loss: 0.001147\n",
      "14000 steps -> loss: 0.001136\n",
      "15000 steps -> loss: 0.001126\n",
      "16000 steps -> loss: 0.001122\n",
      "17000 steps -> loss: 0.001109\n",
      "18000 steps -> loss: 0.001103\n",
      "19000 steps -> loss: 0.001088\n",
      "20000 steps -> loss: 0.001079\n",
      "21000 steps -> loss: 0.001071\n",
      "22000 steps -> loss: 0.001065\n",
      "23000 steps -> loss: 0.001060\n",
      "24000 steps -> loss: 0.001055\n",
      "25000 steps -> loss: 0.001051\n",
      "26000 steps -> loss: 0.001047\n",
      "27000 steps -> loss: 0.001043\n",
      "28000 steps -> loss: 0.001040\n",
      "29000 steps -> loss: 0.001036\n",
      "30000 steps -> loss: 0.001041\n",
      "31000 steps -> loss: 0.001030\n",
      "32000 steps -> loss: 0.001027\n",
      "33000 steps -> loss: 0.001024\n",
      "34000 steps -> loss: 0.001021\n",
      "35000 steps -> loss: 0.001018\n",
      "36000 steps -> loss: 0.001015\n",
      "37000 steps -> loss: 0.001012\n",
      "38000 steps -> loss: 0.001009\n",
      "39000 steps -> loss: 0.001041\n",
      "40000 steps -> loss: 0.001004\n",
      "41000 steps -> loss: 0.001001\n",
      "42000 steps -> loss: 0.000998\n",
      "43000 steps -> loss: 0.000995\n",
      "44000 steps -> loss: 0.000993\n",
      "45000 steps -> loss: 0.000990\n",
      "46000 steps -> loss: 0.000990\n",
      "47000 steps -> loss: 0.000985\n",
      "48000 steps -> loss: 0.000982\n",
      "49000 steps -> loss: 0.000979\n",
      "50000 steps -> loss: 0.000977\n",
      "51000 steps -> loss: 0.000975\n",
      "52000 steps -> loss: 0.000972\n",
      "53000 steps -> loss: 0.000970\n",
      "54000 steps -> loss: 0.000968\n",
      "55000 steps -> loss: 0.000965\n",
      "56000 steps -> loss: 0.000963\n",
      "57000 steps -> loss: 0.000961\n",
      "58000 steps -> loss: 0.000959\n",
      "59000 steps -> loss: 0.000959\n",
      "60000 steps -> loss: 0.000963\n",
      "61000 steps -> loss: 0.000958\n",
      "62000 steps -> loss: 0.000952\n",
      "63000 steps -> loss: 0.001010\n",
      "64000 steps -> loss: 0.000949\n",
      "65000 steps -> loss: 0.000948\n",
      "66000 steps -> loss: 0.000946\n",
      "67000 steps -> loss: 0.000945\n",
      "68000 steps -> loss: 0.000943\n",
      "69000 steps -> loss: 0.000942\n",
      "70000 steps -> loss: 0.000940\n",
      "71000 steps -> loss: 0.000939\n",
      "72000 steps -> loss: 0.000937\n",
      "73000 steps -> loss: 0.000936\n",
      "74000 steps -> loss: 0.000934\n",
      "75000 steps -> loss: 0.000933\n",
      "76000 steps -> loss: 0.000931\n",
      "77000 steps -> loss: 0.000929\n",
      "78000 steps -> loss: 0.000926\n",
      "79000 steps -> loss: 0.000924\n",
      "80000 steps -> loss: 0.000921\n",
      "81000 steps -> loss: 0.000919\n",
      "82000 steps -> loss: 0.000916\n",
      "83000 steps -> loss: 0.000914\n",
      "84000 steps -> loss: 0.000911\n",
      "85000 steps -> loss: 0.000915\n",
      "86000 steps -> loss: 0.000906\n",
      "87000 steps -> loss: 0.000929\n",
      "88000 steps -> loss: 0.000901\n",
      "89000 steps -> loss: 0.000899\n",
      "90000 steps -> loss: 0.000896\n",
      "91000 steps -> loss: 0.000894\n",
      "92000 steps -> loss: 0.000892\n",
      "93000 steps -> loss: 0.000889\n",
      "94000 steps -> loss: 0.000887\n",
      "95000 steps -> loss: 0.000885\n",
      "96000 steps -> loss: 0.000884\n",
      "97000 steps -> loss: 0.000881\n",
      "98000 steps -> loss: 0.000879\n",
      "99000 steps -> loss: 0.000877\n",
      "100000 steps -> loss: 0.000875\n",
      "Selected 5 parameters\n",
      "1000 steps -> loss: 0.005054\n",
      "2000 steps -> loss: 0.002689\n",
      "3000 steps -> loss: 0.001792\n",
      "4000 steps -> loss: 0.001438\n",
      "5000 steps -> loss: 0.001290\n",
      "6000 steps -> loss: 0.001172\n",
      "7000 steps -> loss: 0.001037\n",
      "8000 steps -> loss: 0.000929\n",
      "9000 steps -> loss: 0.000869\n",
      "10000 steps -> loss: 0.000820\n",
      "11000 steps -> loss: 0.000774\n",
      "12000 steps -> loss: 0.000726\n",
      "13000 steps -> loss: 0.000661\n",
      "14000 steps -> loss: 0.000581\n",
      "15000 steps -> loss: 0.000513\n",
      "16000 steps -> loss: 0.000465\n",
      "17000 steps -> loss: 0.000433\n",
      "18000 steps -> loss: 0.000410\n",
      "19000 steps -> loss: 0.000393\n",
      "20000 steps -> loss: 0.000408\n",
      "21000 steps -> loss: 0.000371\n",
      "22000 steps -> loss: 0.000363\n",
      "23000 steps -> loss: 0.000357\n",
      "24000 steps -> loss: 0.000352\n",
      "25000 steps -> loss: 0.000347\n",
      "26000 steps -> loss: 0.000343\n",
      "27000 steps -> loss: 0.000339\n",
      "28000 steps -> loss: 0.000335\n",
      "29000 steps -> loss: 0.000332\n",
      "30000 steps -> loss: 0.000329\n",
      "31000 steps -> loss: 0.000327\n",
      "32000 steps -> loss: 0.000323\n",
      "33000 steps -> loss: 0.000321\n",
      "34000 steps -> loss: 0.000319\n",
      "35000 steps -> loss: 0.000319\n",
      "36000 steps -> loss: 0.000315\n",
      "37000 steps -> loss: 0.000313\n",
      "38000 steps -> loss: 0.000311\n",
      "39000 steps -> loss: 0.000317\n",
      "40000 steps -> loss: 0.000309\n",
      "41000 steps -> loss: 0.000307\n",
      "42000 steps -> loss: 0.000305\n",
      "43000 steps -> loss: 0.000304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44000 steps -> loss: 0.000302\n",
      "45000 steps -> loss: 0.000301\n",
      "46000 steps -> loss: 0.000320\n",
      "47000 steps -> loss: 0.000299\n",
      "48000 steps -> loss: 0.000298\n",
      "49000 steps -> loss: 0.000296\n",
      "50000 steps -> loss: 0.000295\n",
      "51000 steps -> loss: 0.000294\n",
      "52000 steps -> loss: 0.000293\n",
      "53000 steps -> loss: 0.000292\n",
      "54000 steps -> loss: 0.000293\n",
      "55000 steps -> loss: 0.000291\n",
      "56000 steps -> loss: 0.000290\n",
      "57000 steps -> loss: 0.000289\n",
      "58000 steps -> loss: 0.000288\n",
      "59000 steps -> loss: 0.000287\n",
      "60000 steps -> loss: 0.000286\n",
      "61000 steps -> loss: 0.000285\n",
      "62000 steps -> loss: 0.000285\n",
      "63000 steps -> loss: 0.000284\n",
      "64000 steps -> loss: 0.000283\n",
      "65000 steps -> loss: 0.000282\n",
      "66000 steps -> loss: 0.000282\n",
      "67000 steps -> loss: 0.000281\n",
      "68000 steps -> loss: 0.000280\n",
      "69000 steps -> loss: 0.000280\n",
      "70000 steps -> loss: 0.000279\n",
      "71000 steps -> loss: 0.000278\n",
      "72000 steps -> loss: 0.000277\n",
      "73000 steps -> loss: 0.000277\n",
      "74000 steps -> loss: 0.000276\n",
      "75000 steps -> loss: 0.000276\n",
      "76000 steps -> loss: 0.000275\n",
      "77000 steps -> loss: 0.000274\n",
      "78000 steps -> loss: 0.000274\n",
      "79000 steps -> loss: 0.000273\n",
      "80000 steps -> loss: 0.000273\n",
      "81000 steps -> loss: 0.000272\n",
      "82000 steps -> loss: 0.000272\n",
      "83000 steps -> loss: 0.000271\n",
      "84000 steps -> loss: 0.000274\n",
      "85000 steps -> loss: 0.000270\n",
      "86000 steps -> loss: 0.000270\n",
      "87000 steps -> loss: 0.000269\n",
      "88000 steps -> loss: 0.000269\n",
      "89000 steps -> loss: 0.000284\n",
      "90000 steps -> loss: 0.000269\n",
      "91000 steps -> loss: 0.000267\n",
      "92000 steps -> loss: 0.000267\n",
      "93000 steps -> loss: 0.000266\n",
      "94000 steps -> loss: 0.000266\n",
      "95000 steps -> loss: 0.000265\n",
      "96000 steps -> loss: 0.000265\n",
      "97000 steps -> loss: 0.000265\n",
      "98000 steps -> loss: 0.000272\n",
      "99000 steps -> loss: 0.000264\n",
      "100000 steps -> loss: 0.000263\n",
      "Selected 6 parameters\n",
      "1000 steps -> loss: 0.006178\n",
      "2000 steps -> loss: 0.002463\n",
      "3000 steps -> loss: 0.001808\n",
      "4000 steps -> loss: 0.001506\n",
      "5000 steps -> loss: 0.001335\n",
      "6000 steps -> loss: 0.001194\n",
      "7000 steps -> loss: 0.001075\n",
      "8000 steps -> loss: 0.000957\n",
      "9000 steps -> loss: 0.000780\n",
      "10000 steps -> loss: 0.000653\n",
      "11000 steps -> loss: 0.000599\n",
      "12000 steps -> loss: 0.000561\n",
      "13000 steps -> loss: 0.000531\n",
      "14000 steps -> loss: 0.000504\n",
      "15000 steps -> loss: 0.000480\n",
      "16000 steps -> loss: 0.000460\n",
      "17000 steps -> loss: 0.000444\n",
      "18000 steps -> loss: 0.000426\n",
      "19000 steps -> loss: 0.000413\n",
      "20000 steps -> loss: 0.000402\n",
      "21000 steps -> loss: 0.000392\n",
      "22000 steps -> loss: 0.000383\n",
      "23000 steps -> loss: 0.000375\n",
      "24000 steps -> loss: 0.000367\n",
      "25000 steps -> loss: 0.000361\n",
      "26000 steps -> loss: 0.000354\n",
      "27000 steps -> loss: 0.000348\n",
      "28000 steps -> loss: 0.000342\n",
      "29000 steps -> loss: 0.000341\n",
      "30000 steps -> loss: 0.000333\n",
      "31000 steps -> loss: 0.000328\n",
      "32000 steps -> loss: 0.000323\n",
      "33000 steps -> loss: 0.000335\n",
      "34000 steps -> loss: 0.000316\n",
      "35000 steps -> loss: 0.000312\n",
      "36000 steps -> loss: 0.000309\n",
      "37000 steps -> loss: 0.000305\n",
      "38000 steps -> loss: 0.000302\n",
      "39000 steps -> loss: 0.000299\n",
      "40000 steps -> loss: 0.000296\n",
      "41000 steps -> loss: 0.000326\n",
      "42000 steps -> loss: 0.000290\n",
      "43000 steps -> loss: 0.000287\n",
      "44000 steps -> loss: 0.000284\n",
      "45000 steps -> loss: 0.000281\n",
      "46000 steps -> loss: 0.000280\n",
      "47000 steps -> loss: 0.000276\n",
      "48000 steps -> loss: 0.000273\n",
      "49000 steps -> loss: 0.000271\n",
      "50000 steps -> loss: 0.000268\n",
      "51000 steps -> loss: 0.000267\n",
      "52000 steps -> loss: 0.000264\n",
      "53000 steps -> loss: 0.000262\n",
      "54000 steps -> loss: 0.000260\n",
      "55000 steps -> loss: 0.000261\n",
      "56000 steps -> loss: 0.000256\n",
      "57000 steps -> loss: 0.000255\n",
      "58000 steps -> loss: 0.000260\n",
      "59000 steps -> loss: 0.000252\n",
      "60000 steps -> loss: 0.000250\n",
      "61000 steps -> loss: 0.000249\n",
      "62000 steps -> loss: 0.000249\n",
      "63000 steps -> loss: 0.000246\n",
      "64000 steps -> loss: 0.000245\n",
      "65000 steps -> loss: 0.000248\n",
      "66000 steps -> loss: 0.000256\n",
      "67000 steps -> loss: 0.000262\n",
      "68000 steps -> loss: 0.000241\n",
      "69000 steps -> loss: 0.000240\n",
      "70000 steps -> loss: 0.000256\n",
      "71000 steps -> loss: 0.000238\n",
      "72000 steps -> loss: 0.000237\n",
      "73000 steps -> loss: 0.000236\n",
      "74000 steps -> loss: 0.000236\n",
      "75000 steps -> loss: 0.000235\n",
      "76000 steps -> loss: 0.000234\n",
      "77000 steps -> loss: 0.000233\n",
      "78000 steps -> loss: 0.000232\n",
      "79000 steps -> loss: 0.000232\n",
      "80000 steps -> loss: 0.000231\n",
      "81000 steps -> loss: 0.000230\n",
      "82000 steps -> loss: 0.000286\n",
      "83000 steps -> loss: 0.000229\n",
      "84000 steps -> loss: 0.000229\n",
      "85000 steps -> loss: 0.000228\n",
      "86000 steps -> loss: 0.000228\n",
      "87000 steps -> loss: 0.000227\n",
      "88000 steps -> loss: 0.000226\n",
      "89000 steps -> loss: 0.000228\n",
      "90000 steps -> loss: 0.000225\n",
      "91000 steps -> loss: 0.000225\n",
      "92000 steps -> loss: 0.000224\n",
      "93000 steps -> loss: 0.000227\n",
      "94000 steps -> loss: 0.000224\n",
      "95000 steps -> loss: 0.000223\n",
      "96000 steps -> loss: 0.000222\n",
      "97000 steps -> loss: 0.000222\n",
      "98000 steps -> loss: 0.000221\n",
      "99000 steps -> loss: 0.000221\n",
      "100000 steps -> loss: 0.000220\n",
      "Selected 7 parameters\n",
      "1000 steps -> loss: 0.002798\n",
      "2000 steps -> loss: 0.001647\n",
      "3000 steps -> loss: 0.001207\n",
      "4000 steps -> loss: 0.001040\n",
      "5000 steps -> loss: 0.000934\n",
      "6000 steps -> loss: 0.000861\n",
      "7000 steps -> loss: 0.000792\n",
      "8000 steps -> loss: 0.000714\n",
      "9000 steps -> loss: 0.000633\n",
      "10000 steps -> loss: 0.000571\n",
      "11000 steps -> loss: 0.000558\n",
      "12000 steps -> loss: 0.000482\n",
      "13000 steps -> loss: 0.000456\n",
      "14000 steps -> loss: 0.000429\n",
      "15000 steps -> loss: 0.000411\n",
      "16000 steps -> loss: 0.000394\n",
      "17000 steps -> loss: 0.000380\n",
      "18000 steps -> loss: 0.000368\n",
      "19000 steps -> loss: 0.000357\n",
      "20000 steps -> loss: 0.000347\n",
      "21000 steps -> loss: 0.000339\n",
      "22000 steps -> loss: 0.000332\n",
      "23000 steps -> loss: 0.000324\n",
      "24000 steps -> loss: 0.000318\n",
      "25000 steps -> loss: 0.000313\n",
      "26000 steps -> loss: 0.000306\n",
      "27000 steps -> loss: 0.000301\n",
      "28000 steps -> loss: 0.000296\n",
      "29000 steps -> loss: 0.000292\n",
      "30000 steps -> loss: 0.000287\n",
      "31000 steps -> loss: 0.000283\n",
      "32000 steps -> loss: 0.000280\n",
      "33000 steps -> loss: 0.000276\n",
      "34000 steps -> loss: 0.000273\n",
      "35000 steps -> loss: 0.000270\n",
      "36000 steps -> loss: 0.000267\n",
      "37000 steps -> loss: 0.000264\n",
      "38000 steps -> loss: 0.000262\n",
      "39000 steps -> loss: 0.000259\n",
      "40000 steps -> loss: 0.000257\n",
      "41000 steps -> loss: 0.000255\n",
      "42000 steps -> loss: 0.000253\n",
      "43000 steps -> loss: 0.000303\n",
      "44000 steps -> loss: 0.000249\n",
      "45000 steps -> loss: 0.000248\n",
      "46000 steps -> loss: 0.000245\n",
      "47000 steps -> loss: 0.000254\n",
      "48000 steps -> loss: 0.000241\n",
      "49000 steps -> loss: 0.000240\n",
      "50000 steps -> loss: 0.000238\n",
      "51000 steps -> loss: 0.000237\n",
      "52000 steps -> loss: 0.000235\n",
      "53000 steps -> loss: 0.000242\n",
      "54000 steps -> loss: 0.000232\n",
      "55000 steps -> loss: 0.000232\n",
      "56000 steps -> loss: 0.000234\n",
      "57000 steps -> loss: 0.000228\n",
      "58000 steps -> loss: 0.000228\n",
      "59000 steps -> loss: 0.000225\n",
      "60000 steps -> loss: 0.000228\n",
      "61000 steps -> loss: 0.000223\n",
      "62000 steps -> loss: 0.000222\n",
      "63000 steps -> loss: 0.000220\n",
      "64000 steps -> loss: 0.000219\n",
      "65000 steps -> loss: 0.000218\n",
      "66000 steps -> loss: 0.000217\n",
      "67000 steps -> loss: 0.000216\n",
      "68000 steps -> loss: 0.000215\n",
      "69000 steps -> loss: 0.000214\n",
      "70000 steps -> loss: 0.000213\n",
      "71000 steps -> loss: 0.000212\n",
      "72000 steps -> loss: 0.000211\n",
      "73000 steps -> loss: 0.000210\n",
      "74000 steps -> loss: 0.000209\n",
      "75000 steps -> loss: 0.000208\n",
      "76000 steps -> loss: 0.000207\n",
      "77000 steps -> loss: 0.000207\n",
      "78000 steps -> loss: 0.000205\n",
      "79000 steps -> loss: 0.000204\n",
      "80000 steps -> loss: 0.000203\n",
      "81000 steps -> loss: 0.000202\n",
      "82000 steps -> loss: 0.000202\n",
      "83000 steps -> loss: 0.000201\n",
      "84000 steps -> loss: 0.000200\n",
      "85000 steps -> loss: 0.000199\n",
      "86000 steps -> loss: 0.000199\n",
      "87000 steps -> loss: 0.000198\n",
      "88000 steps -> loss: 0.000197\n",
      "89000 steps -> loss: 0.000196\n",
      "90000 steps -> loss: 0.000195\n",
      "91000 steps -> loss: 0.000195\n",
      "92000 steps -> loss: 0.000194\n",
      "93000 steps -> loss: 0.000193\n",
      "94000 steps -> loss: 0.000193\n",
      "95000 steps -> loss: 0.000192\n",
      "96000 steps -> loss: 0.000195\n",
      "97000 steps -> loss: 0.000190\n",
      "98000 steps -> loss: 0.000190\n",
      "99000 steps -> loss: 0.000189\n",
      "100000 steps -> loss: 0.000188\n",
      "Selected 8 parameters\n",
      "1000 steps -> loss: 0.001369\n",
      "2000 steps -> loss: 0.001076\n",
      "3000 steps -> loss: 0.000913\n",
      "4000 steps -> loss: 0.000780\n",
      "5000 steps -> loss: 0.000683\n",
      "6000 steps -> loss: 0.000614\n",
      "7000 steps -> loss: 0.000560\n",
      "8000 steps -> loss: 0.000510\n",
      "9000 steps -> loss: 0.000460\n",
      "10000 steps -> loss: 0.000425\n",
      "11000 steps -> loss: 0.000397\n",
      "12000 steps -> loss: 0.000373\n",
      "13000 steps -> loss: 0.000375\n",
      "14000 steps -> loss: 0.000338\n",
      "15000 steps -> loss: 0.000325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 steps -> loss: 0.000314\n",
      "17000 steps -> loss: 0.000304\n",
      "18000 steps -> loss: 0.000296\n",
      "19000 steps -> loss: 0.000288\n",
      "20000 steps -> loss: 0.000281\n",
      "21000 steps -> loss: 0.000274\n",
      "22000 steps -> loss: 0.000268\n",
      "23000 steps -> loss: 0.000263\n",
      "24000 steps -> loss: 0.000258\n",
      "25000 steps -> loss: 0.000253\n",
      "26000 steps -> loss: 0.000251\n",
      "27000 steps -> loss: 0.000245\n",
      "28000 steps -> loss: 0.000241\n",
      "29000 steps -> loss: 0.000238\n",
      "30000 steps -> loss: 0.000235\n",
      "31000 steps -> loss: 0.000232\n",
      "32000 steps -> loss: 0.000229\n",
      "33000 steps -> loss: 0.000227\n",
      "34000 steps -> loss: 0.000224\n",
      "35000 steps -> loss: 0.000222\n",
      "36000 steps -> loss: 0.000220\n",
      "37000 steps -> loss: 0.000218\n",
      "38000 steps -> loss: 0.000216\n",
      "39000 steps -> loss: 0.000214\n",
      "40000 steps -> loss: 0.000213\n",
      "41000 steps -> loss: 0.000226\n",
      "42000 steps -> loss: 0.000210\n",
      "43000 steps -> loss: 0.000208\n",
      "44000 steps -> loss: 0.000207\n",
      "45000 steps -> loss: 0.000209\n",
      "46000 steps -> loss: 0.000204\n",
      "47000 steps -> loss: 0.000203\n",
      "48000 steps -> loss: 0.000202\n",
      "49000 steps -> loss: 0.000201\n",
      "50000 steps -> loss: 0.000200\n",
      "51000 steps -> loss: 0.000199\n",
      "52000 steps -> loss: 0.000205\n",
      "53000 steps -> loss: 0.000197\n",
      "54000 steps -> loss: 0.000196\n",
      "55000 steps -> loss: 0.000195\n",
      "56000 steps -> loss: 0.000195\n",
      "57000 steps -> loss: 0.000202\n",
      "58000 steps -> loss: 0.000193\n",
      "59000 steps -> loss: 0.000192\n",
      "60000 steps -> loss: 0.000191\n",
      "61000 steps -> loss: 0.000191\n",
      "62000 steps -> loss: 0.000190\n",
      "63000 steps -> loss: 0.000189\n",
      "64000 steps -> loss: 0.000188\n",
      "65000 steps -> loss: 0.000188\n",
      "66000 steps -> loss: 0.000187\n",
      "67000 steps -> loss: 0.000186\n",
      "68000 steps -> loss: 0.000186\n",
      "69000 steps -> loss: 0.000338\n",
      "70000 steps -> loss: 0.000184\n",
      "71000 steps -> loss: 0.000184\n",
      "72000 steps -> loss: 0.000183\n",
      "73000 steps -> loss: 0.000183\n",
      "74000 steps -> loss: 0.000183\n",
      "75000 steps -> loss: 0.000183\n",
      "76000 steps -> loss: 0.000182\n",
      "77000 steps -> loss: 0.000180\n",
      "78000 steps -> loss: 0.000180\n",
      "79000 steps -> loss: 0.000179\n",
      "80000 steps -> loss: 0.000179\n",
      "81000 steps -> loss: 0.000178\n",
      "82000 steps -> loss: 0.000178\n",
      "83000 steps -> loss: 0.000177\n",
      "84000 steps -> loss: 0.000176\n",
      "85000 steps -> loss: 0.000176\n",
      "86000 steps -> loss: 0.000175\n",
      "87000 steps -> loss: 0.000175\n",
      "88000 steps -> loss: 0.000174\n",
      "89000 steps -> loss: 0.000175\n",
      "90000 steps -> loss: 0.000187\n",
      "91000 steps -> loss: 0.000189\n",
      "92000 steps -> loss: 0.000177\n",
      "93000 steps -> loss: 0.000172\n",
      "94000 steps -> loss: 0.000172\n",
      "95000 steps -> loss: 0.000171\n",
      "96000 steps -> loss: 0.000171\n",
      "97000 steps -> loss: 0.000170\n",
      "98000 steps -> loss: 0.000170\n",
      "99000 steps -> loss: 0.000169\n",
      "100000 steps -> loss: 0.000169\n",
      "Selected 9 parameters\n",
      "1000 steps -> loss: 0.003979\n",
      "2000 steps -> loss: 0.002120\n",
      "3000 steps -> loss: 0.001706\n",
      "4000 steps -> loss: 0.001447\n",
      "5000 steps -> loss: 0.001237\n",
      "6000 steps -> loss: 0.001064\n",
      "7000 steps -> loss: 0.000933\n",
      "8000 steps -> loss: 0.000836\n",
      "9000 steps -> loss: 0.000750\n",
      "10000 steps -> loss: 0.000670\n",
      "11000 steps -> loss: 0.000605\n",
      "12000 steps -> loss: 0.000552\n",
      "13000 steps -> loss: 0.000512\n",
      "14000 steps -> loss: 0.000481\n",
      "15000 steps -> loss: 0.000456\n",
      "16000 steps -> loss: 0.000436\n",
      "17000 steps -> loss: 0.000432\n",
      "18000 steps -> loss: 0.000405\n",
      "19000 steps -> loss: 0.000393\n",
      "20000 steps -> loss: 0.000381\n",
      "21000 steps -> loss: 0.000371\n",
      "22000 steps -> loss: 0.000364\n",
      "23000 steps -> loss: 0.000353\n",
      "24000 steps -> loss: 0.000346\n",
      "25000 steps -> loss: 0.000339\n",
      "26000 steps -> loss: 0.000332\n",
      "27000 steps -> loss: 0.000326\n",
      "28000 steps -> loss: 0.000320\n",
      "29000 steps -> loss: 0.000314\n",
      "30000 steps -> loss: 0.000309\n",
      "31000 steps -> loss: 0.000304\n",
      "32000 steps -> loss: 0.000299\n",
      "33000 steps -> loss: 0.000295\n",
      "34000 steps -> loss: 0.000291\n",
      "35000 steps -> loss: 0.000287\n",
      "36000 steps -> loss: 0.000283\n",
      "37000 steps -> loss: 0.000279\n",
      "38000 steps -> loss: 0.000276\n",
      "39000 steps -> loss: 0.000273\n",
      "40000 steps -> loss: 0.000269\n",
      "41000 steps -> loss: 0.000265\n",
      "42000 steps -> loss: 0.000262\n",
      "43000 steps -> loss: 0.000259\n",
      "44000 steps -> loss: 0.000257\n",
      "45000 steps -> loss: 0.000254\n",
      "46000 steps -> loss: 0.000251\n",
      "47000 steps -> loss: 0.000249\n",
      "48000 steps -> loss: 0.000259\n",
      "49000 steps -> loss: 0.000245\n",
      "50000 steps -> loss: 0.000251\n",
      "51000 steps -> loss: 0.000240\n",
      "52000 steps -> loss: 0.000238\n",
      "53000 steps -> loss: 0.000237\n",
      "54000 steps -> loss: 0.000233\n",
      "55000 steps -> loss: 0.000245\n",
      "56000 steps -> loss: 0.000230\n",
      "57000 steps -> loss: 0.000228\n",
      "58000 steps -> loss: 0.000226\n",
      "59000 steps -> loss: 0.000224\n",
      "60000 steps -> loss: 0.000222\n",
      "61000 steps -> loss: 0.000302\n",
      "62000 steps -> loss: 0.000219\n",
      "63000 steps -> loss: 0.000217\n",
      "64000 steps -> loss: 0.000216\n",
      "65000 steps -> loss: 0.000219\n",
      "66000 steps -> loss: 0.000215\n",
      "67000 steps -> loss: 0.000211\n",
      "68000 steps -> loss: 0.000234\n",
      "69000 steps -> loss: 0.000208\n",
      "70000 steps -> loss: 0.000207\n",
      "71000 steps -> loss: 0.000205\n",
      "72000 steps -> loss: 0.000204\n",
      "73000 steps -> loss: 0.000203\n",
      "74000 steps -> loss: 0.000207\n",
      "75000 steps -> loss: 0.000200\n",
      "76000 steps -> loss: 0.000200\n",
      "77000 steps -> loss: 0.000198\n",
      "78000 steps -> loss: 0.000197\n",
      "79000 steps -> loss: 0.000196\n",
      "80000 steps -> loss: 0.000196\n",
      "81000 steps -> loss: 0.000194\n",
      "82000 steps -> loss: 0.000193\n",
      "83000 steps -> loss: 0.000192\n",
      "84000 steps -> loss: 0.000191\n",
      "85000 steps -> loss: 0.000190\n",
      "86000 steps -> loss: 0.000189\n",
      "87000 steps -> loss: 0.000188\n",
      "88000 steps -> loss: 0.000187\n",
      "89000 steps -> loss: 0.000188\n",
      "90000 steps -> loss: 0.000186\n",
      "91000 steps -> loss: 0.000185\n",
      "92000 steps -> loss: 0.000184\n",
      "93000 steps -> loss: 0.000183\n",
      "94000 steps -> loss: 0.000183\n",
      "95000 steps -> loss: 0.000182\n",
      "96000 steps -> loss: 0.000181\n",
      "97000 steps -> loss: 0.000180\n",
      "98000 steps -> loss: 0.000180\n",
      "99000 steps -> loss: 0.000179\n",
      "100000 steps -> loss: 0.000180\n",
      "Selected 10 parameters\n",
      "Time taken: 1:13:42.981509\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Correlation based selection using Tensorflow library\n",
    "(Training ANN by adding input parameters in order of descending correlation coefficient)\n",
    "Ver 1.2\n",
    "(Add input parameters manually)\n",
    "2 options for calculation of correlation\n",
    "(1.Pearson product-moment correlation/2.Volume-weighted correlation)\n",
    "'''\n",
    "'''\n",
    "Written in Aug.2017 by Serin Yoon\n",
    "Fluid & Thermal Multiphysics Lab.\n",
    "Department of Mechanical Engineering\n",
    "Sogang University\n",
    "'''\n",
    "\n",
    "#import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from datetime import datetime\n",
    "\n",
    "#Set the number of factors\n",
    "numb_data=44208 #the number of data points\n",
    "numb_input=13 #the number of input parameters to read\n",
    "numb_output=1 #the number of the total output parameters\n",
    "numb_neuron=20#the number of neurons for each hidden layer\n",
    "numb_select=10 #the number of parameters to select\n",
    "\n",
    "#Set the learning rate of an optimizer\n",
    "learning_rate=0.001\n",
    "\n",
    "#Set the training steps\n",
    "train_step=100000\n",
    "\n",
    "#Set the criterion option (0: Pearson-correlation coefficient / 1: Spearman-correaltion coefficient)\n",
    "criterion_opt=1\n",
    "\n",
    "#Array configuration to store data\n",
    "input=[[]for j in range(numb_input+2)]\n",
    "output=[[]for j in range(numb_output)]\n",
    "\n",
    "#Read csv file\n",
    "#How to configure the csv file : row = data points, column = parameters\n",
    "#Add input & output parameters to use for training manually\n",
    "#=> input[x].append(i[y]) : Read (x+1)th input parameter from (y+1)th column in the csv file \n",
    "#=> output[x].append(i[y]) : Read (x+1)th output parameter from (y+1)th column in the csv file\n",
    "f=open(\"./new_parameters(18.1.16)/new(18.1.23)/case1+2+3+4_modeling_without_nualpha_scaled_174by87.csv\",'r')\n",
    "csvReader=csv.reader(f)\n",
    "headers=next(csvReader)\n",
    "for i in csvReader:\n",
    "    for j in range(numb_input+2):\n",
    "        input[j].append(i[j])\n",
    "    for k in range(numb_output):\n",
    "        output[k].append(i[numb_input+2])\n",
    "\n",
    "#Convert the data to float type        \n",
    "for i in range(numb_data):\n",
    "    for j in range(numb_input+2):\n",
    "        input[j][i]=float(input[j][i])\n",
    "    for k in range(numb_output):\n",
    "        output[k][i]=float(output[k][i])   \n",
    "         \n",
    "#Calculate correlation coefficient between input and output\n",
    "Corr=[[1. for i in range(numb_output)]for j in range(numb_input)]\n",
    "\n",
    "#Calculate using Pearson method\n",
    "if criterion_opt==0:\n",
    "    print(\"Calculating correlation using Pearson method\")\n",
    "    for i in range(numb_input):\n",
    "        for k in range(numb_output):\n",
    "            #Corr[i][k]=abs(np.corrcoef(input[i],output[k])[0,1])\n",
    "            Corr[i][k],p=pearsonr(input[i],output[k])\n",
    "            Corr[i][k]=abs(Corr[i][k])\n",
    "            \n",
    "else :\n",
    "    print(\"Calculating correlation using Spearman method\")\n",
    "    for i in range(numb_input):\n",
    "        for k in range(numb_output):\n",
    "            Corr[i][k],p=spearmanr(input[i],output[k])\n",
    "            Corr[i][k]=abs(Corr[i][k])\n",
    "            \n",
    "#Set the new input array in descending order of the correlation (Set the 'reverse' option as 'True')\n",
    "sorted_Corr_index=sorted(range(len(Corr)), key=Corr.__getitem__, reverse=True)\n",
    "input_training=[[1. for i in range(numb_data)]for j in range(numb_input+2)]\n",
    "for i in range(numb_data):\n",
    "    input_training[0][i]=input[0][i]\n",
    "    input_training[1][i]=input[1][i]\n",
    "    for j in range(numb_input):\n",
    "        input_training[j+2][i]=input[sorted_Corr_index[j]+2][i]\n",
    "\n",
    "#Store the correlation coefficient values to a text file       \n",
    "Head=[\"\" for x in range(numb_input)]\n",
    "for i in range(numb_input):\n",
    "    Head[i]=headers[sorted_Corr_index[i]+2]\n",
    "\n",
    "fp = open('./new_parameters(18.1.16)/new(18.1.23)/Correlation_btw_input&output.txt','w')\n",
    "for i in range(numb_input):\n",
    "    fp.write(Head[i]+' %.6f\\n' % Corr[i][0])\n",
    "fp.close()\n",
    "\n",
    "#Set arrays for calculation of a correlation between the target output and an actual output\n",
    "Corr_out=[[1. for i in range(numb_output)]for j in range(numb_output)]\n",
    "\n",
    "#Set arrays to store the relative errors and correlation of selected parameters\n",
    "Max_err=np.zeros(numb_select)\n",
    "Mean_err=np.zeros(numb_select)\n",
    "Pearson=np.zeros(numb_select) \n",
    "Spearman=np.zeros(numb_select) \n",
    "\n",
    "startTime = datetime.now()\n",
    "#The number of selected input parameters(t) increases one by one\n",
    "for t in range(numb_select):\n",
    "    numb_training=numb_input-t\n",
    "    input_training2=list(input_training[0:t+3])   \n",
    "\n",
    "    #Neural network structure    \n",
    "    with tf.name_scope(\"1st_hidden_layer\"):\n",
    "        w1 =tf.Variable(tf.random_uniform([numb_neuron,t+3],-1.0,1.0),name='Weight1')\n",
    "        b1 =tf.Variable(tf.random_uniform([numb_neuron,1],-1.0,1.0),name='Bias1')\n",
    "        layer1out=tf.tanh(tf.matmul(w1,input_training2)+b1,name=\"1st_layer_output\")\n",
    "\n",
    "#    with tf.name_scope(\"2nd_hidden_layer\"):\n",
    "#        w2 =tf.Variable(tf.random_uniform([numb_neuron,numb_neuron],-1.0,1.0),name='Weight2')\n",
    "#        b2 =tf.Variable(tf.random_uniform([numb_neuron,1],-1.0,1.0),name='Bias2')\n",
    "#        layer2out=tf.tanh(tf.matmul(w2,layer1out)+b2,name=\"2nd_layer_output\")\n",
    "\n",
    "    with tf.name_scope(\"Output_layer\"):\n",
    "        w3 =tf.Variable(tf.random_uniform([numb_output,numb_neuron],-1.0,1.0),name='Weight3')\n",
    "        finalout=tf.matmul(w3,layer1out,name=\"Final_output\")\n",
    "\n",
    "    #Configure the session to run the graph    \n",
    "    sess=tf.Session()\n",
    "\n",
    "    #Set the values to calculate\n",
    "    deltas=tf.abs(output-finalout)\n",
    "    squared_deltas = tf.square(deltas)\n",
    "    loss = tf.reduce_mean(squared_deltas)\n",
    "    max_value=tf.reduce_max(output)\n",
    "    max_error=tf.reduce_max(deltas)\n",
    "    mean_error=tf.reduce_mean(deltas)\n",
    "    relative_max_error=max_error*100\n",
    "    relative_mean_error=mean_error*100\n",
    "\n",
    "    #Create an optimizer\n",
    "    train = tf.train.AdamOptimizer(learning_rate,use_locking=False,name='AdamOptimizer').minimize(loss)\n",
    "\n",
    "    #Initialize the session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #Training\n",
    "    #input_=tf.placeholder(\"float\",shape=(t+1,None))\n",
    "    #feed_dict={input_:input_training2}\n",
    "    \n",
    "    for step in range(train_step):\n",
    "        sess.run(train)\n",
    "        if (step+1) % 1000 == 0:\n",
    "            print('%i steps -> loss: %f'%(step+1,sess.run(loss)))\n",
    "    corr1 ,p1 = pearsonr(output[0],sess.run(finalout)[0])\n",
    "    corr2 ,p2 = spearmanr(output[0],sess.run(finalout)[0])\n",
    "    Max_err[t]=sess.run(relative_max_error)\n",
    "    Mean_err[t]=sess.run(relative_mean_error)\n",
    "    Pearson[t]=abs(corr1)\n",
    "    Spearman[t]=abs(corr2)\n",
    "    \n",
    "    print('Selected %d parameters' %(t+1))\n",
    "print(\"Time taken:\", datetime.now()-startTime)    \n",
    "fmax = open('./new_parameters(18.1.16)/new(18.1.23)/Corr_Selection_Max_error(s).txt','w')\n",
    "fmean = open('./new_parameters(18.1.16)/new(18.1.23)/Corr_Selection_Mean_error(s).txt','w')\n",
    "fpearson = open('./new_parameters(18.1.16)/new(18.1.23)/Corr_Selection_Pearson(s).txt','w')\n",
    "fspearman = open('./new_parameters(18.1.16)/new(18.1.23)/Corr_Selection_Spearman(s).txt','w')\n",
    "\n",
    "for i in range(numb_select):\n",
    "    fmax.write('%.6f\\n' %Max_err[i])\n",
    "    fmean.write('%.6f\\n' %Mean_err[i])\n",
    "    fpearson.write('%.6f\\n' %Pearson[i])\n",
    "    fspearman.write('%.6f\\n' % Spearman[i])\n",
    "fmax.close()\n",
    "fmean.close()\n",
    "fpearson.close()    \n",
    "fspearman.close()\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
