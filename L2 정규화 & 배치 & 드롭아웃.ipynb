{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1000, train loss: 3.7796, test loss: 3.7894\n",
      "epoch: 2/1000, train loss: 3.5520, test loss: 3.5513\n",
      "epoch: 3/1000, train loss: 3.3506, test loss: 3.3543\n",
      "epoch: 4/1000, train loss: 3.1612, test loss: 3.1630\n",
      "epoch: 5/1000, train loss: 2.9855, test loss: 2.9892\n",
      "epoch: 6/1000, train loss: 2.8215, test loss: 2.8263\n",
      "epoch: 7/1000, train loss: 2.6674, test loss: 2.6720\n",
      "epoch: 8/1000, train loss: 2.5230, test loss: 2.5279\n",
      "epoch: 9/1000, train loss: 2.3869, test loss: 2.3920\n",
      "epoch: 10/1000, train loss: 2.2589, test loss: 2.2645\n",
      "epoch: 11/1000, train loss: 2.1383, test loss: 2.1440\n",
      "epoch: 12/1000, train loss: 2.0247, test loss: 2.0309\n",
      "epoch: 13/1000, train loss: 1.9175, test loss: 1.9238\n",
      "epoch: 14/1000, train loss: 1.8163, test loss: 1.8230\n",
      "epoch: 15/1000, train loss: 1.7208, test loss: 1.7278\n",
      "epoch: 16/1000, train loss: 1.6307, test loss: 1.6379\n",
      "epoch: 17/1000, train loss: 1.5455, test loss: 1.5529\n",
      "epoch: 18/1000, train loss: 1.4649, test loss: 1.4726\n",
      "epoch: 19/1000, train loss: 1.3889, test loss: 1.3968\n",
      "epoch: 20/1000, train loss: 1.3170, test loss: 1.3252\n",
      "epoch: 21/1000, train loss: 1.2491, test loss: 1.2574\n",
      "epoch: 22/1000, train loss: 1.1849, test loss: 1.1934\n",
      "epoch: 23/1000, train loss: 1.1243, test loss: 1.1328\n",
      "epoch: 24/1000, train loss: 1.0669, test loss: 1.0755\n",
      "epoch: 25/1000, train loss: 1.0127, test loss: 1.0213\n",
      "epoch: 26/1000, train loss: 0.9614, test loss: 0.9701\n",
      "epoch: 27/1000, train loss: 0.9130, test loss: 0.9216\n",
      "epoch: 28/1000, train loss: 0.8671, test loss: 0.8758\n",
      "epoch: 29/1000, train loss: 0.8238, test loss: 0.8324\n",
      "epoch: 30/1000, train loss: 0.7828, test loss: 0.7914\n",
      "epoch: 31/1000, train loss: 0.7441, test loss: 0.7526\n",
      "epoch: 32/1000, train loss: 0.7074, test loss: 0.7159\n",
      "epoch: 33/1000, train loss: 0.6728, test loss: 0.6812\n",
      "epoch: 34/1000, train loss: 0.6400, test loss: 0.6484\n",
      "epoch: 35/1000, train loss: 0.6090, test loss: 0.6174\n",
      "epoch: 36/1000, train loss: 0.5797, test loss: 0.5880\n",
      "epoch: 37/1000, train loss: 0.5519, test loss: 0.5602\n",
      "epoch: 38/1000, train loss: 0.5257, test loss: 0.5340\n",
      "epoch: 39/1000, train loss: 0.5009, test loss: 0.5092\n",
      "epoch: 40/1000, train loss: 0.4774, test loss: 0.4857\n",
      "epoch: 41/1000, train loss: 0.4553, test loss: 0.4635\n",
      "epoch: 42/1000, train loss: 0.4343, test loss: 0.4425\n",
      "epoch: 43/1000, train loss: 0.4144, test loss: 0.4226\n",
      "epoch: 44/1000, train loss: 0.3957, test loss: 0.4038\n",
      "epoch: 45/1000, train loss: 0.3779, test loss: 0.3861\n",
      "epoch: 46/1000, train loss: 0.3611, test loss: 0.3693\n",
      "epoch: 47/1000, train loss: 0.3453, test loss: 0.3535\n",
      "epoch: 48/1000, train loss: 0.3302, test loss: 0.3385\n",
      "epoch: 49/1000, train loss: 0.3160, test loss: 0.3244\n",
      "epoch: 50/1000, train loss: 0.3026, test loss: 0.3110\n",
      "epoch: 51/1000, train loss: 0.2899, test loss: 0.2983\n",
      "epoch: 52/1000, train loss: 0.2779, test loss: 0.2864\n",
      "epoch: 53/1000, train loss: 0.2665, test loss: 0.2751\n",
      "epoch: 54/1000, train loss: 0.2558, test loss: 0.2644\n",
      "epoch: 55/1000, train loss: 0.2456, test loss: 0.2543\n",
      "epoch: 56/1000, train loss: 0.2360, test loss: 0.2448\n",
      "epoch: 57/1000, train loss: 0.2269, test loss: 0.2357\n",
      "epoch: 58/1000, train loss: 0.2182, test loss: 0.2272\n",
      "epoch: 59/1000, train loss: 0.2101, test loss: 0.2190\n",
      "epoch: 60/1000, train loss: 0.2024, test loss: 0.2113\n",
      "epoch: 61/1000, train loss: 0.1950, test loss: 0.2039\n",
      "epoch: 62/1000, train loss: 0.1881, test loss: 0.1969\n",
      "epoch: 63/1000, train loss: 0.1816, test loss: 0.1903\n",
      "epoch: 64/1000, train loss: 0.1754, test loss: 0.1841\n",
      "epoch: 65/1000, train loss: 0.1695, test loss: 0.1781\n",
      "epoch: 66/1000, train loss: 0.1640, test loss: 0.1726\n",
      "epoch: 67/1000, train loss: 0.1587, test loss: 0.1673\n",
      "epoch: 68/1000, train loss: 0.1537, test loss: 0.1623\n",
      "epoch: 69/1000, train loss: 0.1490, test loss: 0.1576\n",
      "epoch: 70/1000, train loss: 0.1445, test loss: 0.1531\n",
      "epoch: 71/1000, train loss: 0.1402, test loss: 0.1488\n",
      "epoch: 72/1000, train loss: 0.1362, test loss: 0.1448\n",
      "epoch: 73/1000, train loss: 0.1324, test loss: 0.1409\n",
      "epoch: 74/1000, train loss: 0.1288, test loss: 0.1373\n",
      "epoch: 75/1000, train loss: 0.1253, test loss: 0.1339\n",
      "epoch: 76/1000, train loss: 0.1220, test loss: 0.1306\n",
      "epoch: 77/1000, train loss: 0.1189, test loss: 0.1275\n",
      "epoch: 78/1000, train loss: 0.1160, test loss: 0.1245\n",
      "epoch: 79/1000, train loss: 0.1132, test loss: 0.1217\n",
      "epoch: 80/1000, train loss: 0.1105, test loss: 0.1190\n",
      "epoch: 81/1000, train loss: 0.1080, test loss: 0.1165\n",
      "epoch: 82/1000, train loss: 0.1056, test loss: 0.1141\n",
      "epoch: 83/1000, train loss: 0.1033, test loss: 0.1118\n",
      "epoch: 84/1000, train loss: 0.1011, test loss: 0.1096\n",
      "epoch: 85/1000, train loss: 0.0991, test loss: 0.1075\n",
      "epoch: 86/1000, train loss: 0.0971, test loss: 0.1056\n",
      "epoch: 87/1000, train loss: 0.0952, test loss: 0.1037\n",
      "epoch: 88/1000, train loss: 0.0934, test loss: 0.1019\n",
      "epoch: 89/1000, train loss: 0.0917, test loss: 0.1001\n",
      "epoch: 90/1000, train loss: 0.0901, test loss: 0.0985\n",
      "epoch: 91/1000, train loss: 0.0885, test loss: 0.0969\n",
      "epoch: 92/1000, train loss: 0.0870, test loss: 0.0955\n",
      "epoch: 93/1000, train loss: 0.0856, test loss: 0.0940\n",
      "epoch: 94/1000, train loss: 0.0842, test loss: 0.0927\n",
      "epoch: 95/1000, train loss: 0.0829, test loss: 0.0914\n",
      "epoch: 96/1000, train loss: 0.0817, test loss: 0.0901\n",
      "epoch: 97/1000, train loss: 0.0805, test loss: 0.0889\n",
      "epoch: 98/1000, train loss: 0.0794, test loss: 0.0878\n",
      "epoch: 99/1000, train loss: 0.0783, test loss: 0.0867\n",
      "epoch: 100/1000, train loss: 0.0772, test loss: 0.0856\n",
      "epoch: 101/1000, train loss: 0.0762, test loss: 0.0846\n",
      "epoch: 102/1000, train loss: 0.0752, test loss: 0.0837\n",
      "epoch: 103/1000, train loss: 0.0743, test loss: 0.0827\n",
      "epoch: 104/1000, train loss: 0.0734, test loss: 0.0818\n",
      "epoch: 105/1000, train loss: 0.0726, test loss: 0.0810\n",
      "epoch: 106/1000, train loss: 0.0717, test loss: 0.0802\n",
      "epoch: 107/1000, train loss: 0.0709, test loss: 0.0794\n",
      "epoch: 108/1000, train loss: 0.0702, test loss: 0.0786\n",
      "epoch: 109/1000, train loss: 0.0694, test loss: 0.0779\n",
      "epoch: 110/1000, train loss: 0.0687, test loss: 0.0771\n",
      "epoch: 111/1000, train loss: 0.0680, test loss: 0.0765\n",
      "epoch: 112/1000, train loss: 0.0673, test loss: 0.0758\n",
      "epoch: 113/1000, train loss: 0.0667, test loss: 0.0752\n",
      "epoch: 114/1000, train loss: 0.0661, test loss: 0.0745\n",
      "epoch: 115/1000, train loss: 0.0655, test loss: 0.0739\n",
      "epoch: 116/1000, train loss: 0.0649, test loss: 0.0734\n",
      "epoch: 117/1000, train loss: 0.0643, test loss: 0.0728\n",
      "epoch: 118/1000, train loss: 0.0638, test loss: 0.0723\n",
      "epoch: 119/1000, train loss: 0.0632, test loss: 0.0718\n",
      "epoch: 120/1000, train loss: 0.0627, test loss: 0.0713\n",
      "epoch: 121/1000, train loss: 0.0622, test loss: 0.0708\n",
      "epoch: 122/1000, train loss: 0.0617, test loss: 0.0703\n",
      "epoch: 123/1000, train loss: 0.0612, test loss: 0.0698\n",
      "epoch: 124/1000, train loss: 0.0608, test loss: 0.0694\n",
      "epoch: 125/1000, train loss: 0.0603, test loss: 0.0690\n",
      "epoch: 126/1000, train loss: 0.0599, test loss: 0.0685\n",
      "epoch: 127/1000, train loss: 0.0595, test loss: 0.0681\n",
      "epoch: 128/1000, train loss: 0.0590, test loss: 0.0677\n",
      "epoch: 129/1000, train loss: 0.0586, test loss: 0.0673\n",
      "epoch: 130/1000, train loss: 0.0582, test loss: 0.0670\n",
      "epoch: 131/1000, train loss: 0.0578, test loss: 0.0666\n",
      "epoch: 132/1000, train loss: 0.0575, test loss: 0.0662\n",
      "epoch: 133/1000, train loss: 0.0571, test loss: 0.0659\n",
      "epoch: 134/1000, train loss: 0.0567, test loss: 0.0655\n",
      "epoch: 135/1000, train loss: 0.0564, test loss: 0.0652\n",
      "epoch: 136/1000, train loss: 0.0560, test loss: 0.0649\n",
      "epoch: 137/1000, train loss: 0.0557, test loss: 0.0646\n",
      "epoch: 138/1000, train loss: 0.0553, test loss: 0.0643\n",
      "epoch: 139/1000, train loss: 0.0550, test loss: 0.0639\n",
      "epoch: 140/1000, train loss: 0.0547, test loss: 0.0636\n",
      "epoch: 141/1000, train loss: 0.0544, test loss: 0.0633\n",
      "epoch: 142/1000, train loss: 0.0540, test loss: 0.0631\n",
      "epoch: 143/1000, train loss: 0.0537, test loss: 0.0628\n",
      "epoch: 144/1000, train loss: 0.0534, test loss: 0.0625\n",
      "epoch: 145/1000, train loss: 0.0531, test loss: 0.0622\n",
      "epoch: 146/1000, train loss: 0.0528, test loss: 0.0619\n",
      "epoch: 147/1000, train loss: 0.0526, test loss: 0.0617\n",
      "epoch: 148/1000, train loss: 0.0523, test loss: 0.0614\n",
      "epoch: 149/1000, train loss: 0.0520, test loss: 0.0611\n",
      "epoch: 150/1000, train loss: 0.0517, test loss: 0.0609\n",
      "epoch: 151/1000, train loss: 0.0515, test loss: 0.0606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 152/1000, train loss: 0.0512, test loss: 0.0603\n",
      "epoch: 153/1000, train loss: 0.0509, test loss: 0.0601\n",
      "epoch: 154/1000, train loss: 0.0507, test loss: 0.0598\n",
      "epoch: 155/1000, train loss: 0.0504, test loss: 0.0596\n",
      "epoch: 156/1000, train loss: 0.0501, test loss: 0.0593\n",
      "epoch: 157/1000, train loss: 0.0499, test loss: 0.0591\n",
      "epoch: 158/1000, train loss: 0.0497, test loss: 0.0589\n",
      "epoch: 159/1000, train loss: 0.0494, test loss: 0.0586\n",
      "epoch: 160/1000, train loss: 0.0492, test loss: 0.0584\n",
      "epoch: 161/1000, train loss: 0.0489, test loss: 0.0581\n",
      "epoch: 162/1000, train loss: 0.0487, test loss: 0.0579\n",
      "epoch: 163/1000, train loss: 0.0485, test loss: 0.0577\n",
      "epoch: 164/1000, train loss: 0.0482, test loss: 0.0574\n",
      "epoch: 165/1000, train loss: 0.0480, test loss: 0.0572\n",
      "epoch: 166/1000, train loss: 0.0478, test loss: 0.0570\n",
      "epoch: 167/1000, train loss: 0.0475, test loss: 0.0568\n",
      "epoch: 168/1000, train loss: 0.0473, test loss: 0.0566\n",
      "epoch: 169/1000, train loss: 0.0471, test loss: 0.0563\n",
      "epoch: 170/1000, train loss: 0.0469, test loss: 0.0561\n",
      "epoch: 171/1000, train loss: 0.0467, test loss: 0.0559\n",
      "epoch: 172/1000, train loss: 0.0465, test loss: 0.0557\n",
      "epoch: 173/1000, train loss: 0.0463, test loss: 0.0555\n",
      "epoch: 174/1000, train loss: 0.0460, test loss: 0.0553\n",
      "epoch: 175/1000, train loss: 0.0458, test loss: 0.0551\n",
      "epoch: 176/1000, train loss: 0.0456, test loss: 0.0548\n",
      "epoch: 177/1000, train loss: 0.0454, test loss: 0.0546\n",
      "epoch: 178/1000, train loss: 0.0452, test loss: 0.0544\n",
      "epoch: 179/1000, train loss: 0.0450, test loss: 0.0542\n",
      "epoch: 180/1000, train loss: 0.0448, test loss: 0.0540\n",
      "epoch: 181/1000, train loss: 0.0446, test loss: 0.0538\n",
      "epoch: 182/1000, train loss: 0.0444, test loss: 0.0536\n",
      "epoch: 183/1000, train loss: 0.0442, test loss: 0.0534\n",
      "epoch: 184/1000, train loss: 0.0440, test loss: 0.0532\n",
      "epoch: 185/1000, train loss: 0.0439, test loss: 0.0530\n",
      "epoch: 186/1000, train loss: 0.0437, test loss: 0.0528\n",
      "epoch: 187/1000, train loss: 0.0435, test loss: 0.0526\n",
      "epoch: 188/1000, train loss: 0.0433, test loss: 0.0524\n",
      "epoch: 189/1000, train loss: 0.0431, test loss: 0.0522\n",
      "epoch: 190/1000, train loss: 0.0429, test loss: 0.0520\n",
      "epoch: 191/1000, train loss: 0.0427, test loss: 0.0518\n",
      "epoch: 192/1000, train loss: 0.0426, test loss: 0.0517\n",
      "epoch: 193/1000, train loss: 0.0424, test loss: 0.0515\n",
      "epoch: 194/1000, train loss: 0.0422, test loss: 0.0513\n",
      "epoch: 195/1000, train loss: 0.0420, test loss: 0.0511\n",
      "epoch: 196/1000, train loss: 0.0418, test loss: 0.0509\n",
      "epoch: 197/1000, train loss: 0.0417, test loss: 0.0507\n",
      "epoch: 198/1000, train loss: 0.0415, test loss: 0.0505\n",
      "epoch: 199/1000, train loss: 0.0413, test loss: 0.0503\n",
      "epoch: 200/1000, train loss: 0.0411, test loss: 0.0501\n",
      "epoch: 201/1000, train loss: 0.0410, test loss: 0.0499\n",
      "epoch: 202/1000, train loss: 0.0408, test loss: 0.0498\n",
      "epoch: 203/1000, train loss: 0.0406, test loss: 0.0496\n",
      "epoch: 204/1000, train loss: 0.0405, test loss: 0.0494\n",
      "epoch: 205/1000, train loss: 0.0403, test loss: 0.0492\n",
      "epoch: 206/1000, train loss: 0.0401, test loss: 0.0490\n",
      "epoch: 207/1000, train loss: 0.0400, test loss: 0.0488\n",
      "epoch: 208/1000, train loss: 0.0398, test loss: 0.0486\n",
      "epoch: 209/1000, train loss: 0.0396, test loss: 0.0485\n",
      "epoch: 210/1000, train loss: 0.0395, test loss: 0.0483\n",
      "epoch: 211/1000, train loss: 0.0393, test loss: 0.0481\n",
      "epoch: 212/1000, train loss: 0.0391, test loss: 0.0479\n",
      "epoch: 213/1000, train loss: 0.0390, test loss: 0.0478\n",
      "epoch: 214/1000, train loss: 0.0388, test loss: 0.0476\n",
      "epoch: 215/1000, train loss: 0.0387, test loss: 0.0474\n",
      "epoch: 216/1000, train loss: 0.0385, test loss: 0.0472\n",
      "epoch: 217/1000, train loss: 0.0384, test loss: 0.0471\n",
      "epoch: 218/1000, train loss: 0.0382, test loss: 0.0469\n",
      "epoch: 219/1000, train loss: 0.0380, test loss: 0.0467\n",
      "epoch: 220/1000, train loss: 0.0379, test loss: 0.0466\n",
      "epoch: 221/1000, train loss: 0.0377, test loss: 0.0464\n",
      "epoch: 222/1000, train loss: 0.0376, test loss: 0.0462\n",
      "epoch: 223/1000, train loss: 0.0374, test loss: 0.0461\n",
      "epoch: 224/1000, train loss: 0.0373, test loss: 0.0459\n",
      "epoch: 225/1000, train loss: 0.0371, test loss: 0.0457\n",
      "epoch: 226/1000, train loss: 0.0370, test loss: 0.0456\n",
      "epoch: 227/1000, train loss: 0.0368, test loss: 0.0454\n",
      "epoch: 228/1000, train loss: 0.0367, test loss: 0.0453\n",
      "epoch: 229/1000, train loss: 0.0365, test loss: 0.0451\n",
      "epoch: 230/1000, train loss: 0.0364, test loss: 0.0449\n",
      "epoch: 231/1000, train loss: 0.0362, test loss: 0.0448\n",
      "epoch: 232/1000, train loss: 0.0361, test loss: 0.0446\n",
      "epoch: 233/1000, train loss: 0.0360, test loss: 0.0445\n",
      "epoch: 234/1000, train loss: 0.0358, test loss: 0.0443\n",
      "epoch: 235/1000, train loss: 0.0357, test loss: 0.0441\n",
      "epoch: 236/1000, train loss: 0.0355, test loss: 0.0440\n",
      "epoch: 237/1000, train loss: 0.0354, test loss: 0.0438\n",
      "epoch: 238/1000, train loss: 0.0352, test loss: 0.0437\n",
      "epoch: 239/1000, train loss: 0.0351, test loss: 0.0435\n",
      "epoch: 240/1000, train loss: 0.0350, test loss: 0.0434\n",
      "epoch: 241/1000, train loss: 0.0348, test loss: 0.0432\n",
      "epoch: 242/1000, train loss: 0.0347, test loss: 0.0431\n",
      "epoch: 243/1000, train loss: 0.0345, test loss: 0.0429\n",
      "epoch: 244/1000, train loss: 0.0344, test loss: 0.0428\n",
      "epoch: 245/1000, train loss: 0.0343, test loss: 0.0426\n",
      "epoch: 246/1000, train loss: 0.0341, test loss: 0.0425\n",
      "epoch: 247/1000, train loss: 0.0340, test loss: 0.0423\n",
      "epoch: 248/1000, train loss: 0.0339, test loss: 0.0421\n",
      "epoch: 249/1000, train loss: 0.0337, test loss: 0.0420\n",
      "epoch: 250/1000, train loss: 0.0336, test loss: 0.0418\n",
      "epoch: 251/1000, train loss: 0.0335, test loss: 0.0417\n",
      "epoch: 252/1000, train loss: 0.0333, test loss: 0.0415\n",
      "epoch: 253/1000, train loss: 0.0332, test loss: 0.0414\n",
      "epoch: 254/1000, train loss: 0.0331, test loss: 0.0413\n",
      "epoch: 255/1000, train loss: 0.0329, test loss: 0.0411\n",
      "epoch: 256/1000, train loss: 0.0328, test loss: 0.0410\n",
      "epoch: 257/1000, train loss: 0.0327, test loss: 0.0408\n",
      "epoch: 258/1000, train loss: 0.0326, test loss: 0.0407\n",
      "epoch: 259/1000, train loss: 0.0324, test loss: 0.0405\n",
      "epoch: 260/1000, train loss: 0.0323, test loss: 0.0404\n",
      "epoch: 261/1000, train loss: 0.0322, test loss: 0.0402\n",
      "epoch: 262/1000, train loss: 0.0321, test loss: 0.0401\n",
      "epoch: 263/1000, train loss: 0.0319, test loss: 0.0399\n",
      "epoch: 264/1000, train loss: 0.0318, test loss: 0.0398\n",
      "epoch: 265/1000, train loss: 0.0317, test loss: 0.0396\n",
      "epoch: 266/1000, train loss: 0.0316, test loss: 0.0395\n",
      "epoch: 267/1000, train loss: 0.0314, test loss: 0.0393\n",
      "epoch: 268/1000, train loss: 0.0313, test loss: 0.0392\n",
      "epoch: 269/1000, train loss: 0.0312, test loss: 0.0391\n",
      "epoch: 270/1000, train loss: 0.0311, test loss: 0.0389\n",
      "epoch: 271/1000, train loss: 0.0310, test loss: 0.0388\n",
      "epoch: 272/1000, train loss: 0.0308, test loss: 0.0386\n",
      "epoch: 273/1000, train loss: 0.0307, test loss: 0.0385\n",
      "epoch: 274/1000, train loss: 0.0306, test loss: 0.0383\n",
      "epoch: 275/1000, train loss: 0.0305, test loss: 0.0382\n",
      "epoch: 276/1000, train loss: 0.0304, test loss: 0.0381\n",
      "epoch: 277/1000, train loss: 0.0302, test loss: 0.0379\n",
      "epoch: 278/1000, train loss: 0.0301, test loss: 0.0378\n",
      "epoch: 279/1000, train loss: 0.0300, test loss: 0.0376\n",
      "epoch: 280/1000, train loss: 0.0299, test loss: 0.0375\n",
      "epoch: 281/1000, train loss: 0.0298, test loss: 0.0374\n",
      "epoch: 282/1000, train loss: 0.0297, test loss: 0.0372\n",
      "epoch: 283/1000, train loss: 0.0296, test loss: 0.0371\n",
      "epoch: 284/1000, train loss: 0.0294, test loss: 0.0369\n",
      "epoch: 285/1000, train loss: 0.0293, test loss: 0.0368\n",
      "epoch: 286/1000, train loss: 0.0292, test loss: 0.0367\n",
      "epoch: 287/1000, train loss: 0.0291, test loss: 0.0365\n",
      "epoch: 288/1000, train loss: 0.0290, test loss: 0.0364\n",
      "epoch: 289/1000, train loss: 0.0289, test loss: 0.0363\n",
      "epoch: 290/1000, train loss: 0.0288, test loss: 0.0361\n",
      "epoch: 291/1000, train loss: 0.0287, test loss: 0.0360\n",
      "epoch: 292/1000, train loss: 0.0286, test loss: 0.0359\n",
      "epoch: 293/1000, train loss: 0.0285, test loss: 0.0357\n",
      "epoch: 294/1000, train loss: 0.0283, test loss: 0.0356\n",
      "epoch: 295/1000, train loss: 0.0282, test loss: 0.0354\n",
      "epoch: 296/1000, train loss: 0.0281, test loss: 0.0353\n",
      "epoch: 297/1000, train loss: 0.0280, test loss: 0.0352\n",
      "epoch: 298/1000, train loss: 0.0279, test loss: 0.0350\n",
      "epoch: 299/1000, train loss: 0.0278, test loss: 0.0349\n",
      "epoch: 300/1000, train loss: 0.0277, test loss: 0.0348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 301/1000, train loss: 0.0276, test loss: 0.0346\n",
      "epoch: 302/1000, train loss: 0.0275, test loss: 0.0345\n",
      "epoch: 303/1000, train loss: 0.0274, test loss: 0.0344\n",
      "epoch: 304/1000, train loss: 0.0273, test loss: 0.0343\n",
      "epoch: 305/1000, train loss: 0.0272, test loss: 0.0341\n",
      "epoch: 306/1000, train loss: 0.0271, test loss: 0.0340\n",
      "epoch: 307/1000, train loss: 0.0270, test loss: 0.0339\n",
      "epoch: 308/1000, train loss: 0.0269, test loss: 0.0337\n",
      "epoch: 309/1000, train loss: 0.0268, test loss: 0.0336\n",
      "epoch: 310/1000, train loss: 0.0267, test loss: 0.0335\n",
      "epoch: 311/1000, train loss: 0.0266, test loss: 0.0333\n",
      "epoch: 312/1000, train loss: 0.0265, test loss: 0.0332\n",
      "epoch: 313/1000, train loss: 0.0264, test loss: 0.0331\n",
      "epoch: 314/1000, train loss: 0.0263, test loss: 0.0330\n",
      "epoch: 315/1000, train loss: 0.0262, test loss: 0.0328\n",
      "epoch: 316/1000, train loss: 0.0261, test loss: 0.0327\n",
      "epoch: 317/1000, train loss: 0.0260, test loss: 0.0326\n",
      "epoch: 318/1000, train loss: 0.0259, test loss: 0.0325\n",
      "epoch: 319/1000, train loss: 0.0258, test loss: 0.0323\n",
      "epoch: 320/1000, train loss: 0.0257, test loss: 0.0322\n",
      "epoch: 321/1000, train loss: 0.0256, test loss: 0.0321\n",
      "epoch: 322/1000, train loss: 0.0255, test loss: 0.0320\n",
      "epoch: 323/1000, train loss: 0.0254, test loss: 0.0318\n",
      "epoch: 324/1000, train loss: 0.0253, test loss: 0.0317\n",
      "epoch: 325/1000, train loss: 0.0252, test loss: 0.0316\n",
      "epoch: 326/1000, train loss: 0.0252, test loss: 0.0315\n",
      "epoch: 327/1000, train loss: 0.0251, test loss: 0.0313\n",
      "epoch: 328/1000, train loss: 0.0250, test loss: 0.0312\n",
      "epoch: 329/1000, train loss: 0.0249, test loss: 0.0311\n",
      "epoch: 330/1000, train loss: 0.0248, test loss: 0.0310\n",
      "epoch: 331/1000, train loss: 0.0247, test loss: 0.0309\n",
      "epoch: 332/1000, train loss: 0.0246, test loss: 0.0307\n",
      "epoch: 333/1000, train loss: 0.0245, test loss: 0.0306\n",
      "epoch: 334/1000, train loss: 0.0244, test loss: 0.0305\n",
      "epoch: 335/1000, train loss: 0.0243, test loss: 0.0304\n",
      "epoch: 336/1000, train loss: 0.0243, test loss: 0.0303\n",
      "epoch: 337/1000, train loss: 0.0242, test loss: 0.0301\n",
      "epoch: 338/1000, train loss: 0.0241, test loss: 0.0300\n",
      "epoch: 339/1000, train loss: 0.0240, test loss: 0.0299\n",
      "epoch: 340/1000, train loss: 0.0239, test loss: 0.0298\n",
      "epoch: 341/1000, train loss: 0.0238, test loss: 0.0297\n",
      "epoch: 342/1000, train loss: 0.0237, test loss: 0.0295\n",
      "epoch: 343/1000, train loss: 0.0236, test loss: 0.0294\n",
      "epoch: 344/1000, train loss: 0.0236, test loss: 0.0293\n",
      "epoch: 345/1000, train loss: 0.0235, test loss: 0.0292\n",
      "epoch: 346/1000, train loss: 0.0234, test loss: 0.0291\n",
      "epoch: 347/1000, train loss: 0.0233, test loss: 0.0290\n",
      "epoch: 348/1000, train loss: 0.0232, test loss: 0.0289\n",
      "epoch: 349/1000, train loss: 0.0231, test loss: 0.0287\n",
      "epoch: 350/1000, train loss: 0.0231, test loss: 0.0286\n",
      "epoch: 351/1000, train loss: 0.0230, test loss: 0.0285\n",
      "epoch: 352/1000, train loss: 0.0229, test loss: 0.0284\n",
      "epoch: 353/1000, train loss: 0.0228, test loss: 0.0283\n",
      "epoch: 354/1000, train loss: 0.0227, test loss: 0.0282\n",
      "epoch: 355/1000, train loss: 0.0226, test loss: 0.0281\n",
      "epoch: 356/1000, train loss: 0.0226, test loss: 0.0280\n",
      "epoch: 357/1000, train loss: 0.0225, test loss: 0.0278\n",
      "epoch: 358/1000, train loss: 0.0224, test loss: 0.0277\n",
      "epoch: 359/1000, train loss: 0.0223, test loss: 0.0276\n",
      "epoch: 360/1000, train loss: 0.0223, test loss: 0.0275\n",
      "epoch: 361/1000, train loss: 0.0222, test loss: 0.0274\n",
      "epoch: 362/1000, train loss: 0.0221, test loss: 0.0273\n",
      "epoch: 363/1000, train loss: 0.0220, test loss: 0.0272\n",
      "epoch: 364/1000, train loss: 0.0219, test loss: 0.0271\n",
      "epoch: 365/1000, train loss: 0.0219, test loss: 0.0270\n",
      "epoch: 366/1000, train loss: 0.0218, test loss: 0.0269\n",
      "epoch: 367/1000, train loss: 0.0217, test loss: 0.0268\n",
      "epoch: 368/1000, train loss: 0.0216, test loss: 0.0266\n",
      "epoch: 369/1000, train loss: 0.0216, test loss: 0.0265\n",
      "epoch: 370/1000, train loss: 0.0215, test loss: 0.0264\n",
      "epoch: 371/1000, train loss: 0.0214, test loss: 0.0263\n",
      "epoch: 372/1000, train loss: 0.0213, test loss: 0.0262\n",
      "epoch: 373/1000, train loss: 0.0213, test loss: 0.0261\n",
      "epoch: 374/1000, train loss: 0.0212, test loss: 0.0260\n",
      "epoch: 375/1000, train loss: 0.0211, test loss: 0.0259\n",
      "epoch: 376/1000, train loss: 0.0210, test loss: 0.0258\n",
      "epoch: 377/1000, train loss: 0.0210, test loss: 0.0257\n",
      "epoch: 378/1000, train loss: 0.0209, test loss: 0.0256\n",
      "epoch: 379/1000, train loss: 0.0208, test loss: 0.0255\n",
      "epoch: 380/1000, train loss: 0.0207, test loss: 0.0254\n",
      "epoch: 381/1000, train loss: 0.0207, test loss: 0.0253\n",
      "epoch: 382/1000, train loss: 0.0206, test loss: 0.0252\n",
      "epoch: 383/1000, train loss: 0.0205, test loss: 0.0251\n",
      "epoch: 384/1000, train loss: 0.0205, test loss: 0.0250\n",
      "epoch: 385/1000, train loss: 0.0204, test loss: 0.0249\n",
      "epoch: 386/1000, train loss: 0.0203, test loss: 0.0248\n",
      "epoch: 387/1000, train loss: 0.0202, test loss: 0.0247\n",
      "epoch: 388/1000, train loss: 0.0202, test loss: 0.0246\n",
      "epoch: 389/1000, train loss: 0.0201, test loss: 0.0245\n",
      "epoch: 390/1000, train loss: 0.0200, test loss: 0.0244\n",
      "epoch: 391/1000, train loss: 0.0200, test loss: 0.0243\n",
      "epoch: 392/1000, train loss: 0.0199, test loss: 0.0242\n",
      "epoch: 393/1000, train loss: 0.0198, test loss: 0.0241\n",
      "epoch: 394/1000, train loss: 0.0198, test loss: 0.0240\n",
      "epoch: 395/1000, train loss: 0.0197, test loss: 0.0239\n",
      "epoch: 396/1000, train loss: 0.0196, test loss: 0.0238\n",
      "epoch: 397/1000, train loss: 0.0196, test loss: 0.0237\n",
      "epoch: 398/1000, train loss: 0.0195, test loss: 0.0236\n",
      "epoch: 399/1000, train loss: 0.0194, test loss: 0.0235\n",
      "epoch: 400/1000, train loss: 0.0194, test loss: 0.0234\n",
      "epoch: 401/1000, train loss: 0.0193, test loss: 0.0233\n",
      "epoch: 402/1000, train loss: 0.0192, test loss: 0.0233\n",
      "epoch: 403/1000, train loss: 0.0192, test loss: 0.0232\n",
      "epoch: 404/1000, train loss: 0.0191, test loss: 0.0231\n",
      "epoch: 405/1000, train loss: 0.0190, test loss: 0.0230\n",
      "epoch: 406/1000, train loss: 0.0190, test loss: 0.0229\n",
      "epoch: 407/1000, train loss: 0.0189, test loss: 0.0228\n",
      "epoch: 408/1000, train loss: 0.0188, test loss: 0.0227\n",
      "epoch: 409/1000, train loss: 0.0188, test loss: 0.0226\n",
      "epoch: 410/1000, train loss: 0.0187, test loss: 0.0225\n",
      "epoch: 411/1000, train loss: 0.0187, test loss: 0.0224\n",
      "epoch: 412/1000, train loss: 0.0186, test loss: 0.0224\n",
      "epoch: 413/1000, train loss: 0.0185, test loss: 0.0223\n",
      "epoch: 414/1000, train loss: 0.0185, test loss: 0.0222\n",
      "epoch: 415/1000, train loss: 0.0184, test loss: 0.0221\n",
      "epoch: 416/1000, train loss: 0.0183, test loss: 0.0220\n",
      "epoch: 417/1000, train loss: 0.0183, test loss: 0.0219\n",
      "epoch: 418/1000, train loss: 0.0182, test loss: 0.0218\n",
      "epoch: 419/1000, train loss: 0.0182, test loss: 0.0218\n",
      "epoch: 420/1000, train loss: 0.0181, test loss: 0.0217\n",
      "epoch: 421/1000, train loss: 0.0180, test loss: 0.0216\n",
      "epoch: 422/1000, train loss: 0.0180, test loss: 0.0215\n",
      "epoch: 423/1000, train loss: 0.0179, test loss: 0.0214\n",
      "epoch: 424/1000, train loss: 0.0179, test loss: 0.0214\n",
      "epoch: 425/1000, train loss: 0.0178, test loss: 0.0213\n",
      "epoch: 426/1000, train loss: 0.0177, test loss: 0.0212\n",
      "epoch: 427/1000, train loss: 0.0177, test loss: 0.0211\n",
      "epoch: 428/1000, train loss: 0.0176, test loss: 0.0210\n",
      "epoch: 429/1000, train loss: 0.0176, test loss: 0.0210\n",
      "epoch: 430/1000, train loss: 0.0175, test loss: 0.0209\n",
      "epoch: 431/1000, train loss: 0.0175, test loss: 0.0208\n",
      "epoch: 432/1000, train loss: 0.0174, test loss: 0.0207\n",
      "epoch: 433/1000, train loss: 0.0173, test loss: 0.0206\n",
      "epoch: 434/1000, train loss: 0.0173, test loss: 0.0206\n",
      "epoch: 435/1000, train loss: 0.0172, test loss: 0.0205\n",
      "epoch: 436/1000, train loss: 0.0172, test loss: 0.0204\n",
      "epoch: 437/1000, train loss: 0.0171, test loss: 0.0203\n",
      "epoch: 438/1000, train loss: 0.0171, test loss: 0.0203\n",
      "epoch: 439/1000, train loss: 0.0170, test loss: 0.0202\n",
      "epoch: 440/1000, train loss: 0.0169, test loss: 0.0201\n",
      "epoch: 441/1000, train loss: 0.0169, test loss: 0.0200\n",
      "epoch: 442/1000, train loss: 0.0168, test loss: 0.0200\n",
      "epoch: 443/1000, train loss: 0.0168, test loss: 0.0199\n",
      "epoch: 444/1000, train loss: 0.0167, test loss: 0.0198\n",
      "epoch: 445/1000, train loss: 0.0167, test loss: 0.0198\n",
      "epoch: 446/1000, train loss: 0.0166, test loss: 0.0197\n",
      "epoch: 447/1000, train loss: 0.0166, test loss: 0.0196\n",
      "epoch: 448/1000, train loss: 0.0165, test loss: 0.0195\n",
      "epoch: 449/1000, train loss: 0.0165, test loss: 0.0195\n",
      "epoch: 450/1000, train loss: 0.0164, test loss: 0.0194\n",
      "epoch: 451/1000, train loss: 0.0164, test loss: 0.0193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 452/1000, train loss: 0.0163, test loss: 0.0193\n",
      "epoch: 453/1000, train loss: 0.0163, test loss: 0.0192\n",
      "epoch: 454/1000, train loss: 0.0162, test loss: 0.0191\n",
      "epoch: 455/1000, train loss: 0.0161, test loss: 0.0191\n",
      "epoch: 456/1000, train loss: 0.0161, test loss: 0.0190\n",
      "epoch: 457/1000, train loss: 0.0160, test loss: 0.0189\n",
      "epoch: 458/1000, train loss: 0.0160, test loss: 0.0189\n",
      "epoch: 459/1000, train loss: 0.0159, test loss: 0.0188\n",
      "epoch: 460/1000, train loss: 0.0159, test loss: 0.0187\n",
      "epoch: 461/1000, train loss: 0.0158, test loss: 0.0187\n",
      "epoch: 462/1000, train loss: 0.0158, test loss: 0.0186\n",
      "epoch: 463/1000, train loss: 0.0157, test loss: 0.0186\n",
      "epoch: 464/1000, train loss: 0.0157, test loss: 0.0185\n",
      "epoch: 465/1000, train loss: 0.0156, test loss: 0.0184\n",
      "epoch: 466/1000, train loss: 0.0156, test loss: 0.0184\n",
      "epoch: 467/1000, train loss: 0.0155, test loss: 0.0183\n",
      "epoch: 468/1000, train loss: 0.0155, test loss: 0.0182\n",
      "epoch: 469/1000, train loss: 0.0155, test loss: 0.0182\n",
      "epoch: 470/1000, train loss: 0.0154, test loss: 0.0181\n",
      "epoch: 471/1000, train loss: 0.0154, test loss: 0.0181\n",
      "epoch: 472/1000, train loss: 0.0153, test loss: 0.0180\n",
      "epoch: 473/1000, train loss: 0.0153, test loss: 0.0179\n",
      "epoch: 474/1000, train loss: 0.0152, test loss: 0.0179\n",
      "epoch: 475/1000, train loss: 0.0152, test loss: 0.0178\n",
      "epoch: 476/1000, train loss: 0.0151, test loss: 0.0178\n",
      "epoch: 477/1000, train loss: 0.0151, test loss: 0.0177\n",
      "epoch: 478/1000, train loss: 0.0150, test loss: 0.0177\n",
      "epoch: 479/1000, train loss: 0.0150, test loss: 0.0176\n",
      "epoch: 480/1000, train loss: 0.0149, test loss: 0.0175\n",
      "epoch: 481/1000, train loss: 0.0149, test loss: 0.0175\n",
      "epoch: 482/1000, train loss: 0.0148, test loss: 0.0174\n",
      "epoch: 483/1000, train loss: 0.0148, test loss: 0.0174\n",
      "epoch: 484/1000, train loss: 0.0148, test loss: 0.0173\n",
      "epoch: 485/1000, train loss: 0.0147, test loss: 0.0173\n",
      "epoch: 486/1000, train loss: 0.0147, test loss: 0.0172\n",
      "epoch: 487/1000, train loss: 0.0146, test loss: 0.0171\n",
      "epoch: 488/1000, train loss: 0.0146, test loss: 0.0171\n",
      "epoch: 489/1000, train loss: 0.0145, test loss: 0.0170\n",
      "epoch: 490/1000, train loss: 0.0145, test loss: 0.0170\n",
      "epoch: 491/1000, train loss: 0.0144, test loss: 0.0169\n",
      "epoch: 492/1000, train loss: 0.0144, test loss: 0.0169\n",
      "epoch: 493/1000, train loss: 0.0144, test loss: 0.0168\n",
      "epoch: 494/1000, train loss: 0.0143, test loss: 0.0168\n",
      "epoch: 495/1000, train loss: 0.0143, test loss: 0.0167\n",
      "epoch: 496/1000, train loss: 0.0142, test loss: 0.0167\n",
      "epoch: 497/1000, train loss: 0.0142, test loss: 0.0166\n",
      "epoch: 498/1000, train loss: 0.0141, test loss: 0.0166\n",
      "epoch: 499/1000, train loss: 0.0141, test loss: 0.0165\n",
      "epoch: 500/1000, train loss: 0.0141, test loss: 0.0165\n",
      "epoch: 501/1000, train loss: 0.0140, test loss: 0.0164\n",
      "epoch: 502/1000, train loss: 0.0140, test loss: 0.0164\n",
      "epoch: 503/1000, train loss: 0.0139, test loss: 0.0163\n",
      "epoch: 504/1000, train loss: 0.0139, test loss: 0.0163\n",
      "epoch: 505/1000, train loss: 0.0139, test loss: 0.0162\n",
      "epoch: 506/1000, train loss: 0.0138, test loss: 0.0162\n",
      "epoch: 507/1000, train loss: 0.0138, test loss: 0.0161\n",
      "epoch: 508/1000, train loss: 0.0137, test loss: 0.0161\n",
      "epoch: 509/1000, train loss: 0.0137, test loss: 0.0160\n",
      "epoch: 510/1000, train loss: 0.0137, test loss: 0.0160\n",
      "epoch: 511/1000, train loss: 0.0136, test loss: 0.0159\n",
      "epoch: 512/1000, train loss: 0.0136, test loss: 0.0159\n",
      "epoch: 513/1000, train loss: 0.0135, test loss: 0.0158\n",
      "epoch: 514/1000, train loss: 0.0135, test loss: 0.0158\n",
      "epoch: 515/1000, train loss: 0.0135, test loss: 0.0157\n",
      "epoch: 516/1000, train loss: 0.0134, test loss: 0.0157\n",
      "epoch: 517/1000, train loss: 0.0134, test loss: 0.0156\n",
      "epoch: 518/1000, train loss: 0.0133, test loss: 0.0156\n",
      "epoch: 519/1000, train loss: 0.0133, test loss: 0.0155\n",
      "epoch: 520/1000, train loss: 0.0133, test loss: 0.0155\n",
      "epoch: 521/1000, train loss: 0.0132, test loss: 0.0154\n",
      "epoch: 522/1000, train loss: 0.0132, test loss: 0.0154\n",
      "epoch: 523/1000, train loss: 0.0131, test loss: 0.0153\n",
      "epoch: 524/1000, train loss: 0.0131, test loss: 0.0153\n",
      "epoch: 525/1000, train loss: 0.0131, test loss: 0.0153\n",
      "epoch: 526/1000, train loss: 0.0130, test loss: 0.0152\n",
      "epoch: 527/1000, train loss: 0.0130, test loss: 0.0152\n",
      "epoch: 528/1000, train loss: 0.0130, test loss: 0.0151\n",
      "epoch: 529/1000, train loss: 0.0129, test loss: 0.0151\n",
      "epoch: 530/1000, train loss: 0.0129, test loss: 0.0150\n",
      "epoch: 531/1000, train loss: 0.0128, test loss: 0.0150\n",
      "epoch: 532/1000, train loss: 0.0128, test loss: 0.0149\n",
      "epoch: 533/1000, train loss: 0.0128, test loss: 0.0149\n",
      "epoch: 534/1000, train loss: 0.0127, test loss: 0.0149\n",
      "epoch: 535/1000, train loss: 0.0127, test loss: 0.0148\n",
      "epoch: 536/1000, train loss: 0.0127, test loss: 0.0148\n",
      "epoch: 537/1000, train loss: 0.0126, test loss: 0.0147\n",
      "epoch: 538/1000, train loss: 0.0126, test loss: 0.0147\n",
      "epoch: 539/1000, train loss: 0.0126, test loss: 0.0146\n",
      "epoch: 540/1000, train loss: 0.0125, test loss: 0.0146\n",
      "epoch: 541/1000, train loss: 0.0125, test loss: 0.0146\n",
      "epoch: 542/1000, train loss: 0.0125, test loss: 0.0145\n",
      "epoch: 543/1000, train loss: 0.0124, test loss: 0.0145\n",
      "epoch: 544/1000, train loss: 0.0124, test loss: 0.0144\n",
      "epoch: 545/1000, train loss: 0.0123, test loss: 0.0144\n",
      "epoch: 546/1000, train loss: 0.0123, test loss: 0.0144\n",
      "epoch: 547/1000, train loss: 0.0123, test loss: 0.0143\n",
      "epoch: 548/1000, train loss: 0.0122, test loss: 0.0143\n",
      "epoch: 549/1000, train loss: 0.0122, test loss: 0.0142\n",
      "epoch: 550/1000, train loss: 0.0122, test loss: 0.0142\n",
      "epoch: 551/1000, train loss: 0.0121, test loss: 0.0141\n",
      "epoch: 552/1000, train loss: 0.0121, test loss: 0.0141\n",
      "epoch: 553/1000, train loss: 0.0121, test loss: 0.0141\n",
      "epoch: 554/1000, train loss: 0.0120, test loss: 0.0140\n",
      "epoch: 555/1000, train loss: 0.0120, test loss: 0.0140\n",
      "epoch: 556/1000, train loss: 0.0120, test loss: 0.0139\n",
      "epoch: 557/1000, train loss: 0.0119, test loss: 0.0139\n",
      "epoch: 558/1000, train loss: 0.0119, test loss: 0.0139\n",
      "epoch: 559/1000, train loss: 0.0119, test loss: 0.0138\n",
      "epoch: 560/1000, train loss: 0.0118, test loss: 0.0138\n",
      "epoch: 561/1000, train loss: 0.0118, test loss: 0.0138\n",
      "epoch: 562/1000, train loss: 0.0118, test loss: 0.0137\n",
      "epoch: 563/1000, train loss: 0.0117, test loss: 0.0137\n",
      "epoch: 564/1000, train loss: 0.0117, test loss: 0.0136\n",
      "epoch: 565/1000, train loss: 0.0117, test loss: 0.0136\n",
      "epoch: 566/1000, train loss: 0.0116, test loss: 0.0136\n",
      "epoch: 567/1000, train loss: 0.0116, test loss: 0.0135\n",
      "epoch: 568/1000, train loss: 0.0116, test loss: 0.0135\n",
      "epoch: 569/1000, train loss: 0.0116, test loss: 0.0134\n",
      "epoch: 570/1000, train loss: 0.0115, test loss: 0.0134\n",
      "epoch: 571/1000, train loss: 0.0115, test loss: 0.0134\n",
      "epoch: 572/1000, train loss: 0.0115, test loss: 0.0133\n",
      "epoch: 573/1000, train loss: 0.0114, test loss: 0.0133\n",
      "epoch: 574/1000, train loss: 0.0114, test loss: 0.0133\n",
      "epoch: 575/1000, train loss: 0.0114, test loss: 0.0132\n",
      "epoch: 576/1000, train loss: 0.0113, test loss: 0.0132\n",
      "epoch: 577/1000, train loss: 0.0113, test loss: 0.0131\n",
      "epoch: 578/1000, train loss: 0.0113, test loss: 0.0131\n",
      "epoch: 579/1000, train loss: 0.0112, test loss: 0.0131\n",
      "epoch: 580/1000, train loss: 0.0112, test loss: 0.0130\n",
      "epoch: 581/1000, train loss: 0.0112, test loss: 0.0130\n",
      "epoch: 582/1000, train loss: 0.0112, test loss: 0.0130\n",
      "epoch: 583/1000, train loss: 0.0111, test loss: 0.0129\n",
      "epoch: 584/1000, train loss: 0.0111, test loss: 0.0129\n",
      "epoch: 585/1000, train loss: 0.0111, test loss: 0.0128\n",
      "epoch: 586/1000, train loss: 0.0110, test loss: 0.0128\n",
      "epoch: 587/1000, train loss: 0.0110, test loss: 0.0128\n",
      "epoch: 588/1000, train loss: 0.0110, test loss: 0.0127\n",
      "epoch: 589/1000, train loss: 0.0109, test loss: 0.0127\n",
      "epoch: 590/1000, train loss: 0.0109, test loss: 0.0127\n",
      "epoch: 591/1000, train loss: 0.0109, test loss: 0.0126\n",
      "epoch: 592/1000, train loss: 0.0109, test loss: 0.0126\n",
      "epoch: 593/1000, train loss: 0.0108, test loss: 0.0126\n",
      "epoch: 594/1000, train loss: 0.0108, test loss: 0.0125\n",
      "epoch: 595/1000, train loss: 0.0108, test loss: 0.0125\n",
      "epoch: 596/1000, train loss: 0.0107, test loss: 0.0125\n",
      "epoch: 597/1000, train loss: 0.0107, test loss: 0.0124\n",
      "epoch: 598/1000, train loss: 0.0107, test loss: 0.0124\n",
      "epoch: 599/1000, train loss: 0.0107, test loss: 0.0123\n",
      "epoch: 600/1000, train loss: 0.0106, test loss: 0.0123\n",
      "epoch: 601/1000, train loss: 0.0106, test loss: 0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 602/1000, train loss: 0.0106, test loss: 0.0122\n",
      "epoch: 603/1000, train loss: 0.0106, test loss: 0.0122\n",
      "epoch: 604/1000, train loss: 0.0105, test loss: 0.0122\n",
      "epoch: 605/1000, train loss: 0.0105, test loss: 0.0121\n",
      "epoch: 606/1000, train loss: 0.0105, test loss: 0.0121\n",
      "epoch: 607/1000, train loss: 0.0104, test loss: 0.0121\n",
      "epoch: 608/1000, train loss: 0.0104, test loss: 0.0120\n",
      "epoch: 609/1000, train loss: 0.0104, test loss: 0.0120\n",
      "epoch: 610/1000, train loss: 0.0104, test loss: 0.0120\n",
      "epoch: 611/1000, train loss: 0.0103, test loss: 0.0119\n",
      "epoch: 612/1000, train loss: 0.0103, test loss: 0.0119\n",
      "epoch: 613/1000, train loss: 0.0103, test loss: 0.0119\n",
      "epoch: 614/1000, train loss: 0.0103, test loss: 0.0118\n",
      "epoch: 615/1000, train loss: 0.0102, test loss: 0.0118\n",
      "epoch: 616/1000, train loss: 0.0102, test loss: 0.0118\n",
      "epoch: 617/1000, train loss: 0.0102, test loss: 0.0117\n",
      "epoch: 618/1000, train loss: 0.0102, test loss: 0.0117\n",
      "epoch: 619/1000, train loss: 0.0101, test loss: 0.0117\n",
      "epoch: 620/1000, train loss: 0.0101, test loss: 0.0116\n",
      "epoch: 621/1000, train loss: 0.0101, test loss: 0.0116\n",
      "epoch: 622/1000, train loss: 0.0101, test loss: 0.0116\n",
      "epoch: 623/1000, train loss: 0.0100, test loss: 0.0115\n",
      "epoch: 624/1000, train loss: 0.0100, test loss: 0.0115\n",
      "epoch: 625/1000, train loss: 0.0100, test loss: 0.0115\n",
      "epoch: 626/1000, train loss: 0.0100, test loss: 0.0114\n",
      "epoch: 627/1000, train loss: 0.0099, test loss: 0.0114\n",
      "epoch: 628/1000, train loss: 0.0099, test loss: 0.0114\n",
      "epoch: 629/1000, train loss: 0.0099, test loss: 0.0113\n",
      "epoch: 630/1000, train loss: 0.0099, test loss: 0.0113\n",
      "epoch: 631/1000, train loss: 0.0098, test loss: 0.0113\n",
      "epoch: 632/1000, train loss: 0.0098, test loss: 0.0112\n",
      "epoch: 633/1000, train loss: 0.0098, test loss: 0.0112\n",
      "epoch: 634/1000, train loss: 0.0098, test loss: 0.0112\n",
      "epoch: 635/1000, train loss: 0.0097, test loss: 0.0111\n",
      "epoch: 636/1000, train loss: 0.0097, test loss: 0.0111\n",
      "epoch: 637/1000, train loss: 0.0097, test loss: 0.0111\n",
      "epoch: 638/1000, train loss: 0.0097, test loss: 0.0110\n",
      "epoch: 639/1000, train loss: 0.0097, test loss: 0.0110\n",
      "epoch: 640/1000, train loss: 0.0096, test loss: 0.0110\n",
      "epoch: 641/1000, train loss: 0.0096, test loss: 0.0110\n",
      "epoch: 642/1000, train loss: 0.0096, test loss: 0.0109\n",
      "epoch: 643/1000, train loss: 0.0096, test loss: 0.0109\n",
      "epoch: 644/1000, train loss: 0.0095, test loss: 0.0109\n",
      "epoch: 645/1000, train loss: 0.0095, test loss: 0.0108\n",
      "epoch: 646/1000, train loss: 0.0095, test loss: 0.0108\n",
      "epoch: 647/1000, train loss: 0.0095, test loss: 0.0108\n",
      "epoch: 648/1000, train loss: 0.0095, test loss: 0.0107\n",
      "epoch: 649/1000, train loss: 0.0094, test loss: 0.0107\n",
      "epoch: 650/1000, train loss: 0.0094, test loss: 0.0107\n",
      "epoch: 651/1000, train loss: 0.0094, test loss: 0.0107\n",
      "epoch: 652/1000, train loss: 0.0094, test loss: 0.0106\n",
      "epoch: 653/1000, train loss: 0.0093, test loss: 0.0106\n",
      "epoch: 654/1000, train loss: 0.0093, test loss: 0.0106\n",
      "epoch: 655/1000, train loss: 0.0093, test loss: 0.0105\n",
      "epoch: 656/1000, train loss: 0.0093, test loss: 0.0105\n",
      "epoch: 657/1000, train loss: 0.0093, test loss: 0.0105\n",
      "epoch: 658/1000, train loss: 0.0092, test loss: 0.0105\n",
      "epoch: 659/1000, train loss: 0.0092, test loss: 0.0104\n",
      "epoch: 660/1000, train loss: 0.0092, test loss: 0.0104\n",
      "epoch: 661/1000, train loss: 0.0092, test loss: 0.0104\n",
      "epoch: 662/1000, train loss: 0.0092, test loss: 0.0103\n",
      "epoch: 663/1000, train loss: 0.0091, test loss: 0.0103\n",
      "epoch: 664/1000, train loss: 0.0091, test loss: 0.0103\n",
      "epoch: 665/1000, train loss: 0.0091, test loss: 0.0103\n",
      "epoch: 666/1000, train loss: 0.0091, test loss: 0.0102\n",
      "epoch: 667/1000, train loss: 0.0091, test loss: 0.0102\n",
      "epoch: 668/1000, train loss: 0.0090, test loss: 0.0102\n",
      "epoch: 669/1000, train loss: 0.0090, test loss: 0.0101\n",
      "epoch: 670/1000, train loss: 0.0090, test loss: 0.0101\n",
      "epoch: 671/1000, train loss: 0.0090, test loss: 0.0101\n",
      "epoch: 672/1000, train loss: 0.0090, test loss: 0.0101\n",
      "epoch: 673/1000, train loss: 0.0089, test loss: 0.0100\n",
      "epoch: 674/1000, train loss: 0.0089, test loss: 0.0100\n",
      "epoch: 675/1000, train loss: 0.0089, test loss: 0.0100\n",
      "epoch: 676/1000, train loss: 0.0089, test loss: 0.0100\n",
      "epoch: 677/1000, train loss: 0.0089, test loss: 0.0099\n",
      "epoch: 678/1000, train loss: 0.0088, test loss: 0.0099\n",
      "epoch: 679/1000, train loss: 0.0088, test loss: 0.0099\n",
      "epoch: 680/1000, train loss: 0.0088, test loss: 0.0099\n",
      "epoch: 681/1000, train loss: 0.0088, test loss: 0.0098\n",
      "epoch: 682/1000, train loss: 0.0088, test loss: 0.0098\n",
      "epoch: 683/1000, train loss: 0.0087, test loss: 0.0098\n",
      "epoch: 684/1000, train loss: 0.0087, test loss: 0.0097\n",
      "epoch: 685/1000, train loss: 0.0087, test loss: 0.0097\n",
      "epoch: 686/1000, train loss: 0.0087, test loss: 0.0097\n",
      "epoch: 687/1000, train loss: 0.0087, test loss: 0.0097\n",
      "epoch: 688/1000, train loss: 0.0087, test loss: 0.0096\n",
      "epoch: 689/1000, train loss: 0.0086, test loss: 0.0096\n",
      "epoch: 690/1000, train loss: 0.0086, test loss: 0.0096\n",
      "epoch: 691/1000, train loss: 0.0086, test loss: 0.0096\n",
      "epoch: 692/1000, train loss: 0.0086, test loss: 0.0095\n",
      "epoch: 693/1000, train loss: 0.0086, test loss: 0.0095\n",
      "epoch: 694/1000, train loss: 0.0085, test loss: 0.0095\n",
      "epoch: 695/1000, train loss: 0.0085, test loss: 0.0095\n",
      "epoch: 696/1000, train loss: 0.0085, test loss: 0.0095\n",
      "epoch: 697/1000, train loss: 0.0085, test loss: 0.0094\n",
      "epoch: 698/1000, train loss: 0.0085, test loss: 0.0094\n",
      "epoch: 699/1000, train loss: 0.0085, test loss: 0.0094\n",
      "epoch: 700/1000, train loss: 0.0084, test loss: 0.0094\n",
      "epoch: 701/1000, train loss: 0.0084, test loss: 0.0093\n",
      "epoch: 702/1000, train loss: 0.0084, test loss: 0.0093\n",
      "epoch: 703/1000, train loss: 0.0084, test loss: 0.0093\n",
      "epoch: 704/1000, train loss: 0.0084, test loss: 0.0093\n",
      "epoch: 705/1000, train loss: 0.0084, test loss: 0.0092\n",
      "epoch: 706/1000, train loss: 0.0083, test loss: 0.0092\n",
      "epoch: 707/1000, train loss: 0.0083, test loss: 0.0092\n",
      "epoch: 708/1000, train loss: 0.0083, test loss: 0.0092\n",
      "epoch: 709/1000, train loss: 0.0083, test loss: 0.0092\n",
      "epoch: 710/1000, train loss: 0.0083, test loss: 0.0091\n",
      "epoch: 711/1000, train loss: 0.0083, test loss: 0.0091\n",
      "epoch: 712/1000, train loss: 0.0083, test loss: 0.0091\n",
      "epoch: 713/1000, train loss: 0.0082, test loss: 0.0091\n",
      "epoch: 714/1000, train loss: 0.0082, test loss: 0.0091\n",
      "epoch: 715/1000, train loss: 0.0082, test loss: 0.0090\n",
      "epoch: 716/1000, train loss: 0.0082, test loss: 0.0090\n",
      "epoch: 717/1000, train loss: 0.0082, test loss: 0.0090\n",
      "epoch: 718/1000, train loss: 0.0082, test loss: 0.0090\n",
      "epoch: 719/1000, train loss: 0.0081, test loss: 0.0089\n",
      "epoch: 720/1000, train loss: 0.0081, test loss: 0.0089\n",
      "epoch: 721/1000, train loss: 0.0081, test loss: 0.0089\n",
      "epoch: 722/1000, train loss: 0.0081, test loss: 0.0089\n",
      "epoch: 723/1000, train loss: 0.0081, test loss: 0.0089\n",
      "epoch: 724/1000, train loss: 0.0081, test loss: 0.0088\n",
      "epoch: 725/1000, train loss: 0.0081, test loss: 0.0088\n",
      "epoch: 726/1000, train loss: 0.0080, test loss: 0.0088\n",
      "epoch: 727/1000, train loss: 0.0080, test loss: 0.0088\n",
      "epoch: 728/1000, train loss: 0.0080, test loss: 0.0088\n",
      "epoch: 729/1000, train loss: 0.0080, test loss: 0.0088\n",
      "epoch: 730/1000, train loss: 0.0080, test loss: 0.0087\n",
      "epoch: 731/1000, train loss: 0.0080, test loss: 0.0087\n",
      "epoch: 732/1000, train loss: 0.0080, test loss: 0.0087\n",
      "epoch: 733/1000, train loss: 0.0079, test loss: 0.0087\n",
      "epoch: 734/1000, train loss: 0.0079, test loss: 0.0087\n",
      "epoch: 735/1000, train loss: 0.0079, test loss: 0.0086\n",
      "epoch: 736/1000, train loss: 0.0079, test loss: 0.0086\n",
      "epoch: 737/1000, train loss: 0.0079, test loss: 0.0086\n",
      "epoch: 738/1000, train loss: 0.0079, test loss: 0.0086\n",
      "epoch: 739/1000, train loss: 0.0079, test loss: 0.0086\n",
      "epoch: 740/1000, train loss: 0.0078, test loss: 0.0085\n",
      "epoch: 741/1000, train loss: 0.0078, test loss: 0.0085\n",
      "epoch: 742/1000, train loss: 0.0078, test loss: 0.0085\n",
      "epoch: 743/1000, train loss: 0.0078, test loss: 0.0085\n",
      "epoch: 744/1000, train loss: 0.0078, test loss: 0.0085\n",
      "epoch: 745/1000, train loss: 0.0078, test loss: 0.0085\n",
      "epoch: 746/1000, train loss: 0.0078, test loss: 0.0084\n",
      "epoch: 747/1000, train loss: 0.0077, test loss: 0.0084\n",
      "epoch: 748/1000, train loss: 0.0077, test loss: 0.0084\n",
      "epoch: 749/1000, train loss: 0.0077, test loss: 0.0084\n",
      "epoch: 750/1000, train loss: 0.0077, test loss: 0.0084\n",
      "epoch: 751/1000, train loss: 0.0077, test loss: 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 752/1000, train loss: 0.0077, test loss: 0.0083\n",
      "epoch: 753/1000, train loss: 0.0077, test loss: 0.0083\n",
      "epoch: 754/1000, train loss: 0.0077, test loss: 0.0083\n",
      "epoch: 755/1000, train loss: 0.0076, test loss: 0.0083\n",
      "epoch: 756/1000, train loss: 0.0076, test loss: 0.0083\n",
      "epoch: 757/1000, train loss: 0.0076, test loss: 0.0083\n",
      "epoch: 758/1000, train loss: 0.0076, test loss: 0.0082\n",
      "epoch: 759/1000, train loss: 0.0076, test loss: 0.0082\n",
      "epoch: 760/1000, train loss: 0.0076, test loss: 0.0082\n",
      "epoch: 761/1000, train loss: 0.0076, test loss: 0.0082\n",
      "epoch: 762/1000, train loss: 0.0076, test loss: 0.0082\n",
      "epoch: 763/1000, train loss: 0.0075, test loss: 0.0082\n",
      "epoch: 764/1000, train loss: 0.0075, test loss: 0.0081\n",
      "epoch: 765/1000, train loss: 0.0075, test loss: 0.0081\n",
      "epoch: 766/1000, train loss: 0.0075, test loss: 0.0081\n",
      "epoch: 767/1000, train loss: 0.0075, test loss: 0.0081\n",
      "epoch: 768/1000, train loss: 0.0075, test loss: 0.0081\n",
      "epoch: 769/1000, train loss: 0.0075, test loss: 0.0081\n",
      "epoch: 770/1000, train loss: 0.0075, test loss: 0.0081\n",
      "epoch: 771/1000, train loss: 0.0074, test loss: 0.0080\n",
      "epoch: 772/1000, train loss: 0.0074, test loss: 0.0080\n",
      "epoch: 773/1000, train loss: 0.0074, test loss: 0.0080\n",
      "epoch: 774/1000, train loss: 0.0074, test loss: 0.0080\n",
      "epoch: 775/1000, train loss: 0.0074, test loss: 0.0080\n",
      "epoch: 776/1000, train loss: 0.0074, test loss: 0.0080\n",
      "epoch: 777/1000, train loss: 0.0074, test loss: 0.0080\n",
      "epoch: 778/1000, train loss: 0.0074, test loss: 0.0079\n",
      "epoch: 779/1000, train loss: 0.0074, test loss: 0.0079\n",
      "epoch: 780/1000, train loss: 0.0073, test loss: 0.0079\n",
      "epoch: 781/1000, train loss: 0.0073, test loss: 0.0079\n",
      "epoch: 782/1000, train loss: 0.0073, test loss: 0.0079\n",
      "epoch: 783/1000, train loss: 0.0073, test loss: 0.0079\n",
      "epoch: 784/1000, train loss: 0.0073, test loss: 0.0079\n",
      "epoch: 785/1000, train loss: 0.0073, test loss: 0.0078\n",
      "epoch: 786/1000, train loss: 0.0073, test loss: 0.0078\n",
      "epoch: 787/1000, train loss: 0.0073, test loss: 0.0078\n",
      "epoch: 788/1000, train loss: 0.0072, test loss: 0.0078\n",
      "epoch: 789/1000, train loss: 0.0072, test loss: 0.0078\n",
      "epoch: 790/1000, train loss: 0.0072, test loss: 0.0078\n",
      "epoch: 791/1000, train loss: 0.0072, test loss: 0.0078\n",
      "epoch: 792/1000, train loss: 0.0072, test loss: 0.0078\n",
      "epoch: 793/1000, train loss: 0.0072, test loss: 0.0077\n",
      "epoch: 794/1000, train loss: 0.0072, test loss: 0.0077\n",
      "epoch: 795/1000, train loss: 0.0072, test loss: 0.0077\n",
      "epoch: 796/1000, train loss: 0.0072, test loss: 0.0077\n",
      "epoch: 797/1000, train loss: 0.0072, test loss: 0.0077\n",
      "epoch: 798/1000, train loss: 0.0071, test loss: 0.0077\n",
      "epoch: 799/1000, train loss: 0.0071, test loss: 0.0077\n",
      "epoch: 800/1000, train loss: 0.0071, test loss: 0.0076\n",
      "epoch: 801/1000, train loss: 0.0071, test loss: 0.0076\n",
      "epoch: 802/1000, train loss: 0.0071, test loss: 0.0076\n",
      "epoch: 803/1000, train loss: 0.0071, test loss: 0.0076\n",
      "epoch: 804/1000, train loss: 0.0071, test loss: 0.0076\n",
      "epoch: 805/1000, train loss: 0.0071, test loss: 0.0076\n",
      "epoch: 806/1000, train loss: 0.0071, test loss: 0.0076\n",
      "epoch: 807/1000, train loss: 0.0070, test loss: 0.0076\n",
      "epoch: 808/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 809/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 810/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 811/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 812/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 813/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 814/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 815/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 816/1000, train loss: 0.0070, test loss: 0.0075\n",
      "epoch: 817/1000, train loss: 0.0070, test loss: 0.0074\n",
      "epoch: 818/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 819/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 820/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 821/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 822/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 823/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 824/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 825/1000, train loss: 0.0069, test loss: 0.0074\n",
      "epoch: 826/1000, train loss: 0.0069, test loss: 0.0073\n",
      "epoch: 827/1000, train loss: 0.0069, test loss: 0.0073\n",
      "epoch: 828/1000, train loss: 0.0068, test loss: 0.0073\n",
      "epoch: 829/1000, train loss: 0.0068, test loss: 0.0073\n",
      "epoch: 830/1000, train loss: 0.0068, test loss: 0.0073\n",
      "epoch: 831/1000, train loss: 0.0068, test loss: 0.0073\n",
      "epoch: 832/1000, train loss: 0.0068, test loss: 0.0073\n",
      "epoch: 833/1000, train loss: 0.0068, test loss: 0.0073\n",
      "epoch: 834/1000, train loss: 0.0068, test loss: 0.0073\n",
      "epoch: 835/1000, train loss: 0.0068, test loss: 0.0072\n",
      "epoch: 836/1000, train loss: 0.0068, test loss: 0.0072\n",
      "epoch: 837/1000, train loss: 0.0068, test loss: 0.0072\n",
      "epoch: 838/1000, train loss: 0.0068, test loss: 0.0072\n",
      "epoch: 839/1000, train loss: 0.0067, test loss: 0.0072\n",
      "epoch: 840/1000, train loss: 0.0067, test loss: 0.0072\n",
      "epoch: 841/1000, train loss: 0.0067, test loss: 0.0072\n",
      "epoch: 842/1000, train loss: 0.0067, test loss: 0.0072\n",
      "epoch: 843/1000, train loss: 0.0067, test loss: 0.0072\n",
      "epoch: 844/1000, train loss: 0.0067, test loss: 0.0072\n",
      "epoch: 845/1000, train loss: 0.0067, test loss: 0.0071\n",
      "epoch: 846/1000, train loss: 0.0067, test loss: 0.0071\n",
      "epoch: 847/1000, train loss: 0.0067, test loss: 0.0071\n",
      "epoch: 848/1000, train loss: 0.0067, test loss: 0.0071\n",
      "epoch: 849/1000, train loss: 0.0067, test loss: 0.0071\n",
      "epoch: 850/1000, train loss: 0.0067, test loss: 0.0071\n",
      "epoch: 851/1000, train loss: 0.0066, test loss: 0.0071\n",
      "epoch: 852/1000, train loss: 0.0066, test loss: 0.0071\n",
      "epoch: 853/1000, train loss: 0.0066, test loss: 0.0071\n",
      "epoch: 854/1000, train loss: 0.0066, test loss: 0.0071\n",
      "epoch: 855/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 856/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 857/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 858/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 859/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 860/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 861/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 862/1000, train loss: 0.0066, test loss: 0.0070\n",
      "epoch: 863/1000, train loss: 0.0065, test loss: 0.0070\n",
      "epoch: 864/1000, train loss: 0.0065, test loss: 0.0070\n",
      "epoch: 865/1000, train loss: 0.0065, test loss: 0.0070\n",
      "epoch: 866/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 867/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 868/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 869/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 870/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 871/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 872/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 873/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 874/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 875/1000, train loss: 0.0065, test loss: 0.0069\n",
      "epoch: 876/1000, train loss: 0.0064, test loss: 0.0069\n",
      "epoch: 877/1000, train loss: 0.0064, test loss: 0.0069\n",
      "epoch: 878/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 879/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 880/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 881/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 882/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 883/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 884/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 885/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 886/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 887/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 888/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 889/1000, train loss: 0.0064, test loss: 0.0068\n",
      "epoch: 890/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 891/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 892/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 893/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 894/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 895/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 896/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 897/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 898/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 899/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 900/1000, train loss: 0.0063, test loss: 0.0067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 901/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 902/1000, train loss: 0.0063, test loss: 0.0067\n",
      "epoch: 903/1000, train loss: 0.0063, test loss: 0.0066\n",
      "epoch: 904/1000, train loss: 0.0063, test loss: 0.0066\n",
      "epoch: 905/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 906/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 907/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 908/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 909/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 910/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 911/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 912/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 913/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 914/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 915/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 916/1000, train loss: 0.0062, test loss: 0.0066\n",
      "epoch: 917/1000, train loss: 0.0062, test loss: 0.0065\n",
      "epoch: 918/1000, train loss: 0.0062, test loss: 0.0065\n",
      "epoch: 919/1000, train loss: 0.0062, test loss: 0.0065\n",
      "epoch: 920/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 921/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 922/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 923/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 924/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 925/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 926/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 927/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 928/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 929/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 930/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 931/1000, train loss: 0.0061, test loss: 0.0065\n",
      "epoch: 932/1000, train loss: 0.0061, test loss: 0.0064\n",
      "epoch: 933/1000, train loss: 0.0061, test loss: 0.0064\n",
      "epoch: 934/1000, train loss: 0.0061, test loss: 0.0064\n",
      "epoch: 935/1000, train loss: 0.0061, test loss: 0.0064\n",
      "epoch: 936/1000, train loss: 0.0061, test loss: 0.0064\n",
      "epoch: 937/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 938/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 939/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 940/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 941/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 942/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 943/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 944/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 945/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 946/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 947/1000, train loss: 0.0060, test loss: 0.0064\n",
      "epoch: 948/1000, train loss: 0.0060, test loss: 0.0063\n",
      "epoch: 949/1000, train loss: 0.0060, test loss: 0.0063\n",
      "epoch: 950/1000, train loss: 0.0060, test loss: 0.0063\n",
      "epoch: 951/1000, train loss: 0.0060, test loss: 0.0063\n",
      "epoch: 952/1000, train loss: 0.0060, test loss: 0.0063\n",
      "epoch: 953/1000, train loss: 0.0060, test loss: 0.0063\n",
      "epoch: 954/1000, train loss: 0.0060, test loss: 0.0063\n",
      "epoch: 955/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 956/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 957/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 958/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 959/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 960/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 961/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 962/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 963/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 964/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 965/1000, train loss: 0.0059, test loss: 0.0063\n",
      "epoch: 966/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 967/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 968/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 969/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 970/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 971/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 972/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 973/1000, train loss: 0.0059, test loss: 0.0062\n",
      "epoch: 974/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 975/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 976/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 977/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 978/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 979/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 980/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 981/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 982/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 983/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 984/1000, train loss: 0.0058, test loss: 0.0062\n",
      "epoch: 985/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 986/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 987/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 988/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 989/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 990/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 991/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 992/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 993/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 994/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 995/1000, train loss: 0.0058, test loss: 0.0061\n",
      "epoch: 996/1000, train loss: 0.0057, test loss: 0.0061\n",
      "epoch: 997/1000, train loss: 0.0057, test loss: 0.0061\n",
      "epoch: 998/1000, train loss: 0.0057, test loss: 0.0061\n",
      "epoch: 999/1000, train loss: 0.0057, test loss: 0.0061\n",
      "epoch: 1000/1000, train loss: 0.0057, test loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import utils\n",
    "from functools import partial\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "def preprocess(Data,max_norm=1,min_norm=0):\n",
    "    #normalization\n",
    "    Data = (Data - Data.min())/(Data.max() - Data.min())*(max_norm - min_norm) + min_norm\n",
    "    # for i in range(len(Data.columns)):\n",
    "    #     Data[Data.columns[i]] = (Data[Data.columns[i]] - Data[Data.columns[i]].mean())/Data[Data.columns[i]].std()\n",
    "    #     print('standardized')\n",
    "    #standardization\n",
    "    #for i in range(len(Data.columns)):\n",
    "    #    if Data[Data.columns[i]].std()*3 > (Data[Data.columns[i]].max()-Data[Data.columns[i]].min()):\n",
    "    #        Data[Data.columns[i]] = (Data[Data.columns[i]] - Data[Data.columns[i]].mean())/Data[Data.columns[i]].std()\n",
    "    #        print('standardized %d'%i)\n",
    "    #    else:\n",
    "    #        Data = (Data - Data.min())/(Data.max() - Data.min())*(max_norm - min_norm) + min_norm\n",
    "    return Data\n",
    "\n",
    "Data = pd.read_csv(r'D:\\Desktop\\190305\\csvfile\\new\\새 폴더 (3)\\0d_mean_orig_channel.csv')\n",
    "\n",
    "# del Data['Unnamed: 0']\n",
    "# del Data['Nu_t_lsq_n']\n",
    "del Data['PHI-Alpha_t_lsq_n']\n",
    "# del Data['Re']\n",
    "# del Data['Pr']\n",
    "del Data['X']\n",
    "# del Data['Y']\n",
    "del Data['y_plus']\n",
    "del Data['wedge_height']\n",
    "del Data['1/Pr']\n",
    "# del Data['local_volume']\n",
    "\n",
    "# a = Data['local_volume']\n",
    "\n",
    "del Data['local_volume']\n",
    "\n",
    "# Data = pd.DataFrame()\n",
    "# Data['dk_dx_i_abs'] = data['dk_dx_i_abs']\n",
    "# Data['dT/dx'] = data['dT/dx']\n",
    "# Data['dT/dy'] = data['dT/dy']\n",
    "# # Data['Nu_t_lsq_n'] = data['Nu_t_lsq_n']\n",
    "# Data['PHI-Alpha_t_lsq_n'] = data['PHI-Alpha_t_lsq_n']\n",
    "# # Data['PHI-Pr_t_new'] = data['PHI-Pr_t_new']\n",
    "\n",
    "denorm_min = Data['PHI-Pr_t_new'].min()\n",
    "denorm_max = Data['PHI-Pr_t_new'].max()\n",
    "\n",
    "X = Data[Data.columns[:-1]]\n",
    "X_length = len(X.columns)\n",
    "Y = Data[Data.columns[-1:]]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# nu_t = x_test['Nu_t_lsq_n']\n",
    "train_index = x_train['Y']\n",
    "test_index = x_test['Y']\n",
    "\n",
    "del x_train['Y']\n",
    "del x_test['Y']\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "x_test = preprocess(x_test)\n",
    "y_train = preprocess(y_train)\n",
    "y_test = preprocess(y_test)\n",
    "\n",
    "train_index = \n",
    "\n",
    "x_train = x_train.values.tolist()\n",
    "x_test = x_test.values.tolist()\n",
    "y_train = y_train.values.tolist()\n",
    "y_test = y_test.values.tolist()\n",
    "\n",
    "learning_rate = 0.001\n",
    "l2_reg = 0.0005\n",
    "# n_epochs = 10000\n",
    "# batch_size = 128\n",
    "\n",
    "he_init = tf.keras.initializers.he_normal()\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(scale=l2_reg)\n",
    "dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init,use_bias=False, kernel_regularizer=l2_regularizer)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_length-1])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "hidden1 = dense_layer(X,32)\n",
    "hidden1 = tf.layers.dropout(hidden1, 0.8, is_training)\n",
    "hidden2 = dense_layer(hidden1,32)\n",
    "hidden2 = tf.layers.dropout(hidden2, 0.8, is_training)\n",
    "hidden3 = dense_layer(hidden2,32)\n",
    "hidden3 = tf.layers.dropout(hidden3, 0.8, is_training)\n",
    "hidden4 = dense_layer(hidden3,16)\n",
    "hidden4 = tf.layers.dropout(hidden4, 0.8, is_training)\n",
    "hidden5 = dense_layer(hidden4,16)\n",
    "hidden5 = tf.layers.dropout(hidden5, 0.8, is_training)\n",
    "hidden6 = dense_layer(hidden5,16)\n",
    "hidden6 = tf.layers.dropout(hidden6, 0.8, is_training)\n",
    "hidden7 = dense_layer(hidden6,8)\n",
    "hidden7 = tf.layers.dropout(hidden7, 0.8, is_training)\n",
    "hidden8 = dense_layer(hidden7,8)\n",
    "hidden8 = tf.layers.dropout(hidden8, 0.8, is_training)\n",
    "hidden9 = dense_layer(hidden8,4)\n",
    "hidden9 = tf.layers.dropout(hidden9, 0.8, is_training)\n",
    "hidden10 = dense_layer(hidden9,1,activation=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(hidden10 - Y))\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "num_over = 0\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "for epoch_index in range(epochs):\n",
    "    if shuffle:\n",
    "        utils.shuffle(x_train, y_train)\n",
    "    total_batch = int(np.ceil(len(x_train) / batch_size))\n",
    "    for batch_index in range(total_batch):\n",
    "        start = batch_index*batch_size\n",
    "        end = start + batch_size\n",
    "        x_train_batch = x_train[start:end]\n",
    "        y_train_batch = y_train[start:end]\n",
    "        sess.run(train_op, feed_dict={X: x_train_batch, Y:y_train_batch,is_training:True})\n",
    "    \n",
    "    loss_value_train = sess.run(loss, feed_dict={X: x_train, Y: y_train,is_training:False})\n",
    "    loss_value_test = sess.run(loss, feed_dict={X: x_test, Y: y_test,is_training:False})\n",
    "    print('epoch: {}/{}, train loss: {:.4f}, test loss: {:.4f}'.format(epoch_index+1,epochs,loss_value_train,loss_value_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 2/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 3/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 4/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 5/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 6/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 7/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 8/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 9/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 10/1000, train loss: 0.0071, test loss: 0.0079\n",
      "epoch: 11/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 12/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 13/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 14/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 15/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 16/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 17/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 18/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 19/1000, train loss: 0.0070, test loss: 0.0079\n",
      "epoch: 20/1000, train loss: 0.0069, test loss: 0.0079\n",
      "epoch: 21/1000, train loss: 0.0069, test loss: 0.0079\n",
      "epoch: 22/1000, train loss: 0.0069, test loss: 0.0079\n",
      "epoch: 23/1000, train loss: 0.0069, test loss: 0.0080\n",
      "epoch: 24/1000, train loss: 0.0069, test loss: 0.0080\n",
      "epoch: 25/1000, train loss: 0.0069, test loss: 0.0080\n",
      "epoch: 26/1000, train loss: 0.0069, test loss: 0.0080\n",
      "epoch: 27/1000, train loss: 0.0069, test loss: 0.0080\n",
      "epoch: 28/1000, train loss: 0.0068, test loss: 0.0080\n",
      "epoch: 29/1000, train loss: 0.0068, test loss: 0.0081\n",
      "epoch: 30/1000, train loss: 0.0068, test loss: 0.0081\n",
      "epoch: 31/1000, train loss: 0.0068, test loss: 0.0081\n",
      "epoch: 32/1000, train loss: 0.0068, test loss: 0.0081\n",
      "epoch: 33/1000, train loss: 0.0068, test loss: 0.0081\n",
      "epoch: 34/1000, train loss: 0.0068, test loss: 0.0082\n",
      "epoch: 35/1000, train loss: 0.0068, test loss: 0.0082\n",
      "epoch: 36/1000, train loss: 0.0068, test loss: 0.0082\n",
      "epoch: 37/1000, train loss: 0.0068, test loss: 0.0082\n",
      "epoch: 38/1000, train loss: 0.0068, test loss: 0.0082\n",
      "epoch: 39/1000, train loss: 0.0068, test loss: 0.0083\n",
      "epoch: 40/1000, train loss: 0.0068, test loss: 0.0083\n",
      "epoch: 41/1000, train loss: 0.0067, test loss: 0.0083\n",
      "epoch: 42/1000, train loss: 0.0067, test loss: 0.0083\n",
      "epoch: 43/1000, train loss: 0.0067, test loss: 0.0083\n",
      "epoch: 44/1000, train loss: 0.0067, test loss: 0.0083\n",
      "epoch: 45/1000, train loss: 0.0067, test loss: 0.0083\n",
      "epoch: 46/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 47/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 48/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 49/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 50/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 51/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 52/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 53/1000, train loss: 0.0067, test loss: 0.0084\n",
      "epoch: 54/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 55/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 56/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 57/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 58/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 59/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 60/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 61/1000, train loss: 0.0067, test loss: 0.0085\n",
      "epoch: 62/1000, train loss: 0.0066, test loss: 0.0085\n",
      "epoch: 63/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 64/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 65/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 66/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 67/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 68/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 69/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 70/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 71/1000, train loss: 0.0066, test loss: 0.0086\n",
      "epoch: 72/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 73/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 74/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 75/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 76/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 77/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 78/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 79/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 80/1000, train loss: 0.0066, test loss: 0.0087\n",
      "epoch: 81/1000, train loss: 0.0066, test loss: 0.0088\n",
      "epoch: 82/1000, train loss: 0.0066, test loss: 0.0088\n",
      "epoch: 83/1000, train loss: 0.0066, test loss: 0.0088\n",
      "epoch: 84/1000, train loss: 0.0066, test loss: 0.0088\n",
      "epoch: 85/1000, train loss: 0.0066, test loss: 0.0088\n",
      "epoch: 86/1000, train loss: 0.0066, test loss: 0.0088\n",
      "epoch: 87/1000, train loss: 0.0065, test loss: 0.0088\n",
      "epoch: 88/1000, train loss: 0.0065, test loss: 0.0088\n",
      "epoch: 89/1000, train loss: 0.0065, test loss: 0.0089\n",
      "epoch: 90/1000, train loss: 0.0065, test loss: 0.0089\n",
      "epoch: 91/1000, train loss: 0.0065, test loss: 0.0089\n",
      "epoch: 92/1000, train loss: 0.0065, test loss: 0.0089\n",
      "epoch: 93/1000, train loss: 0.0065, test loss: 0.0089\n",
      "epoch: 94/1000, train loss: 0.0065, test loss: 0.0089\n",
      "epoch: 95/1000, train loss: 0.0065, test loss: 0.0089\n",
      "epoch: 96/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 97/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 98/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 99/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 100/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 101/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 102/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 103/1000, train loss: 0.0065, test loss: 0.0090\n",
      "epoch: 104/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 105/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 106/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 107/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 108/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 109/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 110/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 111/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 112/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 113/1000, train loss: 0.0065, test loss: 0.0091\n",
      "epoch: 114/1000, train loss: 0.0065, test loss: 0.0092\n",
      "epoch: 115/1000, train loss: 0.0065, test loss: 0.0092\n",
      "epoch: 116/1000, train loss: 0.0065, test loss: 0.0092\n",
      "epoch: 117/1000, train loss: 0.0065, test loss: 0.0092\n",
      "epoch: 118/1000, train loss: 0.0064, test loss: 0.0092\n",
      "epoch: 119/1000, train loss: 0.0064, test loss: 0.0092\n",
      "epoch: 120/1000, train loss: 0.0064, test loss: 0.0092\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7b3e56849aff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mx_train_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0my_train_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_train_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss_value_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_index in range(epochs):\n",
    "    if shuffle:\n",
    "        utils.shuffle(x_train, y_train)\n",
    "    total_batch = int(np.ceil(len(x_train) / batch_size))\n",
    "    for batch_index in range(total_batch):\n",
    "        start = batch_index*batch_size\n",
    "        end = start + batch_size\n",
    "        x_train_batch = x_train[start:end]\n",
    "        y_train_batch = y_train[start:end]\n",
    "        sess.run(train_op, feed_dict={X: x_train_batch, Y:y_train_batch,is_training:True})\n",
    "    \n",
    "    loss_value_train = sess.run(loss, feed_dict={X: x_train, Y: y_train,is_training:False})\n",
    "    loss_value_test = sess.run(loss, feed_dict={X: x_test, Y: y_test,is_training:False})\n",
    "    print('epoch: {}/{}, train loss: {:.4f}, test loss: {:.4f}'.format(epoch_index+1,epochs,loss_value_train,loss_value_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(sess.run(hidden10,feed_dict={X: x_test,is_training:False}),y_test,s=1)\n",
    "plt.axis([0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
